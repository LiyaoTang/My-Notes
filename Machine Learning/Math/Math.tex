\chapter{Math}

\section{Convolution}

\begin{itemize}
\item Definition
	\begin{itemize}
	\item $\displaystyle f* g (z) = \int_{\mathbb R}f(x)g(z-x) dx,$ where $f(x),g(x)$ are functions in $\mathbb R$
	\end{itemize}
	
\item Statistical Meaning
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item X,Y: independent random variables, with pdf's given by $f$ and $g$
		\item $Z = X+Y$, with pdf given by $h(z)$:
		\end{itemize}
		
	\item $\Rightarrow h(z) = f * g (z)$
		\begin{itemize}
		\item derivation
		\end{itemize}
	\begin{align*} H(z) &= P(Z<z) = P(X+Y<z) \\ &= \int_x P(X=x)P(X+Y<z|X=x) dx \\ &= \int_x f(x)P(Y<z-x)dx \\ &= \int_x f(x)G_Y(z-x)dx  \end{align*} 
	\begin{align*} \Rightarrow h(x) &= \frac d {dz} H(z) = \frac d {dz} \int_x f(x)G_Y(z-x)dx \\ &= \int_x f(x) \frac {dG_Y(z-x)}{dz} dx \\ &= \int_x f(x) g(z-x)dx \\ &= f * g (z) \end{align*}
	\end{itemize}
\end{itemize}
 
\section{Linear Algebra}

\subsection{Essence}
\subsubsection{Vector}
\begin{itemize}
\item Interpretation
	\begin{itemize}
	\item Movement
		\begin{itemize}
		\item direction
		\item distance
		\end{itemize}
	\item Numeric in High Dimensions
		\begin{itemize}
		\item in 1-$D$: +/- represents direction
		\item in n-$D$: +/- alone each dimension combined to represent an overall direction \\
		(direction of the n-$D$ numeric - vector)
		\end{itemize}
	\item Abstraction
		\begin{itemize}
		\item a vector space with operations that can be applied on different math concept \\
		(e.g. function derivatives is actually in vector space)
		\end{itemize}
	\end{itemize}

\item Numerics Multiplication
	\begin{itemize}
	\item Scaling
		\begin{itemize}
		\item the number scales the distance of vector (direction remains) \\
			$\Rightarrow$ such number thus also called scalar
		\item $\Rightarrow$ scale alone each axis by that scalar \\
			$2\mathbf x = 2x_1e_2+...+2x_ne_n$, \\ 
			where $e_1,...,e_N$ are vector defining coordinates
		\end{itemize}
	\end{itemize}

\item Linear Combination
	\begin{itemize}
	\item Vector Adding: Generalization of Numerical Adding
		\begin{itemize}
		\item in 1-$D$: joint movement along single axis
		\item in n-$D$: joint movement along each axis $\Rightarrow$ a joint movement in n-$D$ space
		\end{itemize}
	\item Definition: $\mathbf x = a_1\mathbf x_1 + ... + a_n\mathbf x_n$
		\begin{itemize}
		\item the vectors $\mathbf x_1,..., \mathbf x_n$ only altered linearly (as only being scaled) \\
			$\Rightarrow \mathbf x$ direction \& size are linear combination of that in $\mathbf x_1,..., \mathbf x_n$
		\end{itemize}
	\item Span of $\{\mathbf x_1, \mathbf x_2,...,\mathbf x_n\}$
		\begin{itemize}
		\item the $n$-D space $S$ constructed by linear combination of $\mathbf x_1, \mathbf x_2,...,\mathbf x_n$
		\end{itemize}
	\item $x_0$ linearly dependent on $\{x_1,.,,,x_n\}$ 
		\begin{itemize}
		\item $x_0$ can be constructed by linear combination of $\{x_1,.,,,x_n\}$ \\ 
		(already in the span space $S$)
		\item $\Leftrightarrow$ function $a_0\mathbf x_0 + ... + a_n\mathbf x_n=0$ has other solution than $a_0=...=a_n=0$
		\end{itemize}
	\item $x_0$ linearly INdependent with $\{x_1,.,,,x_n\}$	
		\begin{itemize}
		\item $x_0$ can NOT be constructed by linear combination of $\{x_1,.,,,x_n\}$ \\ 
		(not in the span space $S$, will increase the dimension of $S$ if adopted)
		\item $\Leftrightarrow$ function $a_0\mathbf x_0 + ... + a_n\mathbf x_n=0$ has and only has solution $a_0=...=a_n=0$
		\end{itemize}
	\end{itemize}

\item Special Vectors
	\begin{itemize}
	\item $n$-D Zero Vector $\mathbf x$
		\begin{itemize}
		\item 
		\end{itemize}
	\item Unit Vector
	\item Basis of Vector Space $S^{n}$
		\begin{itemize}
		\item general basis: a set of linearly independent vectors that span the space \\
		(i.e. a set of linearly independent $n$-D vectors)
		\item unit basis: a general basis where every vector is unit vector
		\item orthogonal basis: a general basis where all vectors are orthogonal with each other
		\item unit orthogonal basis: a basis that is also a unit basis and an orthogonal basis
		\end{itemize}
		$\Rightarrow$ coordinate: the scaler to composite a vector given a specific basis
	\end{itemize}
\end{itemize}

\subsubsection{Linear Transformation and Maps}
\begin{itemize}
\item Linear Transformation
	\begin{itemize}
	\item Transformation
		\begin{itemize}
		\item a function mapping: vector $\rightarrow$ vector
		\item a vector movement: scale \& rotate all possible input vectors (i.e. a vector space)
		\end{itemize}
	\item Transformation with Linearity $L(\cdot)$
		\begin{itemize}
		\item intuition: lines remain lines \& origin remains origin
		\item definition: a transformation $L(\cdot)$ is linear if
			\begin{itemize}
			\item additivity: $L(\mathbf x_1 + \mathbf x_2) = L(\mathbf x_1) + L(\mathbf x_2)$
			\item scaling: $L(a\mathbf x) = aL(\mathbf x)$, where $a$ is scaler
			\end{itemize}
		\end{itemize}
	\item Features of $L(\cdot)$ given $\mathbf x = x_1e_1 +...+ x_ne_n$
		\begin{itemize}
		\item same scaler for coordinates
		\begin{align*} \Rightarrow L(\mathbf x) &= L(x_1e_1 +...+x_ne_n) \\ &= L(x_1e_1)+...+L(x_ne_n) \\ &= x_1L(e_1)+...+x_nL(e_n) \end{align*}
		$\Rightarrow$ transformed vector $\mathbf x'=L(\mathbf x)$ has the same coord under the transformed basis
		\end{itemize}
	\end{itemize}

\item Linear Map
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item map $F:V\rightarrow X$ is a linear map if it is a linear transformation, \\
		where $V,X$ are vector spaces
		\end{itemize}
	\end{itemize}

\item Multilinear Maps
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item map $F:\underbrace{V\times...\times V}_{k \text{ copies}} \rightarrow X$ is multilinear/$k$-linear if it is linear in each slot \\
		i.e. $F(\mathbf v_1, ..., a\mathbf v_i+b\mathbf v_i',...,\mathbf v_k) = aF(\mathbf v_1, ..., \mathbf v_i,...,\mathbf v_k) + bF(\mathbf v_1, ..., b\mathbf v_i',...,\mathbf v_k)$ \\ 
		i.e. for fixed $\mathbf v_1,...,\mathbf v_{i-1},\mathbf v_{i+1},...,\mathbf v_{k}, F$ reduced to linear map with $\mathbf v_i$ as variable \\ 
		(where $V, X$ are vector spaces)
		\end{itemize}
	\item Alternating Maps
		\begin{itemize}
		\item map $F$ is alternating if, its output is $\mathbf 0$ whenever two vectors in inputs are identical
		\end{itemize}
	\item for Multilinear Map $F$: $F$ Alternating $\Leftrightarrow F(...,\mathbf v,...,\mathbf w, ...)=-F(...,\mathbf w,...,\mathbf v, ...)$ \\
	i.e. for multilinear map $F$, $F$ alternating $\Leftrightarrow$ swapping two inputs flips sign of output
		\begin{itemize}
		\item proof: given multilinear and alternating map $F$, for any $\mathbf v, \mathbf w$
		\begin{align*}
		0 &= F(..., (\mathbf v+\mathbf w), ..., (\mathbf v+\mathbf w), ...) \\
		&= F(...,\mathbf v, ..., \mathbf v, ...) + F(...,\mathbf w, ..., \mathbf w, ...) + F(...,\mathbf v, ..., \mathbf w, ...) + F(...,\mathbf w, ..., \mathbf v, ...) \\
		&= F(...,\mathbf v, ..., \mathbf w, ...) + F(...,\mathbf w, ..., \mathbf v, ...)
		\end{align*}
		$\Rightarrow F(...,\mathbf v, ..., \mathbf w, ...) = -F(...,\mathbf w, ..., \mathbf v, ...)$
		\item proof: given multilinear map $F: F(...,\mathbf v, ..., \mathbf w, ...) = -F(...,\mathbf w, ..., \mathbf v, ...)$ \\
		$\Rightarrow F(...,\mathbf v, ..., \mathbf v, ...) = -F(...,\mathbf v, ..., \mathbf v, ...)$ \\
		$\Rightarrow F(...,\mathbf v, ..., \mathbf v, ...) = 0$, hence alternating
				
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Matrix}
\begin{itemize}
\item Matrix for Linear Transformation $L : S\rightarrow S'$
	\begin{itemize}
	\item Representing Space Transformation
		\begin{itemize}
		\item package the transformed basis under the original basis using matrix $M$ \\
		i.e. represent the $e'_1,...,e'_n = L(e_1),...,L(e_n)$ under the original basis $e_1,...,e_n$
		$\Rightarrow M = [e'_1, ..., e'_n]$, with all transformed basis as column vectors \\
		$\Rightarrow M$ represent the result of linear transformation for the basis of $S$
		\item hence, determine a linear space transformation $L:S\rightarrow S'$ using $e_1,...,e_n$ \\
		for $M_{m\times n}$ matrix: a linear transformation from $n$-D space to $m$-D space
			\begin{itemize}
			\item $m<n$: projecting to subspace
			\item $m>n$: expanding into a hyper-plane/-line/etc (constrained in hyper-space)
			\end{itemize}
		 
		\end{itemize}
	\item Performing Space Transformation
		\begin{itemize}
		\item $\displaystyle \forall i\in \{1,...,n\}, x'_i = i^\text{th}$ component of $\mathbf x'=L(\mathbf x)$, then $x'_i = (e'_{1i}+...+e'_{n_i})x_i$ \\
		(as proved above)
		\item $\Rightarrow$ output vector $\mathbf x' = L(\mathbf x)=M\mathbf x$ under the original basis $e_1,...,e_n$ \\
		hence the rule for matrix multiplication
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth,center]{./Math/"matrix-performing space transformation".png}
		\caption*{where green and red columns the $\{e_1',e_2'\}$ under $\{e_1,e_2\}$, $[x,y]$ the input vector $\mathbf x$}
		\end{figure}
		\end{itemize}
	\end{itemize}

\item Matrices for Composition of Linear Transformation
	\begin{itemize}
	\item Transformation $L_1, L_2$ as Matrix $M_1, M_2$
		\begin{itemize}
		\item as easy to prove $M_2\cdot (M_1 \cdot \mathbf x) = (M_1\cdot M_2) \cdot \mathbf x$ \\
		$\Rightarrow L_2(L_1(\cdot))$ $\Leftrightarrow$ the composition of transformation defined by $M_2\cdot M_1$
		\item $\Rightarrow \mathbf x$ first transformed by $L_1$ then $L_2 \Leftrightarrow$ transformed by $M_2\cdot M_1$ 
		\end{itemize}
	\item Linear Transformation on Space
		\begin{itemize}
		\item given a basis matrix $E = [e_1,...,e_n]$, a transformed basis $L_1(E) = L_1(e_1), ..., L_1(e_n)$ \\
		($e_1,...,e_n$ as column vectors) \\
		$\Rightarrow L_2(L_1(e_k))$ performs $L_2$ transformation on the $k^\text{th}$ vector of $L_1(E)$
		\item hence to perform $L_2$ on the transformed space $L_1(E) \Rightarrow L_2(L_1(E))$
		\item given that $L_1(E) = M_1 \Rightarrow L_2(L_1(E)) = M_2 \cdot M_1$ \\ 
		(due to the derivation of linear transformation as matrix)
		\item hence, $M_2\cdot M_1$ denotes 
			\begin{itemize}
			\item a further $L_2$ transformation on a transformed space $L_1(E)$ \\
			(all result represented under the original basis $E$)
			\item the final transformed basis (first $L_1$ then $L_2$) under original basis $E$ \\
			$\Rightarrow$ denotes a composite transformation of $L_2(L_1(\cdot))$
			\end{itemize}
		
		\end{itemize}
	\item Multiplication between Matrices
		\begin{itemize}
		\item $\Rightarrow$ composite linear transformation\textbf{s} into single linear transformation
		\item $\Rightarrow$ linear transformation on vector\textbf{s}/space (generalized from single vector)
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth,center]{"./Math/matrix-composition of transformations".png}
		\caption*{where $M_1=[L_1(e_1), L_1(e_2)], M_2=[L_2(e_1), L_2(e_2)]$, the result $=[L_2(L_1(e_1)), L_2(L_1(e_2))]$ \\ (all under basis $E=[e_1,e_2]$)}
		\end{figure}
		\end{itemize}
	\end{itemize}
	
\item Understanding Properties
	\begin{itemize}
		\item $(AB)C = A(BC)$
		\begin{itemize}
			\item both meaning apply transformation $C$ then $B$ then $A$...
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsubsection{Inverse of Matrix}
\begin{itemize}
\item Inverse of Linear Transformation in Square Matrix
	\begin{itemize}
	\item Interpretation
		\begin{itemize}
		\item a transformation $L^{-1}$ to inverse the effect of another transformation $L$ \\
		$\Rightarrow \forall \mathbf x, L^{-1}L(\mathbf x) = \mathbf x$
		\item $\Rightarrow M^{-1}M = I$, where $M$ for $L$, $M^{-1}$ for $L^{-1}$, $I$ the identity matrix
		\end{itemize}
	\item Requirement for $M_{n\times n}$
		\begin{itemize}
		\item transformed basis in $M$ still span the space! \\
		(otherwise, transformed vectors projected onto subspace \& can NOT be reversed)
		\item $\Rightarrow$ column vectors (transformed basis) in $M_{n\times n}$ linearly INdependent
		\end{itemize}
	\end{itemize}
\item Non-Square Matrix
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item left inversion: $M^{-1}_{n\times m}\cdot M_{m\times n} = I_{n\times n}$
		\item right inversion: $M_{m\times n}\cdot M^{-1}_{n\times m} = I_{m\times m}$
		\end{itemize}	
	\end{itemize}
\end{itemize}

\subsubsection{Rank and Dimension}
\begin{itemize}
\item Dimension
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item the number of linearly INdependent vectors in a vector space \\
		$\Rightarrow$ the least number of vector required to span the space
		\end{itemize}
	\item Dimension of Column Space
		\begin{itemize}
		\item $\Rightarrow$ the linearly independent col vectors \\
		i.e. the rank of $M$
		\end{itemize}
	\end{itemize}

\item Measuring Linear Transformation on Dimension
	\begin{itemize}
	\item Measuring the Transformed Space
		\begin{itemize}
		\item rank of $M: R(M)=r$: basis in $M$ span a $r$-dimension (column) space \\
		note: the column space of $M$: transformed space defined by column basis
		\item full rank: the dimension of column space is as high as possible
		\end{itemize}
	\end{itemize}
	
\item Null Space (Kernel) of $M_{m\times n}$
	\begin{itemize}
	\item Vectors in Null Space
		\begin{itemize}
		\item $\forall \mathbf x, M\mathbf x = \mathbf 0$ \\
		(i.e. all the vectors in null space transformed into $\mathbf 0$ by $M$, hence the name)
		\item $\mathbf 0$ always in null space
		\end{itemize}
	\item Dimension of Null Space
		\begin{itemize}
		\item $D(\text{null space}) = n - R(M)$ \\
		\item understanding: $R(M)$ vectors chosen for basis of col space of $M$ \\ 
		$\Rightarrow \mathbf x$ able to freely combine the remaining col vectors, with $M\mathbf x=0$ still satisfied
		\end{itemize}
	\end{itemize}
	
\item Understanding Properties
	\begin{itemize}
	\item $R(M_{m\times n}) \leq \min\{m, n\}$
		\begin{itemize}
		\item $m<n$: projecting a $n$-D vector to a $m$-D subspace, hence at most of rank $m$
		\item $m>n$: $M$ consists of $n$ vectors of $m$ dimensions, define at most $n$-D space
		\end{itemize}
	\item $R(M_{m\times n}) = \min\{m, n\} \Leftrightarrow M$ Full Rank
		\begin{itemize}
		\item from definition, it is as high as possible
		\item $m$ vectors with $n$ dimensions, span at most a $\min\{m, n\}$ space \\ 
		either linearly dependent ($m<n$), or not enough independent vectors ($m>n$)
		\end{itemize}
	\end{itemize}
	
\item Linear Dependency
	\begin{itemize}
	\item $R(M_{m\times n})=\min\{m,n\}$ (Full Rank)
		\begin{itemize}
		\item $m=n$: $M$ col vectors linear INdependent as $n$ vector spans an $N$-D space
		\item $m<n$: $M$ col vectors linear dependent as $n$ vector spans an $M$-D subspace
		\item $m>n$: $M$ col vectors linear INdependent as $n$ vector spans at least an $N$-D space
		\end{itemize}
	\item $R(M_{m\times n}) < \min\{m,n\}$
		\begin{itemize}
		\item $m\le n$: $M$ col vector linear dependent
		\item $m > n$: $M$ col vector MAY be linear independent/dependent
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Dot Product}
\begin{itemize}
\item Projecting to 1-D
	\begin{itemize}
	\item Unit Orthogonal Basis
		\begin{itemize}
		\item $\mathbf x\cdot \mathbf y = x_1y_1 +...+ x_ny_n = \mathbf x^T \cdot \mathbf y = \mathbf y^T\cdot \mathbf x$ \\
		($\mathbf x,\mathbf y$ assumed to be column vector / matrix with single column)
		\end{itemize}
	\item General Basis $E=[e_1,...,e_n]$
		\begin{itemize}
		\item $\Rightarrow \mathbf x = x_1e_1+...+x_ne_n=E\mathbf x, \mathbf y = y_1e_1+...+y_ne_n = E \mathbf y$ \\
		$\Rightarrow \mathbf x\cdot \mathbf y = (E\mathbf x) \cdot (E \mathbf y) = \mathbf x^TE^TE\mathbf y$
		\item understanding: 
			\begin{enumerate}
			\item $\mathbf x = E\mathbf x$: transfer back to a representation under unit orthogonal basis
			\item for $E=I$, back to the unit orthogonal case
			\end{enumerate}
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item each basis $e_i\in E$ projected to $1$-D space (a scalar) by transformation $\mathbf x/\mathbf y$
		\item $\Rightarrow \mathbf x\cdot \mathbf y$: projecting $\mathbf x/\mathbf y$ to $1$-D line using transformation $\mathbf y/\mathbf x$ \\ 
		(direction/scaling effect of projection can be alter by choice of basis $E$ though)
		\end{itemize}
	\end{itemize}
\item Duality
	\begin{itemize}
	\item Dual Vector
		\begin{itemize}
		\item the vector $\in\mathbb R^n$ represent a projection (linear transformation) to $1$-D line \\
		(hence the vector equivalent to the matrix with 1 row defining the projection)
		\item $\Rightarrow$ performing transformation on a vector $\Leftrightarrow$ taking product with the dual vector
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Exterior (Wedge) Product}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $k$th exterior product $\wedge^kV$ is a vector space, with a map $\psi:\underbrace{V\times...\times V}_{k \text{ times}} \rightarrow \wedge^kV$, \\
		where $V\in \mathbb R^n$ a vector space, map $\psi$ the exterior multiplication \\ 
		(note: $\psi(\mathbf v_1,...,\mathbf v_k) = \mathbf v_1\wedge...\wedge\mathbf v_k$, also called $k$-blade or $k$-form)
		\end{itemize}
	\item Properties of Space $\wedge^kV$
		\begin{itemize}
		\item $\psi$ is alternating multilinear map
		\item for basis $\{e_1,...,e_n\}$ of $V\in\mathbb R^n$ \\ 
		$\Rightarrow \{e_{i_1}\wedge ...\wedge e_{i_k}|1\leq i_1 <...< i_k \leq n\}$ a basis of $\wedge^kV$ \\
		$\Rightarrow e_{i_1}\wedge ...\wedge e_{i_k}$ can form any permutation of the same input by swapping order \\
		(due to its alternating multilinearity) \\
		e.g. for $\wedge^2V: e_1\wedge e_1 = e_2\wedge e_2=\mathbf 0,\space e_1\wedge e_2=-e_2\wedge e_1 \Rightarrow$ not linearly independent
		\item sum of $k$-wedge is still in the vector space $\wedge^k V$ (as can be expressed by its basis)
		\item $F:\underbrace{V\times...\times V}_k\rightarrow X$ uniquely factors into $\underbrace{V\times...\times V}_k\xrightarrow{\psi} \wedge^k V \xrightarrow{\text{ }\underline F\text{ }} X$, \\ 
		where $F$ any alternating multilinear map, $\psi$ exterior multiplication, $\underline F$ linear map \\
		(since $\psi$ also an alternating multilinear map \& $\underline F$ to preserve linearity)
		\end{itemize}
	\item Inferred Propositions
		\begin{itemize}
		\item with $V\in\mathbb R^n$, dim $\wedge^kV=\left(\begin{smallmatrix} n \\ k \end{smallmatrix}\right) = C^k_n$, due to the form of its basis \\ 
		note: for $k > n$, $\wedge^kV=\{0\}$ \\
		(top exterior power)
		\item $k$-blade $\mathbf v_1\wedge...\wedge\mathbf v_k=0 \Leftrightarrow \mathbf v_1,...,\mathbf v_k$ linearly dependent \\
		($k$-blade $\mathbf v_1\wedge...\wedge\mathbf v_k \neq 0 \Leftrightarrow \mathbf v_1,...,\mathbf v_k$ linearly INdependent)
			\begin{itemize}
			\item if $\mathbf v_1,...,\mathbf v_k$ linearly dependent $\Rightarrow \mathbf v_1\wedge...\wedge\mathbf v_k=0$ as $\psi$ alternating
			\item if $\mathbf v_1\wedge...\wedge\mathbf v_k=0$, assume $\mathbf v_1,...,\mathbf v_k$ INdependent \\
			$\Rightarrow \mathbf v_1\wedge...\wedge\mathbf v_k$ a basis of $\wedge^kV \Rightarrow$ basis $=0$ \\
			$\Rightarrow$ assumption failed, $\mathbf v_1,...,\mathbf v_k$ linearly dependent
			\end{itemize}
			(wedge dependence lemma)
		\item $\mathbf v_1\wedge...\wedge \mathbf v_k = c(\mathbf w_1\wedge...\wedge \mathbf w_k) \Leftrightarrow \text{span}(\mathbf v_1, ..., \mathbf v_k)=\text{span}(\mathbf w_1, ..., \mathbf w_k)$, \\ 
		where $\mathbf v_1\wedge...\wedge \mathbf v_k, \mathbf w_1\wedge...\wedge\mathbf w_k\in\wedge^kV$ \& non-zero, scalar $c \neq 0$
			\begin{itemize}
			\item if $\text{span}(\mathbf v_1, ..., \mathbf v_k)=\text{span}(\mathbf w_1, ..., \mathbf w_k)$ \\
			$\Rightarrow$ each $\mathbf v_1, ..., \mathbf v_k$ expressed by a linear combination of $\mathbf w_1, ..., \mathbf w_k$ \\
			$\Rightarrow \mathbf v_1\wedge...\wedge \mathbf v_k = c(\mathbf w_1\wedge...\wedge \mathbf w_k)$ as $\psi$ alternating multilinear \\
			($c\neq0$ as both pure wedges non-zero)
			\item if $\mathbf v_1\wedge...\wedge \mathbf v_k = c(\mathbf w_1\wedge...\wedge \mathbf w_k)$ \\ 
			let $U_v = \text{span}(\mathbf v_1,...,\mathbf v_k), U_w = \text{span}(\mathbf w_1,...,\mathbf w_k), U_{vw} = U_v\cap U_w$ \\
			$\Rightarrow$ assume $U_v \neq U_w$, $U_{vw}$ has dimension $l$\\
			$\Rightarrow \exists l$ vectors in $\{\mathbf v_1,...,\mathbf v_k\}, \{ \mathbf w_1,...,\mathbf w_k\}$ as 2 sets of basis of $U_{vw}$ \\
			$\Rightarrow$ change $l$ vectors to be a common basis for $U_{vw}$: $\{ \mathbf u_1,...,\mathbf u_l \}$ \\
			$\Rightarrow$ now $\mathbf v_1,...,\mathbf v_{k-l}, \mathbf u_1,...,\mathbf u_l, \mathbf w_1,...,\mathbf w_{k-l}$  linearly independent \\
			$\Rightarrow$ $\psi(v_1,...,\mathbf v_{k-l}, \mathbf u_1,...,\mathbf u_l) \psi(\mathbf w_1,...,\mathbf w_{k-l}, \mathbf u_1,...,\mathbf u_l)$ 2 different basis for $\wedge^kV$ \\
			while $\psi(\mathbf v_1,...,\mathbf v_{k-l}, \mathbf u_1,...,\mathbf u_l) = c_v\psi(\mathbf v_1,...,\mathbf v_{k})$ \\ 
			similarly $\psi(\mathbf w_1,...,\mathbf w_{k-l}, \mathbf u_1,...,\mathbf u_l) = c_w\psi(\mathbf w_1,...,\mathbf w_{k})$ \\
			$\Rightarrow \psi(\mathbf v_1,...,\mathbf v_{k}) \neq c \psi(\mathbf w_1,...,\mathbf w_{k}), (c\neq0)$ (as basis linearly independent) \\
			$\Rightarrow$ conflict, assumption failed, hence $\text{span}(\mathbf v_1,...,\mathbf v_k) = \text{span}(\mathbf w_1,...,\mathbf w_k)$
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item Induced Maps
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsubsection{Determinant}
\begin{itemize}
\item Construction from Exterior Product
	\begin{itemize}
	\item Definition
	\item Independence of Basis
	\item Multiplicativity
	\item Relationship to Invertibility
	\end{itemize}
\item Measuring Linear Transformation on Volume
	\begin{itemize}
	\item Unit Volume
		\begin{itemize}
		\item unit volume $v=\norm{e_1 \extp ...\extp e_n}$ with basis $e_1,...,e_n$ \\
		(for orthogonal basis, $v=\norm{e_1}\times...\times\norm{e_n}$)
		\item after transformation: $L(v) = \norm{L(e_1)\extp...\extp L(e_n)} = \norm{Me_1\extp...\extp Me_n}$
		\end{itemize}
		where $\extp$ is the exterior product
	\item Measuring Change of Unit Volume
		\begin{itemize}
		\item $\Rightarrow \det(M) = \frac {L(v)}{v} = \norm{M_0\times...\times M_n}$, \\
		assuming unit orthogonal basis $E=I$, where $I$ is identity matrix \\
		(note: interpretation of $\det(\cdot) \leftrightarrow$ spacial interpretation of cross product $\times$)
		\item hence, the rule of calculating $\det(\cdot)$ \\
		\end{itemize}
	\end{itemize}
	
\item Measuring Change of Orientation
	\begin{itemize}
	\item Orientation of Space
		\begin{itemize}
		\item jointly defined by the direction \& order of the sequence of basis vector \\
		i.e. the positive/negative part of each axis in sequence
		\end{itemize}
	\item Measuring the Change
		\begin{itemize}
		\item $\det(M) < 0$ if axises flipped over once (for an odd times) \\
		$\det(M) > 0$ if flipped for even times
		\item interpretation: the flipped axis approaches $0$ then expanded into the negative \\
		(measured by original basis $E$)
		\end{itemize}
	\end{itemize}
	
\item Linear Dependency
	\begin{itemize}
	\item $\det(M) = 0$
		\begin{itemize}
		\item volume in current space becomes 0 \\
		$\Rightarrow$ dimensions decreases after transformation applied \\
		$\Rightarrow$ i.e. transformed basis not able to span the current space
		\item $\Rightarrow$ basis in $M$ NOT linearly INdependent! $\Leftrightarrow \det(M)=0$
		\end{itemize}
	\item $\det(M) \neq 0$
		\begin{itemize}
		\item volume still exist \\
		$\Rightarrow$ dimensions remain \& transformed basis still span the space
		\item $\Rightarrow$ basis in $M$ is linearly INdependent $\Leftrightarrow \det(M) \neq 0$
		\end{itemize}
	\end{itemize}

\item Understanding Properties
	\begin{itemize}
	\item $\det(M_1M_2) = \det(M_1)\det(M_2)$:
		\begin{itemize}
		\item left: final volume \& orientation changed after transformation $M_2$ then $M_1$
		\item right: the volume scaled by one transformation, then further by the other; \\ 
		(similar for orientation, as measured by sign)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Cross Product}
\begin{itemize}
\item Calculation via Determinant
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item for $\mathbf v_1,...,\mathbf v_{n-1}\in\mathbb R^n, \mathbf v_1\times...\times \mathbf v_{n-1}=\det\left( \begin{smallmatrix} \text{--- }& \mathbf v_1 &\text{ ---} \\ &\vdots& \\ \text{--- }& \mathbf v_{n-1} &\text{ ---} \\ e_1 & \dots & e_n \end{smallmatrix} \right)$
		\item construct matrix with each vectors as row \& basis $e_1,...,e_n$ as the last row \\ 
		$\Rightarrow$ an $N\times N$ matrix (fake $e_1,...,e_n$ as number) \\
		$\Rightarrow$ re-combined into a vector $\mathbf w=\mathbf v_1\times...\times \mathbf v_{n-1}\in\mathbf R^n$
		\item actually, carry out the hodge star operator implicitly (via determinant)
		\end{itemize}
	\item Direction
		\begin{itemize}
		\item perpendicular to the $\text{span}(\mathbf v_1,...,\mathbf v_{n-1})$
		\end{itemize}
	\end{itemize}
\item Understanding of Calculation via Determinant
	\begin{itemize}
	\item Equivalent Linear Transformation
		\begin{itemize}
			\item view the constructed $\det(\cdot)$ as transformation $F:(\mathbf v_1,...,\mathbf v_{n-1}, \mathbf x) \rightarrow y \in R$ \\
			$\Rightarrow$ a linear transformation to $1$-D number line \\
			$\Rightarrow \exists \mathbf p\in\mathbb R^n, y=F(\mathbf x) = \mathbf p \cdot \mathbf x$ (due to duality of dot product)
			\item note that: transformation $\mathbf p$ (or $F$) is generated by vector $\mathbf v_1,...,\mathbf v_{n-1}$
		\end{itemize}
	\item Algebraic View
		\begin{itemize}
		\item view variable $\mathbf x=x_1,...,x_n$ as the coefficient for each basis $e_1,...,e_n$
		\item $\Rightarrow \mathbf p$ describes the linear transformation of $F$ on the vector space \\
		(on arbitrary vector $\mathbf x$)
		\item $\Rightarrow$ dual vector $\mathbf p=\mathbf v_1\times...\times \mathbf v_{n-1}$
		\end{itemize}
	\item Geometric View
		\begin{itemize}
		\item as $\det(\mathbf v_1,...,\mathbf v_{n-1}, \mathbf x)$ measures their high-dimension volume $y$ \\
		$\Rightarrow y = \mathbf p \cdot\mathbf x = \norm{\mathbf p}\cdot\mathbf x_{\parallel \mathbf p}$, \\
		where $\mathbf x_{\parallel \mathbf p}$ the component of $\mathbf x$ parallel to $\mathbf p$
		\item while $y = \text{volume}(\mathbf v_1,...,\mathbf v_{n-1})\cdot \mathbf x_{\perp \text{span}(\mathbf v_1,...,\mathbf v_{n-1})}$ (integration perspective), \\
		where $\mathbf x_{\perp \text{span}(\mathbf v_1,...,\mathbf v_{n-1})}$ the component of $\mathbf x$ perpendicular to $\text{span}(\mathbf v_1,...,\mathbf v_{n-1})$
		\item $\Rightarrow$ find a $\mathbf p$, s.t. $\forall \mathbf x, \det(\mathbf v_1,...,\mathbf v_{n-1})\cdot \mathbf x_{\perp \text{span}(\mathbf v_1,...,\mathbf v_{n-1})} = \norm{\mathbf p}\cdot\mathbf x_{\parallel \mathbf p}$
		\item $\Rightarrow$ only solution: $\mathbf p\perp\text{span}(\mathbf v_1,...,\mathbf v_{n-1})$ and $\norm{\mathbf p} = \det(\mathbf v_1,...,\mathbf v_{n-1})$ \\ 
		(naturally accounting for signed volume via direction of $\mathbf p$)
		\end{itemize} 
	\end{itemize}
\end{itemize}

\subsubsection{Linear System of Equations}
\begin{itemize}
\item Linearity
	\begin{itemize}
	\item Linear Combination of Variables
		\begin{itemize}
		\item coefficients matrix $A$: holding the coefficient for each equation (in row)
		\item variables vector $\mathbf x$: holding variables as column vector
		\item constants vector $\mathbf v$: holding target constant for each equations as column vector
		\end{itemize}
		$\Rightarrow A\mathbf x = \mathbf v$ for a set of linear equations
	\item Linear Transformation Perspective
		\begin{itemize}
		\item $\mathbf x$/$\mathbf v$ as original/transformed vectors
		\item columns of $A$ (coefficients for the same variable) as transformed basis
		\end{itemize}
		$\Rightarrow$ finding a start position $\mathbf x$ which, after transformation $A$, lands on $\mathbf v$		
	\end{itemize}

\item Existence of Solution(s)
	\begin{itemize}
	\item Transformed Basis Linearly INdependent
		\begin{itemize}
		\item single solution exists, as $A\mathbf x=\mathbf v \Leftrightarrow \mathbf x = A^{-1}\mathbf v$ \\
		(use the inverse transformation $A^{-1}$ to find the input $\mathbf x$ using output $\mathbf v$) \\
		$\Rightarrow$ single unique $\mathbf x$ found
		\end{itemize}
	\item Transformed Basis Linearly Dependent ($\det(A) = 0$)
		\begin{itemize}
		\item multiple solutions: transformed basis in $A$ linearly dependent with $\mathbf v$ \\
		$\Rightarrow$ i.e. $\mathbf v$ in the column space of $A$ \\
		(special case where $\mathbf v=\mathbf 0$: solution space = null space of $A$)
		\item no solution: basis in $A$ linearly INdependent with $\mathbf v$ \\
		$\Rightarrow$ i.e. can NOT possibly be described by transformed basis in $A$
		\end{itemize}
	\end{itemize}

\item Cramer's Rule
	\begin{itemize}
	\item Basis Represented by Determinant
		\begin{itemize}
		\item $\forall i = {1,...,n},\det(e_1,...,e_{i-1}, \mathbf x, e_{i+1},...e_n) = \mathbf x_{\parallel e_i}$, \\
		where $\mathbf x_{\parallel e_i}$ the component of $\mathbf x$ on basis $e_i$ (as the $i^\text{th}$ coord of $\mathbf x$)
		\item similarly, $\det(a_1,...,a_{i-1}, \mathbf v, a_{i+1}, ..., a_n) = \mathbf v_{\parallel a_n}$, \\
		where $a_i$ the $i^\text{th}$ col vector of $A$ \\
		$\Rightarrow$ represent the $i^\text{th}$ coord of transformed vector $\mathbf v$ under transformed basis $A$
		\end{itemize}
	\item Transformation Described by Determinant
		\begin{itemize}
		\item $\det(A)$ measures the change of unit volume from $I$ to $A$
		\item $\det(e_1,...,e_{i-1}, \mathbf x, e_{i+1},...e_n) $ the volume before transformation
		\item $\det (a_1,...,a_{i-1}, \mathbf v, a_{i+1}, ..., a_n)$ the volume after transformation
		\item  $\Rightarrow \det (a_1,...,a_{i-1}, \mathbf v, a_{i+1}, ..., a_n) = \det(e_1,...,e_{i-1}, \mathbf x, e_{i+1},...e_n) \cdot \det(A)$
		\end{itemize}
	\item Solving $\mathbf x$
		\begin{itemize}
		\item hence, $\displaystyle \mathbf x_{\parallel e_i} = \det(e_1,...,e_{i-1}, \mathbf x, e_{i+1},...e_n) = \frac {\det (a_1,...,a_{i-1}, \mathbf v, a_{i+1}, ..., a_n)}{\det(A)}$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Eigenvector and Eigenvalue}
\begin{itemize}
\item Invariability in Linear Transformation
	\begin{itemize}
	\item Invariable Line
		\begin{itemize}
		\item vector before and after transformation $A$ are on the same \textbf{line} \\
		$\Rightarrow A\mathbf x = \lambda\mathbf x$ (under the same basis) \\
		$\Rightarrow$ transformation $A$ NOT rotate those $\mathbf x$, but only stretch them, by a factor $\lambda$
		\item $\mathbf x$ the eigenvector for those invariable line; \\ 
		$\lambda$ the eigenvalue for corresponding stretching factor
		\end{itemize}
	\item Representing Transformation
		\begin{itemize}
		\item for rotation, the eigenvector\&value can be more expressive than a matrix
		\end{itemize}
	\item Solving Eigenvector\&value
		\begin{itemize}
		\item solving $A\mathbf x = \lambda \mathbf x$ for $\mathbf x$ and $\lambda$ \\
		$\Rightarrow$ equivalent with solving $(A-\lambda I)\mathbf x = \mathbf 0$
		\item a necessary condition for non-zero solution $\mathbf x$: $\det (A-\lambda I) = 0$ \\
		(understanding 0: as solution existence condition for linear system of equations) \\
		(understanding 1: non-zero $\mathbf x$ transformed by $A-\lambda I$ into $\mathbf 0$) \\
		(understanding 2: non-zero $\mathbf x$ perpendicular to all row vectors in $A-\lambda I$) \\
		(understanding 3: otherwise, $\mathbf x = (A-\lambda I)^{-1}\mathbf 0 = \mathbf 0$)
		\item $\Rightarrow$ with no/single/multiple $\lambda\in\mathbb R$ solved, bring back to relation $(A-\lambda I)\mathbf x = 0$ \\ 
		$\Rightarrow$ to solve for a line/span, consisting of eigenvector(s) \\ 
		(note: no $\lambda\in\mathbb R$ solved, no eigenvector then)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Translation of Basis}
\begin{itemize}
\item Translation between Coordinate
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $\mathbf x=(x_1,...,x_n)$: a coordinate under basis $A_{n\times n}$
		\item $\mathbf b=(b_1,...,b_n)$: a coordinate under basis $E=I$
		\item $\overrightarrow {v}$: the same vector described by $\mathbf x$ \& $\mathbf b$
		\item $A_{n\times n}$: the transformed basis described by basis before transformation (i.e. $E$) \\
		(with col vectors of $A$ linearly INdependent)
		\item $A_{n\times n}^{-1}$: the transformed basis s.t. $A^{-1}AE=E$ \\ 
		$\Rightarrow$ describe transformation $A\rightarrow E$ by basis before transformation (i.e. $AE=A$)
		\item $M_{n\times n}$: a matrix for arbitrary transformation $L:E\rightarrow ?$, described by basis $E$
		\end{itemize}
	\item Translating Coordinate
		\begin{itemize}
		\item $A\mathbf x$: describe $\overrightarrow {v}$ by basis $E$ \\
		$\Rightarrow$ translate the vector coord from basis $A$ to basis $E$
		\item $A^{-1}\mathbf b$: describe $\overrightarrow {v}$ by basis $A$ \\
		$\Rightarrow$ translate the vector coord from basis $E$ to basis $A$
		\item $\Rightarrow A\mathbf x = \mathbf b$
		\end{itemize}
	\item Translating Transformation
		\begin{itemize}
		\item translate coord $\mathbf x$ (under $A$) to be under $E=I$: $A\mathbf x$ \\
		(translate to the basis where $M$ known)
		\item transform the vector by $M$: $MA\mathbf x$ (under basis $E$)
		\item translate the vector back to be under basis $A$: $A^{-1}MA\mathbf x$
		\item $\Rightarrow \mathbf x' = A^{-1}MA\mathbf x$, where $A^{-1}MA$ the transformation $L$ described under basis $A$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Diagonal Matrix}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item must be a square matrix
		\item non-zero values allowed only on the diagonal of a matrix
		\end{itemize}
	\end{itemize}
\item Transformation in Diagonal Matrix
	\begin{itemize}
	\item Interpretation
		\begin{itemize}
		\item as the transformation only stretch original basis $I$ by diagonal value \\
		$\Rightarrow$ all transformed basis is eigenvectors, with eigenvalue the diagonal value!
		\end{itemize}
	\item Eigenbasis
		\begin{itemize}
		\item the basis formed by eigenvectors of a matrix $A_{n\times n}$ \\
		\end{itemize}
	\end{itemize}
\item Diagonalizing Matrix
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item for transformation $A_{n\times n}$ under basis $I$, solve all eigenvalues $\lambda = [\lambda_1,...,\lambda_m]$
		\item if $m=n$, check all eigenvectors $Q = [\mathbf v_1,...,\mathbf v_m]$ to be linearly INdependent
		(i.e. check if eigenvectors able to form a valid basis for $n$-D space) \\
		$\Leftrightarrow$ check there are $n$ distinct eigenvalues $u$
		\item if so, $A$ said to be \textbf{diagonalizable}
		\item $\Rightarrow$ reconstruct transformation $A$ under eigenbasis (view $V$ as transformed basis) \\
		$\Rightarrow Q^{-1} A Q = \Lambda$, or $A = Q\Lambda Q^{-1}$, where $\Lambda=\text{diag}[\lambda_1,...,\lambda_n]$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item numerical proof: $Q^{-1}AQ = Q^{-1}[\lambda_1 \mathbf v_1,...,\lambda_n\mathbf v_n] = \Lambda$
		\item translating transformation $A$ into a transformation $A'$ under eigenbasis \\
		$\Rightarrow$ for $\mathbf x$, transformed into eigenbasis $Q^{-1}\mathbf x$ \\
		$\Rightarrow$ perform $k$ transformation in eigenbasis $\Lambda^kQ^{-1}\mathbf x$ \\
		$\Rightarrow$ then transformed back $Q\Lambda^kQ^{-1} \mathbf x$
		\item $\Rightarrow$ easy to perform any diagonalizable transformation
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Interchanging Coordinates}
\subsubsection{N-dimensional Spherical Coordinates}
\begin{itemize}
\item Notation
	\begin{itemize}
	\item $N$-dim Euclidean Space $E_N$
		\begin{itemize}
		\item $e_1, e_2,...,e_N$: a group of orthonormal basis of $E_N$
		\item $\mathbf x = (x_1, x_2,...,x_N)$: vector in $E_n$
		\item $\mathbf x_{i-N}$: projection of $\mathbf x$ onto subspace spanned by $e_i,...,e_N$ \\
			$\displaystyle \Rightarrow \mathbf x_{i-N} = \sum_{n=i}^N x_ne_n$
		\end{itemize}
	\item Spherical Coordinates
		\begin{itemize}
		\item $r=\norm{\mathbf x}$: the norm of $\mathbf x$
		\item $\phi_i \in [0,\pi]$: angle between $\mathbf x_{i-N}$ and $e_i$
		\item $r_i=\norm{\mathbf x_{i-N}}$: norm of projection $\mathbf x_{i-N}$, with $r_1 = r$
		\end{itemize}
	\end{itemize}
	
\item Observation
	\begin{itemize}
	\item Space $e_1,...,e_N$:
		\begin{itemize}
		\item $\cos \phi_1 = \frac{\mathbf x e_1}{\norm{\mathbf x} \norm{e_1}} =\frac {x_1}{r} $ \\ 
			$\Rightarrow x_1 = r\cos\phi_1$ \\
			$\displaystyle \Rightarrow \mathbf x = r\cos\phi_1 e_1 + \sum_{n=2}^Nx_ne_n$
		\end{itemize}
	\item Space $e_2,...,e_N$:
		\begin{itemize}
		\item from above: $\displaystyle \mathbf x^2 = r^2\cos^2\phi_1 + \sum_{n=2}^N x_n^2 = r^2 $ \\ 
			$\displaystyle \Rightarrow \sum_{n=2}^N x_n^2 = r^2\sin^2\phi_1$
		\item $\displaystyle \mathbf x_{2-N} = \sum_{n=2}^Nx_ne_n$ \\
		$\displaystyle \Rightarrow \begin{cases} \displaystyle  \norm {\mathbf x_{2-N}}^2 = \sum_{n=2}^Nx_n^2 = r^2\sin^2\phi_1 = r_2^2 \\ \displaystyle \cos\phi_2 = \frac {\mathbf x_{2-N}\cdot e_2}{\norm{\mathbf x_{2-N}}\norm{e_2}} = \frac{x_2}{r_2} \end{cases}$ \\
		$\Rightarrow \begin{cases} r_2 = r\sin\phi_1 & (\text{ as }\phi_1\in [0,\pi]) \\ x_2 = r_2\cos\phi_2 = r\sin\phi_1\cos\phi_2 \end{cases}$ \\
		$\displaystyle \Rightarrow \mathbf x_{2-N} = r\sin\phi_1\cos\phi_2e_2 + \sum_{n=3}^Nx_ne_n$ \\
		\end{itemize}
	\item Space $e_3,...,e_N$:
		\begin{itemize}
		\item from above: $\displaystyle \mathbf x_{2-N}^2 = r^2\sin^2\phi_1\cos^2\phi_2 + \sum_{n=3}^{N}x_n^2 = r^2\sin^2\phi_1$ \\
			$\displaystyle \Rightarrow \sum_{n=3}^N x_n^2 = r^2\sin^2\phi_1\sin^2\phi_2$
		\item $\displaystyle \mathbf x_{3-N} = \sum_{n=3}^N$ \\
		$\displaystyle \Rightarrow \begin{cases} \displaystyle \norm{\mathbf x_{3-N}}^2 = \sum_{n=3}^Nx_n^2 = r^2\sin^2\phi_1\sin^2\phi_2 = r_3^2 \\ \displaystyle \cos\phi_3 = \frac {\mathbf x_{3-N}\cdot e_3} {\norm{\mathbf x_{3-N}} \norm{e_3}} = \frac{x_3}{r_3} \end{cases}$ \\
		$\displaystyle \Rightarrow \begin{cases} r_3 = r \sin \phi_1 \sin \phi_2 & (\text{ as } \phi_1,\phi_2\in[0,\pi]) \\ x_3 = r_3\cos\phi_3 \end{cases}$ \\
		$\displaystyle \Rightarrow \mathbf x_{3-N} = r\sin\phi_1\sin\phi_2e_3 + \sum_{n=4}^Nx_ne_n$
		\end{itemize}
	\end{itemize}

\item Proof for $x_i$
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item $\displaystyle \mathbf x_{i-N} = \sum_{n=i}^{N} x_ne_n$ \\
		$\displaystyle \Rightarrow \cos\phi_i = \frac {\mathbf x_{i-N}\cdot e_i}{\norm{\mathbf x_{i-N}}\norm{e_i}} = \frac {x_i}{r_i}$ \\
		$\Rightarrow x_i = r_i\cos\phi_i$
		\end{itemize}
	\end{itemize}

\item Induction
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item $\displaystyle \forall i \ge 2, r_i = r\prod_{j=1}^{i-1}\sin\phi_j$
		\end{itemize}
	\item Base Case ($i=2$)
		\begin{itemize}
		\item as in observation, $\displaystyle r_2 = r\sin\phi_1 = r\prod_{j=1}^{2-1}\sin\phi_j$
		\end{itemize}
	\item Step Case
		\begin{itemize}
		\item assumption $\displaystyle r_i = r\prod_{j=1}^{i-1}\sin\phi_j$
		\item procedure: $\displaystyle \mathbf x_{i-N} = \sum_{n=i}^Nx_ne_n = r_i\cos\phi_i + \sum_{n=i+1}^Nx_ne_n$ \\
			$\displaystyle \Rightarrow \norm{x_{i-N}}^2 = r_i^2\cos^2\phi_i + \sum_{n=i+1}^N x_n^2 = r_i^2$ \\
			$\displaystyle \Rightarrow \norm{x_{i+1-N}}^2 = \sum_{n=i+1}^Nx_n^2 = r_i^2\sin^2\phi = r_{i+1}^2$ \\
			$\displaystyle \Rightarrow r_{i+1} = r_i\sin\phi_i = r\prod_{j=1}^i\sin\phi_j$
		\end{itemize}
	\end{itemize}

\item Derivation
	\begin{itemize}
	\item $x_i$ from Combined Proofs
		\begin{itemize}
		\item $x_i = \begin{cases} r\cos\phi_1 & i = 1 \\ \displaystyle r\cos\phi_i\prod_{j=1}^{i-1}\sin\phi_j & 2\le i \le N \end{cases}$
		\end{itemize}
	\item Last 2 Dimensions
		\begin{itemize}
		\item $\displaystyle \mathbf x_{(N-1)-N} = x_{N-1}\cdot e_{N-1} + x_N\cdot e_N, \text{ with } r_{N-1} = r\prod_{j=1}^{N-2}\sin\phi_j$ \\
		$\Rightarrow \norm{\mathbf x_{(N-1)-N}} = f(\phi_{N-1}, \phi_N)=r_{N-1}$ \\
		$\Rightarrow \phi_{N-1},\phi_N$ not independent\textbf{!} \\
		(actually, if $e_N = e_{N-1} + \frac \pi 2$, then $\phi_N = \phi_{N-1} - \frac \pi 2$) \\
		
		$\Rightarrow$ define $\theta \in [0,2\pi)$ instead of $\phi_{N-1}, \phi_N\in[0,\pi]$ \\
		$\displaystyle \Rightarrow x_{N-1} = r_{N-1}\sin\theta, x_{N} = r_{N-1}\cos\theta$ (interchangeable)
		\end{itemize}
	\item Final Spherical Coordinates
		\begin{itemize}
		\item $x_i = \begin{cases} r\cos\phi_1 & i = 1 \\ \displaystyle r\cos\phi_i\prod_{j=1}^{i-1}\sin\phi_j & 2\le i \le N-1 \\ \displaystyle r\sin\theta\prod_{j=1}^{N-2}\sin\phi_j & i=N-1 \\ \displaystyle r\cos\theta\prod_{j=1}^{N-2}\sin\phi_j & i=N \end{cases}$
		\end{itemize}
	\end{itemize}

\end{itemize}

 
\section{Calculus}

\subsection{Integral}
\subsubsection{Interchanging Coordinates in Integral}
\begin{itemize}
\item General Theory
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $(x,y)$: coordinate under Field $D$
		\item $(u,v)$: coordinate under Field $D'$
		\item $\displaystyle T: \begin{cases} x=x(u,v), \\ y=y(u,v) \end{cases}$: transformation from $D$ to $D'$
		\end{itemize} 
	\item Assumption
		\begin{itemize}
		\item $f(x,y)$ continuous in $D$
		\item transformation $T$'s partial $1^{st}$ order derivatives continuous on $D'$
		\item transformation $T$'s Jacobian $J(u,v) = \frac {\partial (x,y)}{\partial (u,v)} \neq 0$
		\item transformation $T: D\rightarrow D'$ is 1-1 mapping
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\item take infinitely small square in $D':$
		\begin{align*} &M_4'(u,v+\delta v),&&M_3'(u+\delta u, v+\delta v),\\&M_1'(u, v),&&M_2'(u+\delta u, v) \end{align*} \\
		$\Rightarrow$ after transformation to $D:$ 
		\begin{align*} 
		&M_4(x(u,v+\delta v),y(u,v+\delta v)),&&M_3(x(u+\delta u, v+\delta v),y(u+\delta u, v+\delta v)),\\&M_1(x(u, v),y(u, v)),&&M_2(x(u+\delta u, v),y(u+\delta u, v)) 
		\end{align*}
		\begin{align*} 
		\Rightarrow &x_2 - x_1 = x(u+\delta u, v) - x(u,v) = \frac {\partial x}{\partial u}\rvert_{(u,v)} \delta u \\
		&x_4 - x_1 = x(u, v+\delta v) - x(u,v) = \frac {\partial x}{\partial v}\rvert_{(u,v)} \delta v \\
		&y_2 - y_1 = y(u+\delta u, v) - y(u,v) = \frac {\partial y}{\partial u}\rvert_{(u,v)} \delta u \\
		&y_4 - y_1 = y(u, v+\delta v) - y(u,v) = \frac {\partial y}{\partial v}\rvert_{(u,v)} \delta v
		\end{align*} \\
		as $\delta u, \delta v \rightarrow 0$, curvilinear boundary quadrilateral $M_1M_2M_3M_4 \rightarrow$ parallelogram % ({\kaishu 曲边四边形}) ({\kaishu 平行四边形})
		\begin{align*}
		\Rightarrow S_{M_1M_2M_3M_4} &= \abs{\overrightarrow{M_1M_2} \times \overrightarrow{M_1M_4}} = \abs{\begin{vmatrix} x_2-x_1 & y_2-y_1 \\ x_4-x_1 & y_4-y_1 \end{vmatrix}} \\&= \abs{ \begin{vmatrix} \frac {\partial x}{\partial u} \delta u & \frac {\partial y}{\partial u} \delta u \\ \frac {\partial x}{\partial v} \delta v & \frac {\partial y}{\partial v} \delta v \end{vmatrix} } = \abs{\begin{vmatrix} \frac {\partial x}{\partial u} & \frac {\partial y}{\partial u} \\ \frac {\partial x}{\partial v} & \frac {\partial y}{\partial v} \end{vmatrix}} \delta u\delta v \\ &= \abs{J(u,v)}\delta u\delta v 
		\end{align*}
		\Item \begin{align*} &\Rightarrow \text{ infinitely small area } \delta \sigma = dxdy = \abs{J(u,v)}\delta u\delta v \\ &\Rightarrow \int\int_D f(x,y) dxdy = \int\int_{D'} f(x(u,v), y(u,v)) |J(u,v)| dudv \end{align*}
		
		\end{itemize}
	\end{itemize}
\item Integral in Cartesian $\rightarrow$ Polar
	\begin{itemize}
	\item Result
		\begin{itemize}
		\item $dxdy = rdrd\theta$
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\item from general transformation: $x = r\cos(\theta), y = r\sin(\theta)$
			
		$\Rightarrow dxdy = \abs{J(r,\theta)} drd\theta = rdrd\theta$
			
		\item from direct calculation of infinite small area in polar coordinate 
		
		$\Rightarrow d\sigma = \frac 12 (r+dr)^2d\theta - \frac 12 r^2d\theta = rdrd\theta +\frac 12 (dr)^2d\theta$
			
		$\Rightarrow d\sigma = rdrd\theta, \text{when } dr,d\theta \rightarrow 0$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Gaussian Integral}
\begin{itemize}
\item Gaussian Function
	\begin{itemize}
	\item $\displaystyle f(x) = \rm e^{-a(x+b)^2}$
		\begin{itemize}
		\item special form: $\displaystyle f(x) = \rm e^{-(x)^2}$
		\item alternative form: $\displaystyle f(x) = \rm e^{ax^2+bx+c}$
		\item no indefinite integral $\displaystyle \int_a^b \rm e^{-x^2}$
		\item only definite integral $\displaystyle \int_{-\infty}^{+\infty} \rm e^{-x^2}$
		\end{itemize}
	\end{itemize}
\item Direct Integral
	\begin{itemize}
	\Item \begin{align*}\displaystyle (\int^{+\infty}_{-\infty} \rm e^{-a(x+b)^2}dx)^2 &= \int^{+\infty}_{-\infty} \rm e^{-a(x+b)^2}dx\int^{+\infty}_{-\infty} \rm e^{-a(y+b)^2}dy \\ &= \int\int^{+\infty}_{-\infty} \rm e^{-a[(x+b)^2  + (y+b)^2]}d(x+b)d(y+b) = \int\int^{+\infty}_{-\infty} \rm e^{-a(x^2+y^2)}dxdy \\ &= \int_0^{2\pi}\int_0^{+\infty} \rm e^{-ar^2}rdrd\theta  \\ &= \frac \pi a \end{align*}
	
	$\displaystyle \Rightarrow \int^{+\infty}_{-\infty} \rm e^{-a(x+b)^2}dx = \sqrt{\frac \pi a}, \text{ alternatively } \int^{+\infty}_{-\infty} \rm e^{ax^2+bx+c}dx = \sqrt{\frac {\pi} {-a}} \cdot \rm e^{\frac {b^2}{4a} + c}$
	\end{itemize}

\item Even Moment of Gaussian Function
	\begin{itemize}
	\Item \begin{align*} \displaystyle \int_{-\infty}^{+\infty} x^{2n}\rm{e}^{-ax^2}dx &= (-1)^n\int^{+\infty}_{-\infty} \frac {\partial^n} {\partial a^n} \rm{e}^{-ax^2}dx \\ &= (-1)^n \frac {\partial^n} {\partial a^n} \int_{-\infty}^{+\infty} \rm{e}^{-ax^2}dx && \text{by parameter differeation} \\ &= (-1)^n\sqrt{\pi} \frac {\partial^n} {\partial a^n} a^{-\frac 12} \\ &= \sqrt {\frac {\pi}{a}} \frac {(2n-1)!!}{(2a)^n}, &&\text{where } !! \text{ is double factorial} \end{align*}
	\end{itemize}
\end{itemize}

 
\section{Probability Theory}

\subsection{Introduction}

\subsubsection{Background}
\begin{itemize}
\item Measuring Uncertainty
	\begin{itemize}
	\item Source of Uncertainty
		\begin{itemize}
		\item noise in reality \& observation
		\item finite size of data (limited information)
		\end{itemize}
	\end{itemize}
\item Derivation
	\begin{itemize}
	\item Quantifying Belief
		\begin{itemize}
		\item by Cox (1946): if numerical values used to
		represent degrees of belief, a simple set of axioms encoding common sense
		properties of such beliefs will lead \underline{uniquely} to a set of rules for manipulating degrees of
		belief that are equivalent to the \underline{sum and product rules of probability}
		\end{itemize}
	\item Measuring Uncertainty
		\begin{itemize}
		\item by Jaynes (2003): probability theory can be regarded as an extension of
		Boolean logic to situations involving uncertainty
		\end{itemize}
	\item Common Destination
		\begin{itemize}
		\item numerical quantities to measure uncertainty, derived from different sets of properties/axioms, behave precisely according to the rules of probability
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{The Basic}
\begin{itemize}
\item Notation
	\begin{itemize}
	\item $X,Y$: random variable
	\end{itemize}
\item Discrete 
		\begin{itemize}
		\item $P(X,Y)$: joint probability of $X,Y$ taking their values
		\item $P(X)$: marginal probability of $X$ taking its value
		\item $P(X|Y)$: conditional probability of $X$ taking its value given $Y$ observed / determined
	\end{itemize}
	\item Continuous
		\begin{itemize}
		\item $P(x) = P_X(x)$: cumulative probability of value for variable $X < x$
		\item $p(x)$: probability density,
			\begin{itemize}
			\item where $\displaystyle \lim_{\delta x \rightarrow 0} P(X\in(x,x+\delta x)) = \lim_{\delta x \rightarrow 0}p(x)\delta x \Rightarrow \displaystyle P(X\in(a,b))=\int_a^b p(x)dx$
			\item $\displaystyle \Rightarrow p(x )\ge 0 \text{ and } \int^{+\infty}_{-\infty} p(x) = 1$
			\item $\Rightarrow P(z) = \int_{-\infty}^z p(x)dx$
			\end{itemize}
		\end{itemize}
\item Basic Rules
	\begin{itemize}
	\item Sum Rule
		\begin{itemize}
		\item $\displaystyle P(X) = \sum_Y P(X,Y)$, where $X,Y$ are discrete
		\item $\displaystyle P(X) = \int_Y P(X,Y)$, where $X,Y$ are continuous
		
		(formal justification requires measure theory)
		\end{itemize}
	\item Product Rule
		\begin{itemize}
		\item $P(X, Y) = P(Y|X)P(X)$
		\item $P(X, Y) = P(Y)P(X)$, where $X,Y$ are independent
		\end{itemize}
	\item $\Rightarrow$ Bayes' Rule
		\begin{itemize}
		\item $\displaystyle P(Y|X) = \frac {P(X|Y)P(Y)} {P(X)} = \frac{P(X|Y)P(Y)}{\sum_YP(X|Y)P(Y)}$, where $Y$ are discrete
		\item $\displaystyle P(Y|X) = \frac {P(X|Y)P(Y)} {P(X)} = \frac{P(X|Y)P(Y)}{\int_YP(X|Y)P(Y)}$, where $Y$ are continuous
		\end{itemize}
	\end{itemize}
\item Interpretation of Bayes
	\begin{itemize}
	\item Normalization
		\begin{itemize}
		\item the $\sum$, $\int$ can be interpreted as a \textbf{normalization constant}
		
		$\Rightarrow \textbf{posterior} \propto \textbf{likelihood} \times \textbf{prior}$
		\end{itemize}
	\item Prior
		\begin{itemize}
		\item $P(Y)$: available probability of desired variable \textbf{before} anything observed
		
		$\Rightarrow Y$ usually model parameters
		\end{itemize}
	\item Posterior
		\begin{itemize}
		\item $P(Y|X)$: obtained probability of desired variable \textbf{after} observation
		
		$\Rightarrow$ if $X,Y$ independent, observation has no effect $\Rightarrow$ prior $=$ posterior
		\end{itemize}
	\item Likelihood
		\begin{itemize}
		\item $P(X|Y)$: how probable/likely of $X$ being observed under different setting of $Y$
		
		\end{itemize}
	\item Prior $\rightarrow$ Posterior
		\begin{itemize}
		\item a process of incorporating the evidence provided by observation
		\end{itemize}
	\end{itemize}
\end{itemize}
 
\subsection{Expectations and Covariances}

\subsubsection{Expectation}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item Expectation of $f(x)$ under $p(x)$
		\begin{itemize}
		\item discrete $x$: $\displaystyle \mathbb{E}_{p}[f] = \sum_xp(x)f(x)$
		\item continuous $x$: $\displaystyle \mathbb{E}_{p}[f] = \int p(x)f(x)dx$
		\item approximation with $N$ points drawn from $p(x)$: $\displaystyle \mathbb E_p[f] \simeq \frac 1N \sum_{n=1}^N f(x_n) $
		
		(when $N\rightarrow \infty$, $\simeq$ becomes $=$)
		\end{itemize}
	\item Multivariate Expectation
		\begin{itemize}
		\item Marginal Expectation of $f(x,y)$ on $x$: $\displaystyle \mathbb{E}_x[f(x,y)] = \sum_xp(x)f(x,y)$ 
		
		(hence a function of $y$)
		\item Conditional Expectation $f(x)$ on $p(x|y)$: $\displaystyle \mathbb{E}[f|y] = \sum_x p(x|y)f(x)$
		\end{itemize}
	\end{itemize}
\item Independence
	\begin{itemize}
	\item Independent $x,y$
		\begin{itemize}
		\item $\displaystyle \mathbb{E}_{xy}[x,y] = \sum_{x,y}p(x,y)xy = \sum_{x,y}p(x)p(y)xy = \mathbb{E}[x]\mathbb{E}[y]$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Variance}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item Variance of $f(x)$:
		\begin{itemize}
		\Item \begin{align*} \text{var}[f] &= \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2] \\ &= \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2 \end{align*}
		\end{itemize}
	\item Covariance
	\end{itemize}
\end{itemize}

\subsubsection{Covariance}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item between Variables $x,y$
		\begin{itemize}
		\Item \begin{align*} \text{cov}[x,y] &= \mathbb{E}_{x,y}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])] \\ &= \mathbb{E}_{x,y}[xy] - \mathbb{E}[x]\mathbb{E}[y] \end{align*}
		\end{itemize}
	\item between Vectors $\mathbf x, \mathbf y$ (column vectors)
		\begin{itemize}
		\Item \begin{align*} \text{cov}[\mathbf x, \mathbf y] &= \mathbb{E}_{\mathbf x,\mathbf y}[(\mathbf x - \mathbb{E}[\mathbf x]) \cdot (\mathbf y^T -\mathbb{E}[\mathbf y^T])] \\ &= \mathbb{E}_{\mathbf x,\mathbf y}[\mathbf{xy}^T] - \mathbb{E}[\mathbf x]\mathbb E[\mathbf y^T] \end{align*}
		
		(pairwise covariance between components of $\mathbf x,\mathbf y$)
		\end{itemize}
	\item within Vector $\mathbf x$
		\begin{itemize}
		\item $\text{cov}[\mathbf x] \equiv \text{conv}[\mathbf x, \mathbf x]$ 
		
		(pairwise covariance between its components)
		\end{itemize}
	\end{itemize}
\item Independence Variable
	\begin{itemize}
	\item Independent $x,y$
		\begin{itemize}
		\item $\text{cov}[x,y] = \mathbb E_{x,y}[xy] -\mathbb{E}[x]\mathbb{E}[y] = 0$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Transformations of Random Variables}

\subsubsection{Inverse Image}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item function $g:\mathbb{R} \rightarrow \mathbb{R}$
		\item set $A$ in $\mathbb{R}$
		\end{itemize}
	\item Inverse Image on Set $A$
		\begin{itemize}
		\item $\displaystyle g^{-1}(A) = \{x\in\mathbb{R}|g(x)\in A\}$
		
		$\Leftrightarrow x\in g^{-1}(A) \text{ if and only if } g(x) \in A$
		
		interpretation: for each element in $A$, get its original value before $g$ applied
		\end{itemize}
	\end{itemize}
\item Properties
	\begin{itemize}
	\item $g^{-1}(\mathbb{R}) = \mathbb{R}$, as $g$ is defined on $\mathbb{R}$
	\item $\forall \text{ set } A, g^{-1}(A^c) = g^{-1}(A)^c$, where $A^c$ is the complement of set $A$
	\item $\displaystyle \forall \text{ collection of sets } \{A_\lambda | \lambda \in \Lambda\}, g^{-1}\left( \bigcup_\lambda A_\lambda \right) = \bigcup_\lambda g^{-1}(A_\lambda)$
	\item General Transformation $Y=g(X)$
		\begin{itemize}
		\item $P(Y\in A) = P(g(X)\in A) = P(X \in g^{-1}(A))$
		\end{itemize}
	\end{itemize}
	
\end{itemize}

\subsubsection{Discrete Variable}
\begin{itemize}
\item Variable
	\begin{itemize}
	\item $X$: random variable with probability mass function $P_X(x)$
	\item $Y = g(X)$, with probability mass function $P_Y(y)$
	\end{itemize}
\item Probability Mass Function
	\begin{itemize}
	\item $\displaystyle P_Y(y) = \sum_{x\in g^{-1}(y)} P_X(x)$, as $X=x$ is independent and mutually exclusive	
	
	note: $g^{-1}(y)$ denotes $g^{-1}(\{y\})$
	\item Example
		\begin{itemize}
		\item uniform random variable $X$ on $\{-n,...,n\}$ with $Y=|X|$
		
		$\Rightarrow P_X(x) = \frac 1 {2n+1}$
			
		$\Rightarrow P_Y(y) = \begin{cases} \frac 1 {2n+1}, &x=0, \\ \frac 2 {2n+1}, &x\neq0. \end{cases}$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Continuous}
\begin{itemize}
\item Variable
	\begin{itemize}
	\item $X$: random variable with cumulative distribution $P_X(x)$, density $p_X(x)$
	\item $Y=g(X)$, with cumulative distribution $P_Y(y)$, density $p_Y(y)$
	\end{itemize}

\item Cumulative Distribution
	\begin{itemize}
	\item Strictly Monotone Increasing $g$
		\begin{itemize}
		\item $P_Y(y) = P(Y\le y) = P(g(X) \le y) = P(X \le g^{-1}(y)) = P_X(g^{-1}(y))$
		\end{itemize}
	\item Strictly Monotone Decreasing $g$
		\begin{itemize}
		\item $P_Y(y) = P(Y\le y) = P(g(X) \le y) = P(X \ge g^{-1}(y)) = 1 - P_X(g^{-1}(y))$
		\end{itemize}
	\end{itemize}

\item Probability Density
	\begin{itemize}
	\item Strictly Monotone $g$ (an one-to-one function)
		\begin{itemize}
		\item $\displaystyle p_Y(y) = \frac d {dy} P_Y(y) = \frac {d P_Y(y)}{d\space  g^{-1}(y)} \frac{d \space g^{-1}(y)}{dy} = p_X(g^{-1}(y)) \abs*{\frac{d}{dy} g^{-1}(y)}$, 
		
		as $g^{-1}$ has the same monotony as $g$, combined with the sign in $P_Y$ to give the $\abs*\cdot$
		
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Gaussian Distribution}
 
\subsubsection{Definition}
\begin{itemize}
\item Univariate Gaussian
	\begin{itemize}
	\item Variable
		\begin{itemize}
		\item mean: $\mu$
		\item variance: $\sigma^2 \Rightarrow$ reciprocal of the variance $\beta = \frac 1 {\sigma^2}$ (also called precision)
		\end{itemize}
	\item Probability Dense Function (PDF)
		\begin{itemize}
		\item $\displaystyle \mathcal N(x|\mu, \sigma^2) = \frac 1 {(2\pi \sigma^2)^{1/2}} \exp \left\{ -\frac 1 {2\sigma^2}(x-\mu)^2 \right\}$
		
		$\Rightarrow$ satisfying probability axioms: $\displaystyle \mathcal N(x|\mu,\sigma^2)>0 \text{ and } \int_{-\infty}^{+\infty} \mathcal N (x|\mu,\sigma^2) dx = 1$
		\end{itemize}
	\item Expectation
		\begin{itemize}
		\item $\displaystyle \mathbb{E}[x] = \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)xdx=\mu$
		\begin{align*}
		\Rightarrow \displaystyle \mathbb E [x^2] &= \int_{-\infty}^{+\infty} \mathcal{N}(x|\mu, \sigma^2)x^2dx \\ 
		&= \frac 1 {(2\pi \sigma^2)^{1/2}} \int_{-\infty}^{+\infty} x^2 \exp \left\{ -\frac 1 {2\sigma^2}(x-\mu)^2 \right\} dx \\ 
		&= \pi^{-\frac 12} \int_{-\infty}^{+\infty} (\sqrt{2\sigma^2}x+\mu)^2 \exp(-x^2) dx, \text{ substituting } a=\frac{x-\mu}{\sqrt{2\sigma^2}} \\ 
		&= \pi^{-\frac 12} (2\sigma^2\int_R x^2\rm{e}^{-x^2}dx + 2\mu\sqrt{2\sigma^2}\int_R x\rm{e}^{-x^2}dx + \mu^2 \int_R \rm{e}^{-x^2}dx ) \\ 
		&= \pi^{-\frac 12} ( 2\sigma^2\int_R x^2\rm{e}^{-x^2}dx + 2\mu\sqrt{2\sigma^2}\cdot 0 + \mu^2\sqrt\pi ) \\ 
		&= 2\sigma^2\pi^{-\frac 12}\int_R x^2\rm{e}^{-x^2}dx + \mu^2 \\ 
		&= \sigma^2 + \mu^2, \text{ by 2nd moment of Guassian or } (x\rm e^{-x^2})' = e^{-x^2}-2x^2\rm e^{-x^2}
		\end{align*}
		\end{itemize}
	\item Variance
		\begin{itemize}
		\item $\displaystyle \text{var}[x] = \mathbb E[x^2] - \mathbb E[x]^2 = \sigma^2$
		\end{itemize}
	\end{itemize}
\item Multivariate ($d$-dimensional) Gaussian
	\begin{itemize}
	\item Variable
		\begin{itemize}
		\item mean: $\mu \in \mathbb R^d$
		\item covariance matrix: $\Sigma_{d\times d}$
		\end{itemize}
	\item Probability Dense Function (PDF)
		\begin{itemize}
		\item $\displaystyle \mathcal N_d(\mathbf x|\mathbf{\mu},\mathbf \Sigma) = \frac 1 {(2\pi)^{d/2} |\Sigma|^{1/2}} \exp \left\{ -\frac 1 2 (x-\mu)^T \Sigma^{-1} (x-\mu) \right\} $,
		
		noted as $X\sim \mathcal N_d (x|\mu,\Sigma)$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Multivariate Gaussian}
\begin{itemize}
\item Dimensionality
	\begin{itemize}
	\item Volume of High Dimensional Sphere
		\begin{itemize}
		\item for $n=2k, k\in N^+, V_{2k}(R) = \frac {\pi^k}{k!} R^{2k}$ 
		\item for $n=2k+1, k\in N, V_{2k}(R) = \frac {2^{k+1}\pi^k}{(2k+1)!!} R^{2k+1}$ 
		\item $\displaystyle \Rightarrow \lim_{D\rightarrow+\infty} \frac {V_D(1)-V_D(1-\epsilon)}{V_D(1)} = \lim_{D\rightarrow+\infty}1-1(1-\epsilon)^D = 1$
			$\Rightarrow$ the volume of a $D$-sphere concentrate in a thin shell near the surface! \\
			(actually, in the corner of a high dimensional cube as shown below)
		\end{itemize}
	\item Volume of High Dimensional Cube
		\begin{itemize}
		\item 
		\item $\Rightarrow$ volume ratio of hyper sphere and hyper cube: 
		$\Rightarrow$ the volume of a $D$-cube concentrates in its corner!
		$\Rightarrow$ distance function in high dimensional space CAN be useless
		\end{itemize}
	\item High Dimensional Distribution
	\item High Dimensional Gaussian
		\begin{itemize}
		\item probability density with respect to radius $r$ for various dimension $D$ \\
			$\Rightarrow$ most density are in a thin shell at a specific $r$			
		\end{itemize}
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth,center]{./Math/"multivariate gaussian-mass of distribution".jpg}
		\end{figure}
	\item Facing High Dimensionality
	\end{itemize}
\end{itemize}

\subsubsection{}
\begin{itemize}
\item Convolution of Gaussian
	\begin{itemize}
	\item Integral of Gaussians $\int G_1G_2dx$
		\begin{itemize}
		\item $G_1\sim \mathcal N_d(x|a,A), \space G_2 \sim \mathcal N_d(x|b,B)$
		\end{itemize}
		\begin{align*} \displaystyle \Rightarrow & \int \mathcal N_d(x|a,A) N_d(x|b,B)dx \\ = & \int \frac 1 {(2\pi)^{d/2} |A|^{1/2}} e^{-\frac 1 2 (x-a)^TA^{-1}(x-a)} \frac 1 {(2\pi)^{d/2}|B|^{1/2}} e^{-\frac 1 2 (x-b)^TB^{-1}(x-b)} dx \\ = & \int \frac 1 {(2\pi)^{d/2} |A|^{1/2}} \frac 1 {(2\pi)^{d/2}|B|^{1/2}} e^{-\frac 12 [(x-a)^TA^{-1}(x-a)+(x-b)^TB^{-1}(x-b)]} \\ = & \int \frac 1 {(2\pi)^{d/2} |A|^{1/2}} \frac 1 {(2\pi)^{d/2}|B|^{1/2}}  e^{-\frac 12 [(x-c)^T (A^{-1}+B^{-1}) (x-c)+(a-b)^TC(a-b)]},\\ & \text{where } c=(A^{-1}+B^{-1})^{-1}(A^{-1}a+B^{-1}b), C=A^{-1}(A^{-1}+B^{-1})^{-1}B^{-1} = (A+B)^{-1} \\ = & \frac {\begin{vmatrix}(A^{-1}+B^{-1})^{-1}\end{vmatrix}^{1/2}} {(2\pi)^{d/2}|A|^{1/2}|B|^{1/2}} e^{-\frac 12 (a-b)^TC(a-b)} \int \frac 1 {(2\pi)^{d/2}\begin{vmatrix}(A^{-1}+B^{-1})^{-1}\end{vmatrix}^{1/2}} e^{-\frac 12 (x-c)^T(A^{-1}+B^{-1}) (x-c)} dx \\ =& \frac {\begin{vmatrix}(A^{-1}+B^{-1})^{-1}\end{vmatrix}^{1/2}} {(2\pi)^{d/2}|A|^{1/2}|B|^{1/2}} e^{-\frac 12 (a-b)^TC(a-b)} \\ =& \frac 1 {(2\pi)^{d/2} (|A||B||A^{-1}+B^{-1}|)^{1/2}} e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |ABA^{-1}+ABB^{-1}|^{1/2}} e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |ABA^{-1}+A| } e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |A(B+A)A^{-1}| } e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |A+B|^{1/2} } e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \end{align*}
			
	\item $\Rightarrow$ Convolution of Gaussians $G1* G2$
		\begin{itemize}
		\item $G_1\sim \mathcal N_d(a,A),G_2\sim \mathcal N_d(b,B)$
		\end{itemize}
		\begin{align*} \displaystyle G_1* G_2 (z) &= \int G_1(x)G_2(z-x)dx \\ &= \int \mathcal N_d (x|a,A)\mathcal N_d(z-x|b,B)dx \\ &= \int \mathcal N_d (x|a,A) \cdot \frac 1 {(2\pi)^{d/2} |B|^{1/2}} e^{-\frac 12 (z-x-b)^TB^{-1} (z-x-b)} dx \\ &= \int \mathcal N_d (x|a,A)\mathcal N_d(x|z-b,B)dx \\&= \frac 1 {(2\pi)^{d/2}|A+B|^{1/2}} e^{-\frac 12(z-(a+b))^T (A+B)^{-1} (z-(a+b))} \\&= \boldsymbol{\mathcal N_d(z|a+b,A+B)}  
		\end{align*}
	\end{itemize}
\end{itemize}
 
 
\subsection{Bayesian Interpretation of Probability}
\subsubsection{Contrasting Frequentist Estimator}
\begin{itemize}
\item Posterior $\displaystyle p(\mathbf w|\mathcal D) = \frac {p(\mathcal D|\mathbf w)p(\mathbf w)}{p(\mathcal D)}$
	\begin{itemize}
	\item Notation
 		\begin{itemize}
 		\item $\mathcal D$ the observed dataset
 		\item $\mathbf w$ the vector for model parameters
 		\end{itemize}
	\item Bayesian
		\begin{itemize}
		\item only one single dataset $\mathcal D$ (the observed one)
 		\item uncertainty expressed as distribution over $\mathbf w$
 		\item model's error: use likelihood / posterior directly (or after taking $\log$)
 		\item pros
 			\begin{enumerate}
 			\item naturally incorporating prior knowledge as prior distribution (of $\mathbf w$)
 			\end{enumerate}
 		\item cons
 			\begin{enumerate}
 			\item prior usually selected for mathematic convenience 
 			\end{enumerate}
 		\end{itemize}
 	\item Frequentist Estimator
 		\begin{itemize}
 		\item parameters $\mathbf w$ already determined / fixed by 'estimator' (model)
 		\item error bars of the model obtained by considering the distribution over $\mathcal D$
 		\item model's error: bootstrap procedure
 			\begin{enumerate}
 			\item generate dataset(s) by drawing from the observed $\mathcal D$ with replacement
 			\item sampling $L$ datasets with the same size as $\mathcal D$
 			\item error = variability of predictions between the sampled datasets
 			\end{enumerate}
 		\item pros
 			\begin{enumerate}
 			\item protect the conclusion from false prior knowledge
 			\end{enumerate}
 		\item cons
 			\begin{enumerate}
 			\item sensitive to observation (extreme cases), especially under small dataset
			\end{enumerate}
		\end{itemize}
	\end{itemize}
\end{itemize}
 
\subsubsection{Parameter Estimation}
\begin{itemize}
\item Bias vs. Variance


\item Taking Logarithm
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item monotonically increasing function $\Rightarrow \arg\max_{\theta}f(x;\theta) = \arg\max_\theta\log f(x;\theta)$
		\item simplify mathematical analysis $\Rightarrow \prod \rightarrow \sum$
		\item numerical stability $\Rightarrow \text{ avoid } \prod (\text{small probabilities})$
		
		(may otherwise underflow the numerical precision)
		\end{itemize}
	\end{itemize}

\item Maximum Likelihood Estimation for Gaussian
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $X=\{x_1,...,x_N\}$: observed $N$ data points
		\end{itemize}
	\item Assumption
		\begin{itemize}
		\item data points are i.i.d. (identically and independently distributed)
		\item underlying distribution is Gaussian $\mathcal N(\mu, \sigma)$
		\end{itemize}
	\item Likelihood
		\begin{itemize}
		\item $\displaystyle p(X|\mu,\sigma^2) = \prod_{n=1}^N \mathcal(x_n|\mu,\sigma^2)$
		\item $\displaystyle \Rightarrow \ln p(X|\mu,\sigma) = -\frac 1{2\sigma^2} \sum_{n=1}^N(x_n-\mu)^2 -\frac N2 \ln \sigma^2 -\frac N2 \ln (2\pi)$
		\end{itemize}
	\item Solution
		\begin{itemize}
		\item $\displaystyle \text{let } \frac {\partial}{\partial \mu} \ln p(X|\mu,\sigma^2) = 0 \Rightarrow \mu_{\text{ML}}=\frac 1N \sum_{n=1}^Nx_n $
		\item $\displaystyle \text{let } \frac {\partial}{\partial \sigma^2} \ln p(X|\mu,\sigma^2)=0 \Rightarrow \sigma_\text{ML}^2 = \frac 1N \sum_{n=1}^N (x_n- \mu_\text{ML})$
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\Item \begin{align*} \displaystyle \mathbb E[\mu_\text{ML}] = \mathbb E[\frac 1N \sum_{n=1}^Nx_n] = \frac 1N \sum_{n=1}^N \mathbb E[x_n] = \mu \tag{as \(x_1,...,x_N\) i.i.d, drawn from \(\mathcal N(\mu,\sigma^2)\), thus \(\sim \mathcal N(\mu,\sigma^2)\)} \end{align*}
		$\Rightarrow$ unbiased estimation of mean
		
		\Item \begin{align*} \mathbb E[\sigma_\text{ML}^2] &= \mathbb E[\frac 1N \sum_{n=1}^N(x_n-\mu_{\text{ML}})^2] \\
		&= \mathbb E [ \frac 1N \sum_{n=1}^{N}(x_n^2-2\mu_\text{ML}x_n+\mu_\text{ML}^2) ] \\ 
		&= \frac 1N \sum_{n=1}^N \mathbb E[x_n^2] - \mathbb E[\frac 1N \sum_{n=1}^N 2x_n \mu_\text{ML}] + \mathbb E[\mu_\text{ML}^2] \\ 
		&= \frac 1N \sum_{n=1}^N \mathbb E[x_n^2] - 2\mathbb E[\mu_\text{ML}^2] + \mathbb E[\mu_\text{ML}^2] \\
		&= \frac 1N \sum_{n=1}^N \mathbb E[x_n^2] - \frac 1{N^2} \sum_{i,j=1}^N \mathbb E[x_ix_j] \\
		&= \frac 1N \sum_{n=1}^N (\sigma^2 + \mu^2) - \frac 1{N^2} [N(N-1)\mu^2 + N(\sigma^2+\mu^2)] \tag{by 2nd moment of Gaussian \(\mathbb E[x^2]\) and i.i.d assumption} \\
		&= \left(\frac {N-1}N  \right) \sigma^2 \end{align*}
		$\Rightarrow$ biased variance \textbf{!} \\
		$\Rightarrow $ unbiased variance $\displaystyle \hat \sigma^2 = \frac N {N-1}\sigma^2_\text{ML} = \frac 1{N-1} \sum_{n=1}^N (x_n-\mu_\text{ML})^2 $ \\
		interpretation: $N-1$  degree of freedom, \\
		(as calculating $\sigma^2$ needs $\mu$, which help pin down $x_N$ given $x_1,...,x_{N-1}$) \\
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Predictive Distribution}
\begin{itemize}
\item Probabilistic Prediction
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $\bf x, \bf t$: vector of data examples and corresponding ground truth
		\item $\bf w$: model parameters
		\item $x, t$: new data example for prediction and its ground truth
		\end{itemize}
	\item Prediction by Model
		\begin{itemize}
		\item $p(t|x, \mathbf{w}')$, where $\mathbf w'$ is the best fit parameters founded
		\end{itemize}
	\item Prediction by Data
		\begin{itemize}
		\item $\displaystyle p(t|x,\mathbf{x},\mathbf{t}) = \int p(t|x,\mathbf{w})p(\mathbf{w}|\mathbf{x},\mathbf{t}) d\mathbf w$, where $\bf w$ marginalized over its posterior 
		\end{itemize}
	\end{itemize}
\end{itemize}