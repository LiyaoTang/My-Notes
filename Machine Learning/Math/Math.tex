\chapter{Math}

\section{Convolution}

\begin{itemize}
\item Definition
	\begin{itemize}
	\item $\displaystyle f* g (z) = \int_{\mathbb R}f(x)g(z-x) dx,$ where $f(x),g(x)$ are functions in $\mathbb R$
	\end{itemize}
	
\item Statistical Meaning
	\begin{itemize}
	
	\item Notation
		\begin{itemize}
		\item X,Y: independent random variables, with pdf's given by $f$ and $g$
		\item $Z = X+Y$, with pdf given by $h(z)$:
		\end{itemize}
		
	\item $\Rightarrow h(z) = f * g (z)$
		\begin{itemize}
		\item derivation
		\end{itemize}
	\begin{align*} H(z) &= P(Z<z) = P(X+Y<z) \\ &= \int_x P(X=x)P(X+Y<z|X=x) dx \\ &= \int_x f(x)P(Y<z-x)dx \\ &= \int_x f(x)G_Y(z-x)dx  \end{align*} 
	\begin{align*} \Rightarrow h(x) &= \frac d {dz} H(z) = \frac d {dz} \int_x f(x)G_Y(z-x)dx \\ &= \int_x f(x) \frac {dG_Y(z-x)}{dz} dx \\ &= \int_x f(x) g(z-x)dx \\ &= f * g (z) \end{align*}
	
	\end{itemize}
\end{itemize}
 
\section{Linear Algebra}

\subsection{Essence}
\subsubsection{Vector}
\begin{itemize}
\item Interpretation
	\begin{itemize}
	\item Movement
		\begin{itemize}
		\item direction
		\item distance
		\end{itemize}
	\item Numeric in High Dimensions
		\begin{itemize}
		\item in 1-$D$: +/- represents direction
		\item in n-$D$: +/- alone each dimension combined to represent an overall direction \\
			(direction of the n-$D$ numeric - vector)
		\end{itemize}
	\end{itemize}

\item Numerics Multiplication
	\begin{itemize}
	\item Scaling
		\begin{itemize}
		\item the number scales the distance of vector (direction remains) \\
			$\Rightarrow$ such number thus also called scalar
		\item $\Rightarrow$ scale alone each axis by that scalar \\
			$(2\mathbf x)^2 = 4\mathbf x^2 = 4 (x_1 e_1 + ... x_ne_n)^2 = (2x_1e_2+...+2x_ne_n)^2$, \\ 
			where $e_1,...,e_N$ are vector defining coordinates
		\end{itemize}
	\end{itemize}

\item Vector Adding
	\begin{itemize}
	\item Generalization of Numerical Adding
		\begin{itemize}
		\item in 1-$D$: joint movement along single axis
		\item in n-$D$: joint movement along each axis $\Rightarrow$ a joint movement in n-$D$ space
		\end{itemize}
	\item Linear Combination $\mathbf x = a\mathbf v + b\mathbf w$
		\begin{itemize}
		\item the vectors $\mathbf v, \mathbf w$ only changed linearly \\
			$\Rightarrow \mathbf x$ direction \& size are linear combination of that in $\mathbf v, \mathbf w$
		\end{itemize}
	\end{itemize}

\end{itemize}

\subsubsection{Coordinate}
\begin{itemize}
\item Foundation
	\begin{itemize}
	\item Basis Vectors
		\begin{itemize}
		\item define the direction \& size of a unit movement alone each axis
		\end{itemize}
	\end{itemize}
\item Orthonormal Coordinate
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsection{Interchanging Coordinates}

\subsubsection{N-dimensional Spherical Coordinates}
\begin{itemize}
\item Notation
	\begin{itemize}
	\item $N$-dim Euclidean Space $E_N$
		\begin{itemize}
		\item $e_1, e_2,...,e_N$: a group of orthonormal basis of $E_N$
		\item $\mathbf x = (x_1, x_2,...,x_N)$: vector in $E_n$
		\item $\mathbf x_{i-N}$: projection of $\mathbf x$ onto subspace spanned by $e_i,...,e_N$ \\
			$\displaystyle \Rightarrow \mathbf x_{i-N} = \sum_{n=i}^N x_ne_n$
		\end{itemize}
	\item Spherical Coordinates
		\begin{itemize}
		\item $r=\norm{\mathbf x}$: the norm of $\mathbf x$
		\item $\phi_i \in [0,\pi]$: angle between $\mathbf x_{i-N}$ and $e_i$
		\item $r_i=\norm{\mathbf x_{i-N}}$: norm of projection $\mathbf x_{i-N}$, with $r_1 = r$
		\end{itemize}
	\end{itemize}
	
\item Observation
	\begin{itemize}
	\item Space $e_1,...,e_N$:
		\begin{itemize}
		\item $\cos \phi_1 = \frac{\mathbf x e_1}{\norm{\mathbf x} \norm{e_1}} =\frac {x_1}{r} $ \\ 
			$\Rightarrow x_1 = r\cos\phi_1$ \\
			$\displaystyle \Rightarrow \mathbf x = r\cos\phi_1 e_1 + \sum_{n=2}^Nx_ne_n$
		\end{itemize}
	\item Space $e_2,...,e_N$:
		\begin{itemize}
		\item from above: $\displaystyle \mathbf x^2 = r^2\cos^2\phi_1 + \sum_{n=2}^N x_n^2 = r^2 $ \\ 
			$\displaystyle \Rightarrow \sum_{n=2}^N x_n^2 = r^2\sin^2\phi_1$
		\item $\displaystyle \mathbf x_{2-N} = \sum_{n=2}^Nx_ne_n$ \\
		$\displaystyle \Rightarrow \begin{cases} \displaystyle  \norm {\mathbf x_{2-N}}^2 = \sum_{n=2}^Nx_n^2 = r^2\sin^2\phi_1 = r_2^2 \\ \displaystyle \cos\phi_2 = \frac {\mathbf x_{2-N}\cdot e_2}{\norm{\mathbf x_{2-N}}\norm{e_2}} = \frac{x_2}{r_2} \end{cases}$ \\
		$\Rightarrow \begin{cases} r_2 = r\sin\phi_1 & (\text{ as }\phi_1\in [0,\pi]) \\ x_2 = r_2\cos\phi_2 = r\sin\phi_1\cos\phi_2 \end{cases}$ \\
		$\displaystyle \Rightarrow \mathbf x_{2-N} = r\sin\phi_1\cos\phi_2e_2 + \sum_{n=3}^Nx_ne_n$ \\
		\end{itemize}
	\item Space $e_3,...,e_N$:
		\begin{itemize}
		\item from above: $\displaystyle \mathbf x_{2-N}^2 = r^2\sin^2\phi_1\cos^2\phi_2 + \sum_{n=3}^{N}x_n^2 = r^2\sin^2\phi_1$ \\
			$\displaystyle \Rightarrow \sum_{n=3}^N x_n^2 = r^2\sin^2\phi_1\sin^2\phi_2$
		\item $\displaystyle \mathbf x_{3-N} = \sum_{n=3}^N$ \\
		$\displaystyle \Rightarrow \begin{cases} \displaystyle \norm{\mathbf x_{3-N}}^2 = \sum_{n=3}^Nx_n^2 = r^2\sin^2\phi_1\sin^2\phi_2 = r_3^2 \\ \displaystyle \cos\phi_3 = \frac {\mathbf x_{3-N}\cdot e_3} {\norm{\mathbf x_{3-N}} \norm{e_3}} = \frac{x_3}{r_3} \end{cases}$ \\
		$\displaystyle \Rightarrow \begin{cases} r_3 = r \sin \phi_1 \sin \phi_2 & (\text{ as } \phi_1,\phi_2\in[0,\pi]) \\ x_3 = r_3\cos\phi_3 \end{cases}$ \\
		$\displaystyle \Rightarrow \mathbf x_{3-N} = r\sin\phi_1\sin\phi_2e_3 + \sum_{n=4}^Nx_ne_n$
		\end{itemize}
	\end{itemize}

\item Proof for $x_i$
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item $\displaystyle \mathbf x_{i-N} = \sum_{n=i}^{N} x_ne_n$ \\
		$\displaystyle \Rightarrow \cos\phi_i = \frac {\mathbf x_{i-N}\cdot e_i}{\norm{\mathbf x_{i-N}}\norm{e_i}} = \frac {x_i}{r_i}$ \\
		$\Rightarrow x_i = r_i\cos\phi_i$
		\end{itemize}
	\end{itemize}

\item Induction
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item $\displaystyle \forall i \ge 2, r_i = r\prod_{j=1}^{i-1}\sin\phi_j$
		\end{itemize}
	\item Base Case ($i=2$)
		\begin{itemize}
		\item as in observation, $\displaystyle r_2 = r\sin\phi_1 = r\prod_{j=1}^{2-1}\sin\phi_j$
		\end{itemize}
	\item Step Case
		\begin{itemize}
		\item assumption $\displaystyle r_i = r\prod_{j=1}^{i-1}\sin\phi_j$
		\item procedure: $\displaystyle \mathbf x_{i-N} = \sum_{n=i}^Nx_ne_n = r_i\cos\phi_i + \sum_{n=i+1}^Nx_ne_n$ \\
			$\displaystyle \Rightarrow \norm{x_{i-N}}^2 = r_i^2\cos^2\phi_i + \sum_{n=i+1}^N x_n^2 = r_i^2$ \\
			$\displaystyle \Rightarrow \norm{x_{i+1-N}}^2 = \sum_{n=i+1}^Nx_n^2 = r_i^2\sin^2\phi = r_{i+1}^2$ \\
			$\displaystyle \Rightarrow r_{i+1} = r_i\sin\phi_i = r\prod_{j=1}^i\sin\phi_j$
		\end{itemize}
	\end{itemize}

\item Derivation
	\begin{itemize}
	\item $x_i$ from Combined Proofs
		\begin{itemize}
		\item $x_i = \begin{cases} r\cos\phi_1 & i = 1 \\ \displaystyle r\cos\phi_i\prod_{j=1}^{i-1}\sin\phi_j & 2\le i \le N \end{cases}$
		\end{itemize}
	\item Last 2 Dimensions
		\begin{itemize}
		\item $\displaystyle \mathbf x_{(N-1)-N} = x_{N-1}\cdot e_{N-1} + x_N\cdot e_N, \text{ with } r_{N-1} = r\prod_{j=1}^{N-2}\sin\phi_j$ \\
		$\Rightarrow \norm{\mathbf x_{(N-1)-N}} = f(\phi_{N-1}, \phi_N)=r_{N-1}$ \\
		$\Rightarrow \phi_{N-1},\phi_N$ not independent\textbf{!} \\
		(actually, if $e_N = e_{N-1} + \frac \pi 2$, then $\phi_N = \phi_{N-1} - \frac \pi 2$) \\
		
		$\Rightarrow$ define $\theta \in [0,2\pi)$ instead of $\phi_{N-1}, \phi_N\in[0,\pi]$ \\
		$\displaystyle \Rightarrow x_{N-1} = r_{N-1}\sin\theta, x_{N} = r_{N-1}\cos\theta$ (interchangeable)
		\end{itemize}
	\item Final Spherical Coordinates
		\begin{itemize}
		\item $x_i = \begin{cases} r\cos\phi_1 & i = 1 \\ \displaystyle r\cos\phi_i\prod_{j=1}^{i-1}\sin\phi_j & 2\le i \le N-1 \\ \displaystyle r\sin\theta\prod_{j=1}^{N-2}\sin\phi_j & i=N-1 \\ \displaystyle r\cos\theta\prod_{j=1}^{N-2}\sin\phi_j & i=N \end{cases}$
		\end{itemize}
	\end{itemize}

\end{itemize}
 
\section{Calculus}

\subsection{Integral}
\subsubsection{Interchanging Coordinates in Integral}
\begin{itemize}
\item General Theory
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $(x,y)$: coordinate under Field $D$
		\item $(u,v)$: coordinate under Field $D'$
		\item $\displaystyle T: \begin{cases} x=x(u,v), \\ y=y(u,v) \end{cases}$: transformation from $D$ to $D'$
		\end{itemize} 
	\item Assumption
		\begin{itemize}
		\item $f(x,y)$ continuous in $D$
		\item transformation $T$'s partial $1^{st}$ order derivatives continuous on $D'$
		\item transformation $T$'s Jacobian $J(u,v) = \frac {\partial (x,y)}{\partial (u,v)} \neq 0$
		\item transformation $T: D\rightarrow D'$ is 1-1 mapping
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\item take infinitely small square in $D':$
		\abovedisplayshortskip=0pt~\vspace*{-0.5\baselineskip}
		\begin{align*} &M_4'(u,v+\delta v),&&M_3'(u+\delta u, v+\delta v),\\&M_1'(u, v),&&M_2'(u+\delta u, v) \end{align*}
		$\Rightarrow$ after transformation to $D:$ 
		\abovedisplayshortskip=0pt~\vspace*{-0.5\baselineskip}
		\begin{align*} &M_4(x(u,v+\delta v),y(u,v+\delta v)),&&M_3(x(u+\delta u, v+\delta v),y(u+\delta u, v+\delta v)),\\&M_1(x(u, v),y(u, v)),&&M_2(x(u+\delta u, v),y(u+\delta u, v)) \end{align*}
		\abovedisplayshortskip=0pt~\vspace*{-1.5\baselineskip}
		\begin{align*} \Rightarrow &x_2 - x_1 = x(u+\delta u, v) - x(u,v) = \frac {\partial x}{\partial u}\rvert_{(u,v)} \delta u \\
		&x_4 - x_1 = x(u, v+\delta v) - x(u,v) = \frac {\partial x}{\partial v}\rvert_{(u,v)} \delta v \\
		&y_2 - y_1 = y(u+\delta u, v) - y(u,v) = \frac {\partial y}{\partial u}\rvert_{(u,v)} \delta u \\
		&y_4 - y_1 = y(u, v+\delta v) - y(u,v) = \frac {\partial y}{\partial v}\rvert_{(u,v)} \delta v
		\end{align*}
		as $\delta u, \delta v \rightarrow 0$, 
		curvilinear boundary quadrilateral $M_1M_2M_3M_4 \rightarrow$ parallelogram % ({\kaishu 曲边四边形}) ({\kaishu 平行四边形})
		\abovedisplayshortskip=0pt~\vspace*{-0.5\baselineskip}
		\begin{align*}\Rightarrow S_{M_1M_2M_3M_4} &= \abs{\overrightarrow{M_1M_2} \times \overrightarrow{M_1M_4}} = \abs{\begin{vmatrix} x_2-x_1 & y_2-y_1 \\ x_4-x_1 & y_4-y_1 \end{vmatrix}} \\&= \abs{ \begin{vmatrix} \frac {\partial x}{\partial u} \delta u & \frac {\partial y}{\partial u} \delta u \\ \frac {\partial x}{\partial v} \delta v & \frac {\partial y}{\partial v} \delta v \end{vmatrix} } = \abs{\begin{vmatrix} \frac {\partial x}{\partial u} & \frac {\partial y}{\partial u} \\ \frac {\partial x}{\partial v} & \frac {\partial y}{\partial v} \end{vmatrix}} \delta u\delta v \\ &= \abs{J(u,v)}\delta u\delta v \end{align*}
		
		\Item \begin{align*} &\Rightarrow \text{ infinitely small area } \delta \sigma = dxdy = \abs{J(u,v)}\delta u\delta v \\ &\Rightarrow \int\int_D f(x,y) dxdy = \int\int_{D'} f(x(u,v), y(u,v)) |J(u,v)| dudv \end{align*}
		
		\end{itemize}
	\end{itemize}
\item Integral in Cartesian $\rightarrow$ Polar
	\begin{itemize}
	\item Result
		\begin{itemize}
		\item $dxdy = rdrd\theta$
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\item from general transformation: $x = r\cos(\theta), y = r\sin(\theta)$
			
		$\Rightarrow dxdy = \abs{J(r,\theta)} drd\theta = rdrd\theta$
			
		\item from direct calculation of infinite small area in polar coordinate 
		
		$\Rightarrow d\sigma = \frac 12 (r+dr)^2d\theta - \frac 12 r^2d\theta = rdrd\theta +\frac 12 (dr)^2d\theta$
			
		$\Rightarrow d\sigma = rdrd\theta, \text{when } dr,d\theta \rightarrow 0$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Gaussian Integral}
\begin{itemize}
\item Gaussian Function
	\begin{itemize}
	\item $\displaystyle f(x) = \rm e^{-a(x+b)^2}$
		\begin{itemize}
		\item special form: $\displaystyle f(x) = \rm e^{-(x)^2}$
		\item alternative form: $\displaystyle f(x) = \rm e^{ax^2+bx+c}$
		\item no indefinite integral $\displaystyle \int_a^b \rm e^{-x^2}$
		\item only definite integral $\displaystyle \int_{-\infty}^{+\infty} \rm e^{-x^2}$
		\end{itemize}
	\end{itemize}
\item Direct Integral
	\begin{itemize}
	\Item \begin{align*}\displaystyle (\int^{+\infty}_{-\infty} \rm e^{-a(x+b)^2}dx)^2 &= \int^{+\infty}_{-\infty} \rm e^{-a(x+b)^2}dx\int^{+\infty}_{-\infty} \rm e^{-a(y+b)^2}dy \\ &= \int\int^{+\infty}_{-\infty} \rm e^{-a[(x+b)^2  + (y+b)^2]}d(x+b)d(y+b) = \int\int^{+\infty}_{-\infty} \rm e^{-a(x^2+y^2)}dxdy \\ &= \int_0^{2\pi}\int_0^{+\infty} \rm e^{-ar^2}rdrd\theta  \\ &= \frac \pi a \end{align*}
	
	$\displaystyle \Rightarrow \int^{+\infty}_{-\infty} \rm e^{-a(x+b)^2}dx = \sqrt{\frac \pi a}, \text{ alternatively } \int^{+\infty}_{-\infty} \rm e^{ax^2+bx+c}dx = \sqrt{\frac {\pi} {-a}} \cdot \rm e^{\frac {b^2}{4a} + c}$
	\end{itemize}

\item Even Moment of Gaussian Function
	\begin{itemize}
	\Item \begin{align*} \displaystyle \int_{-\infty}^{+\infty} x^{2n}\rm{e}^{-ax^2}dx &= (-1)^n\int^{+\infty}_{-\infty} \frac {\partial^n} {\partial a^n} \rm{e}^{-ax^2}dx \\ &= (-1)^n \frac {\partial^n} {\partial a^n} \int_{-\infty}^{+\infty} \rm{e}^{-ax^2}dx && \text{by parameter differeation} \\ &= (-1)^n\sqrt{\pi} \frac {\partial^n} {\partial a^n} a^{-\frac 12} \\ &= \sqrt {\frac {\pi}{a}} \frac {(2n-1)!!}{(2a)^n}, &&\text{where } !! \text{ is double factorial} \end{align*}
	\end{itemize}
\end{itemize}

 
\section{Probability Theory}

\subsection{Introduction}

\subsubsection{Background}
\begin{itemize}
\item Measuring Uncertainty
	\begin{itemize}
	\item Source of Uncertainty
		\begin{itemize}
		\item noise in reality \& observation
		\item finite size of data (limited information)
		\end{itemize}
	\end{itemize}
\item Derivation
	\begin{itemize}
	\item Quantifying Belief
		\begin{itemize}
		\item by Cox (1946): if numerical values used to
		represent degrees of belief, a simple set of axioms encoding common sense
		properties of such beliefs will lead \underline{uniquely} to a set of rules for manipulating degrees of
		belief that are equivalent to the \underline{sum and product rules of probability}
		\end{itemize}
	\item Measuring Uncertainty
		\begin{itemize}
		\item by Jaynes (2003): probability theory can be regarded as an extension of
		Boolean logic to situations involving uncertainty
		\end{itemize}
	\item Common Destination
		\begin{itemize}
		\item numerical quantities to measure uncertainty, derived from different sets of properties/axioms, behave precisely according to the rules of probability
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{The Basic}
\begin{itemize}
\item Notation
	\begin{itemize}
	\item $X,Y$: random variable
	\end{itemize}
\item Discrete 
		\begin{itemize}
		\item $P(X,Y)$: joint probability of $X,Y$ taking their values
		\item $P(X)$: marginal probability of $X$ taking its value
		\item $P(X|Y)$: conditional probability of $X$ taking its value given $Y$ observed / determined
	\end{itemize}
	\item Continuous
		\begin{itemize}
		\item $P(x) = P_X(x)$: cumulative probability of value for variable $X < x$
		\item $p(x)$: probability density,
			\begin{itemize}
			\item where $\displaystyle \lim_{\delta x \rightarrow 0} P(X\in(x,x+\delta x)) = \lim_{\delta x \rightarrow 0}p(x)\delta x \Rightarrow \displaystyle P(X\in(a,b))=\int_a^b p(x)dx$
			\item $\displaystyle \Rightarrow p(x )\ge 0 \text{ and } \int^{+\infty}_{-\infty} p(x) = 1$
			\item $\Rightarrow P(z) = \int_{-\infty}^z p(x)dx$
			\end{itemize}
		\end{itemize}
\item Basic Rules
	\begin{itemize}
	\item Sum Rule
		\begin{itemize}
		\item $\displaystyle P(X) = \sum_Y P(X,Y)$, where $X,Y$ are discrete
		\item $\displaystyle P(X) = \int_Y P(X,Y)$, where $X,Y$ are continuous
		
		(formal justification requires measure theory)
		\end{itemize}
	\item Product Rule
		\begin{itemize}
		\item $P(X, Y) = P(Y|X)P(X)$
		\item $P(X, Y) = P(Y)P(X)$, where $X,Y$ are independent
		\end{itemize}
	\item $\Rightarrow$ Bayes' Rule
		\begin{itemize}
		\item $\displaystyle P(Y|X) = \frac {P(X|Y)P(Y)} {P(X)} = \frac{P(X|Y)P(Y)}{\sum_YP(X|Y)P(Y)}$, where $Y$ are discrete
		\item $\displaystyle P(Y|X) = \frac {P(X|Y)P(Y)} {P(X)} = \frac{P(X|Y)P(Y)}{\int_YP(X|Y)P(Y)}$, where $Y$ are continuous
		\end{itemize}
	\end{itemize}
\item Interpretation of Bayes
	\begin{itemize}
	\item Normalization
		\begin{itemize}
		\item the $\sum$, $\int$ can be interpreted as a \textbf{normalization constant}
		
		$\Rightarrow \textbf{posterior} \propto \textbf{likelihood} \times \textbf{prior}$
		\end{itemize}
	\item Prior
		\begin{itemize}
		\item $P(Y)$: available probability of desired variable \textbf{before} anything observed
		
		$\Rightarrow Y$ usually model parameters
		\end{itemize}
	\item Posterior
		\begin{itemize}
		\item $P(Y|X)$: obtained probability of desired variable \textbf{after} observation
		
		$\Rightarrow$ if $X,Y$ independent, observation has no effect $\Rightarrow$ prior $=$ posterior
		\end{itemize}
	\item Likelihood
		\begin{itemize}
		\item $P(X|Y)$: how probable/likely of $X$ being observed under different setting of $Y$
		
		\end{itemize}
	\item Prior $\rightarrow$ Posterior
		\begin{itemize}
		\item a process of incorporating the evidence provided by observation
		\end{itemize}
	\end{itemize}
\end{itemize}
 
\subsection{Expectations and Covariances}

\subsubsection{Expectation}

\begin{itemize}
\item Definition
	\begin{itemize}
	\item Expectation of $f(x)$ under $p(x)$
		\begin{itemize}
		\item discrete $x$: $\displaystyle \mathbb{E}_{p}[f] = \sum_xp(x)f(x)$
		\item continuous $x$: $\displaystyle \mathbb{E}_{p}[f] = \int p(x)f(x)dx$
		\item approximation with $N$ points drawn from $p(x)$: $\displaystyle \mathbb E_p[f] \simeq \frac 1N \sum_{n=1}^N f(x_n) $
		
		(when $N\rightarrow \infty$, $\simeq$ becomes $=$)
		\end{itemize}
	\item Multivariate Expectation
		\begin{itemize}
		\item Marginal Expectation of $f(x,y)$ on $x$: $\displaystyle \mathbb{E}_x[f(x,y)] = \sum_xp(x)f(x,y)$ 
		
		(hence a function of $y$)
		\item Conditional Expectation $f(x)$ on $p(x|y)$: $\displaystyle \mathbb{E}[f|y] = \sum_x p(x|y)f(x)$
		\end{itemize}
	\end{itemize}
\item Independence
	\begin{itemize}
	\item Independent $x,y$
		\begin{itemize}
		\item $\displaystyle \mathbb{E}_{xy}[x,y] = \sum_{x,y}p(x,y)xy = \sum_{x,y}p(x)p(y)xy = \mathbb{E}[x]\mathbb{E}[y]$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Variance}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item Variance of $f(x)$:
		\begin{itemize}
		\Item \begin{align*} \text{var}[f] &= \mathbb{E}[(f(x) - \mathbb{E}[f(x)])^2] \\ &= \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2 \end{align*}
		\end{itemize}
	\item Covariance
	\end{itemize}
\end{itemize}

\subsubsection{Covariance}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item between Variables $x,y$
		\begin{itemize}
		\Item \begin{align*} \text{cov}[x,y] &= \mathbb{E}_{x,y}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])] \\ &= \mathbb{E}_{x,y}[xy] - \mathbb{E}[x]\mathbb{E}[y] \end{align*}
		\end{itemize}
	\item between Vectors $\mathbf x, \mathbf y$ (column vectors)
		\begin{itemize}
		\Item \begin{align*} \text{cov}[\mathbf x, \mathbf y] &= \mathbb{E}_{\mathbf x,\mathbf y}[(\mathbf x - \mathbb{E}[\mathbf x]) \cdot (\mathbf y^T -\mathbb{E}[\mathbf y^T])] \\ &= \mathbb{E}_{\mathbf x,\mathbf y}[\mathbf{xy}^T] - \mathbb{E}[\mathbf x]\mathbb E[\mathbf y^T] \end{align*}
		
		(pairwise covariance between components of $\mathbf x,\mathbf y$)
		\end{itemize}
	\item within Vector $\mathbf x$
		\begin{itemize}
		\item $\text{cov}[\mathbf x] \equiv \text{conv}[\mathbf x, \mathbf x]$ 
		
		(pairwise covariance between its components)
		\end{itemize}
	\end{itemize}
\item Independence Variable
	\begin{itemize}
	\item Independent $x,y$
		\begin{itemize}
		\item $\text{cov}[x,y] = \mathbb E_{x,y}[xy] -\mathbb{E}[x]\mathbb{E}[y] = 0$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Transformations of Random Variables}

\subsubsection{Inverse Image}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item function $g:\mathbb{R} \rightarrow \mathbb{R}$
		\item set $A$ in $\mathbb{R}$
		\end{itemize}
	\item Inverse Image on Set $A$
		\begin{itemize}
		\item $\displaystyle g^{-1}(A) = \{x\in\mathbb{R}|g(x)\in A\}$
		
		$\Leftrightarrow x\in g^{-1}(A) \text{ if and only if } g(x) \in A$
		
		interpretation: for each element in $A$, get its original value before $g$ applied
		\end{itemize}
	\end{itemize}
\item Properties
	\begin{itemize}
	\item $g^{-1}(\mathbb{R}) = \mathbb{R}$, as $g$ is defined on $\mathbb{R}$
	\item $\forall \text{ set } A, g^{-1}(A^c) = g^{-1}(A)^c$, where $A^c$ is the complement of set $A$
	\item $\displaystyle \forall \text{ collection of sets } \{A_\lambda | \lambda \in \Lambda\}, g^{-1}\left( \bigcup_\lambda A_\lambda \right) = \bigcup_\lambda g^{-1}(A_\lambda)$
	\item General Transformation $Y=g(X)$
		\begin{itemize}
		\item $P(Y\in A) = P(g(X)\in A) = P(X \in g^{-1}(A))$
		\end{itemize}
	\end{itemize}
	
\end{itemize}

\subsubsection{Discrete Variable}
\begin{itemize}
\item Variable
	\begin{itemize}
	\item $X$: random variable with probability mass function $P_X(x)$
	\item $Y = g(X)$, with probability mass function $P_Y(y)$
	\end{itemize}
\item Probability Mass Function
	\begin{itemize}
	\item $\displaystyle P_Y(y) = \sum_{x\in g^{-1}(y)} P_X(x)$, as $X=x$ is independent and mutually exclusive	
	
	note: $g^{-1}(y)$ denotes $g^{-1}(\{y\})$
	\item Example
		\begin{itemize}
		\item uniform random variable $X$ on $\{-n,...,n\}$ with $Y=|X|$
		
		$\Rightarrow P_X(x) = \frac 1 {2n+1}$
			
		$\Rightarrow P_Y(y) = \begin{cases} \frac 1 {2n+1}, &x=0, \\ \frac 2 {2n+1}, &x\neq0. \end{cases}$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Continuous}
\begin{itemize}
\item Variable
	\begin{itemize}
	\item $X$: random variable with cumulative distribution $P_X(x)$, density $p_X(x)$
	\item $Y=g(X)$, with cumulative distribution $P_Y(y)$, density $p_Y(y)$
	\end{itemize}

\item Cumulative Distribution
	\begin{itemize}
	\item Strictly Monotone Increasing $g$
		\begin{itemize}
		\item $P_Y(y) = P(Y\le y) = P(g(X) \le y) = P(X \le g^{-1}(y)) = P_X(g^{-1}(y))$
		\end{itemize}
	\item Strictly Monotone Decreasing $g$
		\begin{itemize}
		\item $P_Y(y) = P(Y\le y) = P(g(X) \le y) = P(X \ge g^{-1}(y)) = 1 - P_X(g^{-1}(y))$
		\end{itemize}
	\end{itemize}

\item Probability Density
	\begin{itemize}
	\item Strictly Monotone $g$ (an one-to-one function)
		\begin{itemize}
		\item $\displaystyle p_Y(y) = \frac d {dy} P_Y(y) = \frac {d P_Y(y)}{d\space  g^{-1}(y)} \frac{d \space g^{-1}(y)}{dy} = p_X(g^{-1}(y)) \abs*{\frac{d}{dy} g^{-1}(y)}$, 
		
		as $g^{-1}$ has the same monotony as $g$, combined with the sign in $P_Y$ to give the $\abs*\cdot$
		
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Gaussian Distribution}
 
\subsubsection{Definition}

\begin{itemize}
\item Univariate Gaussian
	\begin{itemize}
	\item Variable
		\begin{itemize}
		\item mean: $\mu$
		\item variance: $\sigma^2 \Rightarrow$ reciprocal of the variance $\beta = \frac 1 {\sigma^2}$ (also called precision)
		\end{itemize}
	\item Probability Dense Function (PDF)
		\begin{itemize}
		\item $\displaystyle \mathcal N(x|\mu, \sigma^2) = \frac 1 {(2\pi \sigma^2)^{1/2}} \exp \left\{ -\frac 1 {2\sigma^2}(x-\mu)^2 \right\}$
		
		$\Rightarrow$ satisfying probability axioms: $\displaystyle \mathcal N(x|\mu,\sigma^2)>0 \text{ and } \int_{-\infty}^{+\infty} \mathcal N (x|\mu,\sigma^2) dx = 1$
		\end{itemize}
	\item Expectation
		\begin{itemize}
		\item $\displaystyle \mathbb{E}[x] = \int_{-\infty}^{+\infty}\mathcal{N}(x|\mu, \sigma^2)xdx=\mu$
		\begin{align*}
		\Rightarrow \displaystyle \mathbb E [x^2] &= \int_{-\infty}^{+\infty} \mathcal{N}(x|\mu, \sigma^2)x^2dx \\ 
		&= \frac 1 {(2\pi \sigma^2)^{1/2}} \int_{-\infty}^{+\infty} x^2 \exp \left\{ -\frac 1 {2\sigma^2}(x-\mu)^2 \right\} dx \\ 
		&= \pi^{-\frac 12} \int_{-\infty}^{+\infty} (\sqrt{2\sigma^2}x+\mu)^2 \exp(-x^2) dx, \text{ substituting } a=\frac{x-\mu}{\sqrt{2\sigma^2}} \\ 
		&= \pi^{-\frac 12} (2\sigma^2\int_R x^2\rm{e}^{-x^2}dx + 2\mu\sqrt{2\sigma^2}\int_R x\rm{e}^{-x^2}dx + \mu^2 \int_R \rm{e}^{-x^2}dx ) \\ 
		&= \pi^{-\frac 12} ( 2\sigma^2\int_R x^2\rm{e}^{-x^2}dx + 2\mu\sqrt{2\sigma^2}\cdot 0 + \mu^2\sqrt\pi ) \\ 
		&= 2\sigma^2\pi^{-\frac 12}\int_R x^2\rm{e}^{-x^2}dx + \mu^2 \\ 
		&= \sigma^2 + \mu^2, \text{ by 2nd moment of Guassian or } (x\rm e^{-x^2})' = e^{-x^2}-2x^2\rm e^{-x^2}
		\end{align*}
		\end{itemize}
	\item Variance
		\begin{itemize}
		\item $\displaystyle \text{var}[x] = \mathbb E[x^2] - \mathbb E[x]^2 = \sigma^2$
		\end{itemize}
	\end{itemize}
\item Multivariate ($d$-dimensional) Gaussian
	\begin{itemize}
	\item Variable
		\begin{itemize}
		\item mean: $\mu \in \mathbb R^d$
		\item covariance matrix: $\Sigma_{d\times d}$
		\end{itemize}
	\item Probability Dense Function (PDF)
		\begin{itemize}
		\item $\displaystyle \mathcal N_d(\mathbf x|\mathbf{\mu},\mathbf \Sigma) = \frac 1 {(2\pi)^{d/2} |\Sigma|^{1/2}} \exp \left\{ -\frac 1 2 (x-\mu)^T \Sigma^{-1} (x-\mu) \right\} $,
		
		noted as $X\sim \mathcal N_d (x|\mu,\Sigma)$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Multivariate Gaussian}
\begin{itemize}
\item Dimensionality
	\begin{itemize}
		\item Volume of High Dimensional Ball
		\item Volume of High Dimensional Cube
		\item High Dimensional Distribution
		\item High Dimensional Gaussian
			\begin{itemize}
			\item probability density with respect to radius $r$ for various dimension $D$ \\
				$\Rightarrow$ most density are in a thin shell at a specific $r$			
			\end{itemize}
			\begin{figure}[ht]
			\includegraphics[width=0.5\linewidth,center]{./Math/"multivariate gaussian-mass of distribution".jpg}
			\end{figure}
		
	\end{itemize}
\end{itemize}

\subsubsection{}
\begin{itemize}
\item Convolution of Gaussian
	\begin{itemize}
	\item Integral of Gaussians $\int G_1G_2dx$
		\begin{itemize}
		\item $G_1\sim \mathcal N_d(x|a,A), \space G_2 \sim \mathcal N_d(x|b,B)$
		\end{itemize}
		\begin{align*} \displaystyle \Rightarrow & \int \mathcal N_d(x|a,A) N_d(x|b,B)dx \\ = & \int \frac 1 {(2\pi)^{d/2} |A|^{1/2}} e^{-\frac 1 2 (x-a)^TA^{-1}(x-a)} \frac 1 {(2\pi)^{d/2}|B|^{1/2}} e^{-\frac 1 2 (x-b)^TB^{-1}(x-b)} dx \\ = & \int \frac 1 {(2\pi)^{d/2} |A|^{1/2}} \frac 1 {(2\pi)^{d/2}|B|^{1/2}} e^{-\frac 12 [(x-a)^TA^{-1}(x-a)+(x-b)^TB^{-1}(x-b)]} \\ = & \int \frac 1 {(2\pi)^{d/2} |A|^{1/2}} \frac 1 {(2\pi)^{d/2}|B|^{1/2}}  e^{-\frac 12 [(x-c)^T (A^{-1}+B^{-1}) (x-c)+(a-b)^TC(a-b)]},\\ & \text{where } c=(A^{-1}+B^{-1})^{-1}(A^{-1}a+B^{-1}b), C=A^{-1}(A^{-1}+B^{-1})^{-1}B^{-1} = (A+B)^{-1} \\ = & \frac {\begin{vmatrix}(A^{-1}+B^{-1})^{-1}\end{vmatrix}^{1/2}} {(2\pi)^{d/2}|A|^{1/2}|B|^{1/2}} e^{-\frac 12 (a-b)^TC(a-b)} \int \frac 1 {(2\pi)^{d/2}\begin{vmatrix}(A^{-1}+B^{-1})^{-1}\end{vmatrix}^{1/2}} e^{-\frac 12 (x-c)^T(A^{-1}+B^{-1}) (x-c)} dx \\ =& \frac {\begin{vmatrix}(A^{-1}+B^{-1})^{-1}\end{vmatrix}^{1/2}} {(2\pi)^{d/2}|A|^{1/2}|B|^{1/2}} e^{-\frac 12 (a-b)^TC(a-b)} \\ =& \frac 1 {(2\pi)^{d/2} (|A||B||A^{-1}+B^{-1}|)^{1/2}} e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |ABA^{-1}+ABB^{-1}|^{1/2}} e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |ABA^{-1}+A| } e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |A(B+A)A^{-1}| } e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \\=& \frac 1 {(2\pi)^{d/2} |A+B|^{1/2} } e^{-\frac 12 (a-b)^T (A+B)^{-1} (a-b)} \end{align*}
			
	\item $\Rightarrow$ Convolution of Gaussians $G1* G2$
		\begin{itemize}
		\item $G_1\sim \mathcal N_d(a,A),G_2\sim \mathcal N_d(b,B)$
		\end{itemize}
		\begin{align*} \displaystyle G_1* G_2 (z) &= \int G_1(x)G_2(z-x)dx \\ &= \int \mathcal N_d (x|a,A)\mathcal N_d(z-x|b,B)dx \\ &= \int \mathcal N_d (x|a,A) \cdot \frac 1 {(2\pi)^{d/2} |B|^{1/2}} e^{-\frac 12 (z-x-b)^TB^{-1} (z-x-b)} dx \\ &= \int \mathcal N_d (x|a,A)\mathcal N_d(x|z-b,B)dx \\&= \frac 1 {(2\pi)^{d/2}|A+B|^{1/2}} e^{-\frac 12(z-(a+b))^T (A+B)^{-1} (z-(a+b))} \\&= \boldsymbol{\mathcal N_d(z|a+b,A+B)}  
		\end{align*}
	\end{itemize}
\end{itemize}
 
 
\subsection{Bayesian Interpretation of Probability}
\subsubsection{Contrasting Frequentist Estimator}
\begin{itemize}
\item Posterior $\displaystyle p(\mathbf w|\mathcal D) = \frac {p(\mathcal D|\mathbf w)p(\mathbf w)}{p(\mathcal D)}$
	\begin{itemize}
	\item Notation
 		\begin{itemize}
 		\item $\mathcal D$ the observed dataset
 		\item $\mathbf w$ the vector for model parameters
 		\end{itemize}
	\item Bayesian
		\begin{itemize}
		\item only one single dataset $\mathcal D$ (the observed one)
 		\item uncertainty expressed as distribution over $\mathbf w$
 		\item model's error: use likelihood / posterior directly (or after taking $\log$)
 		\item pros
 			\begin{enumerate}
 			\item naturally incorporating prior knowledge as prior distribution (of $\mathbf w$)
 			\end{enumerate}
 		\item cons
 			\begin{enumerate}
 			\item prior usually selected for mathematic convenience 
 			\end{enumerate}
 		\end{itemize}
 	\item Frequentist Estimator
 		\begin{itemize}
 		\item parameters $\mathbf w$ already determined / fixed by 'estimator' (model)
 		\item error bars of the model obtained by considering the distribution over $\mathcal D$
 		\item model's error: bootstrap procedure
 			\begin{enumerate}
 			\item generate dataset(s) by drawing from the observed $\mathcal D$ with replacement
 			\item sampling $L$ datasets with the same size as $\mathcal D$
 			\item error = variability of predictions between the sampled datasets
 			\end{enumerate}
 		\item pros
 			\begin{enumerate}
 			\item protect the conclusion from false prior knowledge
 			\end{enumerate}
 		\item cons
 			\begin{enumerate}
 			\item sensitive to observation (extreme cases), especially under small dataset
			\end{enumerate}
		\end{itemize}
	\end{itemize}
\end{itemize}
 
\subsubsection{Parameter Estimation}
\begin{itemize}
\item Bias vs. Variance


\item Taking Logarithm
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item monotonically increasing function $\Rightarrow \arg\max_{\theta}f(x;\theta) = \arg\max_\theta\log f(x;\theta)$
		\item simplify mathematical analysis $\Rightarrow \prod \rightarrow \sum$
		\item numerical stability $\Rightarrow \text{ avoid } \prod (\text{small probabilities})$
		
		(may otherwise underflow the numerical precision)
		\end{itemize}
	\end{itemize}

\item Maximum Likelihood Estimation for Gaussian
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $X=\{x_1,...,x_N\}$: observed $N$ data points
		\end{itemize}
	\item Assumption
		\begin{itemize}
		\item data points are i.i.d. (identically and independently distributed)
		\item underlying distribution is Gaussian $\mathcal N(\mu, \sigma)$
		\end{itemize}
	\item Likelihood
		\begin{itemize}
		\item $\displaystyle p(X|\mu,\sigma^2) = \prod_{n=1}^N \mathcal(x_n|\mu,\sigma^2)$
		\item $\displaystyle \Rightarrow \ln p(X|\mu,\sigma) = -\frac 1{2\sigma^2} \sum_{n=1}^N(x_n-\mu)^2 -\frac N2 \ln \sigma^2 -\frac N2 \ln (2\pi)$
		\end{itemize}
	\item Solution
		\begin{itemize}
		\item $\displaystyle \text{let } \frac {\partial}{\partial \mu} \ln p(X|\mu,\sigma^2) = 0 \Rightarrow \mu_{\text{ML}}=\frac 1N \sum_{n=1}^Nx_n $
		\item $\displaystyle \text{let } \frac {\partial}{\partial \sigma^2} \ln p(X|\mu,\sigma^2)=0 \Rightarrow \sigma_\text{ML}^2 = \frac 1N \sum_{n=1}^N (x_n- \mu_\text{ML})$
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\Item \begin{align*} \displaystyle \mathbb E[\mu_\text{ML}] = \mathbb E[\frac 1N \sum_{n=1}^Nx_n] = \frac 1N \sum_{n=1}^N \mathbb E[x_n] = \mu \tag{as \(x_1,...,x_N\) i.i.d, drawn from \(\mathcal N(\mu,\sigma^2)\), thus \(\sim \mathcal N(\mu,\sigma^2)\)} \end{align*}
		$\Rightarrow$ unbiased estimation of mean
		
		\Item \begin{align*} \mathbb E[\sigma_\text{ML}^2] &= \mathbb E[\frac 1N \sum_{n=1}^N(x_n-\mu_{\text{ML}})^2] \\
		&= \mathbb E [ \frac 1N \sum_{n=1}^{N}(x_n^2-2\mu_\text{ML}x_n+\mu_\text{ML}^2) ] \\ 
		&= \frac 1N \sum_{n=1}^N \mathbb E[x_n^2] - \mathbb E[\frac 1N \sum_{n=1}^N 2x_n \mu_\text{ML}] + \mathbb E[\mu_\text{ML}^2] \\ 
		&= \frac 1N \sum_{n=1}^N \mathbb E[x_n^2] - 2\mathbb E[\mu_\text{ML}^2] + \mathbb E[\mu_\text{ML}^2] \\
		&= \frac 1N \sum_{n=1}^N \mathbb E[x_n^2] - \frac 1{N^2} \sum_{i,j=1}^N \mathbb E[x_ix_j] \\
		&= \frac 1N \sum_{n=1}^N (\sigma^2 + \mu^2) - \frac 1{N^2} [N(N-1)\mu^2 + N(\sigma^2+\mu^2)] \tag{by 2nd moment of Gaussian \(\mathbb E[x^2]\) and i.i.d assumption} \\
		&= \left(\frac {N-1}N  \right) \sigma^2 \end{align*}
		$\Rightarrow$ biased variance \textbf{!} \\
		$\Rightarrow $ unbiased variance $\displaystyle \hat \sigma^2 = \frac N {N-1}\sigma^2_\text{ML} = \frac 1{N-1} \sum_{n=1}^N (x_n-\mu_\text{ML})^2 $ \\
		interpretation: $N-1$  degree of freedom, \\
		(as calculating $\sigma^2$ needs $\mu$, which help pin down $x_N$ given $x_1,...,x_{N-1}$) \\
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Predictive Distribution}
\begin{itemize}	
\item Probabilistic Prediction
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $\bf x, \bf t$: vector of data examples and corresponding ground truth
		\item $\bf w$: model parameters
		\item $x, t$: new data example for prediction and its ground truth
		\end{itemize}
	\item Prediction by Model
		\begin{itemize}
		\item $p(t|x, \mathbf{w}')$, where $\mathbf w'$ is the best fit parameters founded
		\end{itemize}
	\item Prediction by Data
		\begin{itemize}
		\item $\displaystyle p(t|x,\mathbf{x},\mathbf{t}) = \int p(t|x,\mathbf{w})p(\mathbf{w}|\mathbf{x},\mathbf{t}) d\mathbf w$, where $\bf w$ marginalized over its posterior 
		\end{itemize}
	\end{itemize}
\end{itemize}