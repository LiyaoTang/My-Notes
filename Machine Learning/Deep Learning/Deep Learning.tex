\chapter{Deep Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interview of Fame}

\subsection{Geoffrey Hinton}
\subsubsection{Knowledge Embedding}
\begin{itemize}
	\item BP
	\begin{itemize}
		\item psychology view: knowledge in vectors
		\item semantic AI: knowledge graph
		\item BP algorithm can interpret \& convert between feature vector and graph representation (with some embedding)
	\end{itemize}
	\item Boltzmann Machine
	\begin{itemize}
	\item Leaning Algorithm on Density Net
		\begin{itemize}
		\item same information in forward \& backward propagation to learn feature embedding
		\end{itemize}
	\item Restricted Boltzmann Machine (RBM)
		\begin{itemize}
		\item ways of learning in deep dense net with fast inference
		\item iterative learning (adding layer after the above trained)
		\item ReLU $\Leftrightarrow$ a stack of sigmoid functions (approximately) in RBM
		\item ReLU units initialized to identity for efficient learning
		\end{itemize}
	\end{itemize}
	
	\item EM
		\begin{itemize}
		\item EM with Approximate E Step
		\end{itemize}
	
	\item vs. Symbolic AI
		\begin{itemize}
		\item Symbolic AI: symbolic logic-like expression to do reasoning
		\item yet, maybe state vector to represent knowledge
		\end{itemize}
\end{itemize}

\subsubsection{Brain Science}
\begin{itemize}
\item Brain: Nets Implemented by Evolution
	\begin{itemize}
	\item trying to train without BP
	\item doing BP (get derivatives) with re-construction error (auto-encoder)
	\end{itemize}
\end{itemize}

\subsubsection{Memory in Nets}
\begin{itemize}
\item Fast Weights for Short-term Memory
\item Capsule Net
	\begin{itemize}
	\item structured knowledge representation in each unit (feature with sets of property)
	\item $\Rightarrow$ enable nets to vote rather than filtering - thus better generalization
	\item now working: published in \textbf{2017 NIPS}
	\end{itemize}
\end{itemize}

\subsubsection{Unsupervised Learning}
\begin{itemize}
\item Importance
	\begin{itemize}
	\item better than human eventually (as supervised learning has limited maximum)
	\item GAN as a breakthrough
	\end{itemize}
\end{itemize}

\subsubsection{"Slow" Feature}
\begin{itemize}
\item Non-linear Transform to Find Linear Transform
	\begin{itemize}
	\item find a latent representation containing linear transform to do the work
	\item e.g. change viewpoints: pixels $\rightarrow$ coordinates $\rightarrow$ linear transform $\rightarrow$ back to pixels
	\end{itemize}
\end{itemize}

\subsubsection{Relations between Computers}
\begin{itemize}
\item showing computer data to work
	\begin{itemize}
	\item instead of programming it to work
	\end{itemize}
\end{itemize}


\subsection{Pieter Abbeel}
\subsubsection{Deep Reinforcement Learning}
\begin{itemize}
\item Overall Challenge 
	\begin{itemize}
	\item Representation
	\item Exploration Problem
	\item Credit Assignment
	\item Worst Case Performance
	\end{itemize}
\item Advantage (Deep Nets in RL)
	\begin{itemize}
	\item network capturing the representation (state vector)
	\end{itemize}
\item Question in DRL
	\begin{itemize}
	\item how to learn safely
	\item how to keep learning (under small negative samples) e.g. better than human
	\item can we learn the reinforcement learning program (RL in the RL)
	\item long time horizon
	\item use experience across tasks
	\end{itemize}
\item Success of DRL
	\begin{itemize}
	\item simulated robot inventing walking... $\Rightarrow$ single general algorithms to learn
	\end{itemize}
\end{itemize}

\subsection{Ian Goodfellow}
\subsubsection{Generative Adversarial Networks}
\begin{itemize}
\item Generative Models
	\begin{itemize}
	\item Resembling
		\begin{itemize}
		\item trained to optimized the distribution behind training data \\ 
		(then sampled from that distribution to get more imaginary training data)
		\item $\Rightarrow$ produce data to resemble the training data
		\end{itemize}
	\item Usage
		\begin{itemize}
		\item semi-supervised learning
		\item data augmentation
		\item simulating scientific experiment
		\end{itemize}
	\item Previous Ways
		\begin{itemize}
		\item Boltzmann Machine
		\item Sparse Coding
		\end{itemize}
	\item Now: Generative Adversarial Networks (GANs)
	\item Future
		\begin{itemize}
		\item increase reliability of GANs (stabilizing)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Yoshua Bengio}
\subsubsection{Thoughts}
\begin{itemize}
\item Fallacy
	\begin{itemize}
	\item Smoothness in Nonlinearity
		\begin{itemize}
		\item to ensure non-zero gradients every where
		\end{itemize}
	\end{itemize}
\item Surprising Fact
	\begin{itemize}
	\item ReLU in Deep Net
		\begin{itemize}
		\item inspired initially by biological connection
		\end{itemize}
	\end{itemize}
\item \textbf{Distribution v.s. Symbolic Representation}
	\begin{itemize}
	\item Distributed Representation 
		\begin{itemize}
		\item distributed in lots of units, instead of a symbolic representation in a single cell \\
		(agree on Geoffrey Hinton)
		\end{itemize}
	\item Curse of Dimensionality
		\begin{itemize}
		\item neural net's distributed representation for joint distribution over random variables
		\end{itemize}
	\item $\Rightarrow$ Word Embedding
		\begin{itemize}
		\item generalized to joint distribution over sequence of words
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Works}
\begin{itemize}
\item Piecewise Linear Activation (PLU)
\item Unsupervised Learning
	\begin{itemize}
	\item Focus
		\begin{itemize}
		\item Denoising auto-encoder
		\item GANs
		\end{itemize}
	\item Importance
		\begin{itemize}
		\item human ability: self-teaching, building world-model from perception
		\end{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item underlying concept across two fields: machine can learn through interactions \\
		$\Rightarrow$ learning "good" representation (yet, what is "good")
		\end{itemize}
	\item Possible Directions
		\begin{itemize}
		\item loss function: not even defined for each task \\ 
		(not knowing which is good for what?)
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Machine Translation (Founder)
	\item Generalized into Other Fields
	\end{itemize}
\item Back-prop in Brains (Neural Science
	\begin{itemize}
	\item Reasons for Efficiency of Backprop
	\item Larger Family behind Credit Assignment
	\end{itemize})
\end{itemize}

\subsection{Yuanqing Lin}
\subsubsection{National Deep Learning Lab}
\begin{itemize}
\item Paddle Paddle
\item Baidu Lab
\end{itemize}

\subsection{Andrej Karpathy}
\subsubsection{Human Benchmark}
\begin{itemize}
\item Programming by Showing
	\begin{itemize}
	\item Requirement
		\begin{itemize}
		\item input + output as specification
		\item metric as goal
		\end{itemize}
	\item Writer
		\begin{itemize}
		\item the optimizer
		\end{itemize}
	\end{itemize}
\item Understanding Importance of Benchmark
	\begin{itemize}
	\item importance to do better given the current performance on the dataset \\
	(as important increase after passing human error)
	\end{itemize}
\item Understanding Network Behavior
	\begin{itemize}
	\item compared to the process of human decision
	\end{itemize}
\end{itemize}
\subsubsection{Transfer Learning}
\begin{itemize}
\item Image Task
	\begin{itemize}
	\item feature extractor + fine tune/modification onto various task
	\end{itemize}
\end{itemize}

\subsection{Ruslan Salakhutdinov}
\subsubsection{Restricted Boltzmann Machine}
\begin{itemize}
\item Auto Encoder
	\begin{itemize}
	\item Encoding All Kind of Data
		\begin{itemize}
		\item from digit to face, document, etc...
		\item deeper and deeper structure
		\end{itemize}
	\end{itemize}
\item Training Boltzmann Machine
	\begin{itemize}
	\item Pretraining
		\begin{itemize}
		\item increase the low boundary by training the previous layer
		\item then add another layer to train, ...
		\end{itemize}
	\item Direct Training (with GPU)
		\begin{itemize}
		\item similar, or better result
		\end{itemize}
	\end{itemize}
\item Boltzmann Machine Ability
	\begin{itemize}
	\item Generative Model
		\begin{itemize}
		\item model coupling distributions in data \\
		$\Rightarrow$ scalable (more scalable than current model\&operation)
		\item only way to train the model in the early age
		\end{itemize}
	\end{itemize}
\item Progress on Generative Model
	\begin{itemize}
	\item probabilistic max pooling
	\item variational encoder
	\item deep energy model
	\item semi-supervised Model
	\end{itemize}
\end{itemize}

\subsection{}
\subsubsection{title}
\begin{itemize}
\item 
\end{itemize}

\subsection{Research}
\subsubsection{Topics}
\begin{itemize}
\item Point Cloud
	\begin{itemize}
	\item Operations on Points: how to embed location in operation
		\begin{itemize}
		\item select fixed number of points via coord?: then take weighted average (conv) / max (pooling) on them
		\item need a "select input points" op: like deformable conv?
		\end{itemize}
	\item Bounding Box Directly from Points: no voxel
		\begin{itemize}
		\item clustering + regression on each cluster?
		\end{itemize}
	\end{itemize}
	
\item Unsupervised Learning
	\begin{itemize}
	\item Deep Belief Nets
	\end{itemize}

\item Reinforcement Learning
	\begin{itemize}
	\item Deep Reinforcement Learning
		\begin{itemize}
		\item scalable system
		\item communicative\& cooperating agents
		\end{itemize}
	\end{itemize}

\item One-shot / Transfer Learning
	\begin{itemize}
	\item Learning the Ability to Learn
	\end{itemize}

\item General AI
	\begin{itemize}
	\item Structure for General Task
		\begin{itemize}
		\item neural network or other structure, shared for multiple tasks \\
		(instead of breaking down to different parts like segmentation, detection, etc.) \\
		(instead of the split of cv, nlp, plaining, etc.)
		\item $\Rightarrow$ a full agent (instead of decomposed function) \\
		$\Rightarrow$ optimization method/objective need to be carefully defined
		\end{itemize}
	\item Attempt for General AL
		\begin{itemize}
		\item scaling up supervised learning: imitating human
		\item unsupervised learning: AIXI, artificial evolution, etc.
		\end{itemize}
	\end{itemize}

\item AI Security
	\begin{itemize}
	\item Anti Inducing
		\begin{itemize}
		\item NOT to be fooled/induced to do unappropriated things \\
		(even if algorithm is right)
		\end{itemize}
	\item Built-in Security
	\end{itemize}
\item Fairness in AI
	\begin{itemize}
	\item Dealing Societal Issue
	\item Reflecting Preferred Bias
	\end{itemize}
\item Auto Optimization (Hyperparameter Tunning)
	\begin{itemize}
	\item Swarm Optimization
	\item Expectation Maximization
		\begin{itemize}
		\item target variable $\theta=$ hyperparameters
		\item hidden variable $Z=$ weights of network
		\item data $X=$ dataset
		\end{itemize}
	$\Rightarrow$ 
		\begin{itemize}
		\item E-step: evaluate $\displaystyle \mathbb E_{Z|\theta_n,X}(\ln P(Z,X|\theta))$
			\begin{itemize}
			\item $\ln P(Z, X|\theta)$: log likelihood of hyperparam $\theta$ (for weights \& data to be observed)
			\item $P(Z|\theta_n, X)$: posterior of weights $Z$ 
			\end{itemize}
		$\Rightarrow$ evaluate (approximate) the expectation of the log likelihood of hyperprarm $\theta$ \\
		(from a functional view, train with $\theta_0-\theta_N$, evaluate model $M$ times in training, thus with weights $Z_{00}-Z_{NM}$) \\
		$\Rightarrow$ a matrix with $n$ as row entry, $m$ as column entry, mapping to both $\ln P(Z, X|\theta), P(Z|\theta_n, X)$
		$\Rightarrow$ then marginalize (taking the expectation) over $Z$, to get a (sampled) function over $\theta$
		\item M-step: maximize the result function from E-step
			\begin{itemize}
			\item fit a curve \& maximize w.r.t hyperparams $\theta$
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item World Understanding: after perception
	\begin{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item machine learns from interactions
		\item machine builds a representation of world (like human ability, without fine label)
		\end{itemize}
	$\Rightarrow$ building world-model from perception
	\item Causality Mining
	\end{itemize}
\item Model Interpretation
	\begin{itemize}
	\item Logical Formalization
		\begin{itemize}
		\item deep learning can be understood logically \\
			e.g. what make deep net training harder? understand the limit of current algorithm/model and \textbf{why}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Advises}
\begin{itemize}
\item Learning Direction
	\begin{itemize}
	\item Math
		\begin{itemize}
		\item statistic
		\item linear algebra
		\item calculus
		\item optimization
		\end{itemize}
	\end{itemize}
\item Reading
	\begin{itemize}
	\item read a little bit \& find somewhere intuitively not right
	\begin{itemize}
		\item good intuition: eventually work; \\ 
		bad intuition: not working no matter what it is doing
		\item if other doubts your idea as bullshit $\Rightarrow$ a sign for real good result
	\end{itemize}
	\item a supervisor with similar belief
	\item PhD vs. Company
		\begin{itemize}
		\item amount of mentoring
		\item faster if dedicated supervisor available
		\item resource
		\end{itemize}
	\end{itemize}

\item Practice
	\begin{itemize}
	\item open-source learning resource
	\item open source contribution
		\begin{itemize}
		\item contribute to open source framework (e.g. conv on sparse matrix in TF)
		\item implement the paper, the open source it (as a tool for other)
		\item work on a projected and open source it \\
		$\Rightarrow$ the stage (e.g. github) will bring people to you
		\end{itemize}
	\item implement the tools: to find out how \& why it works \\
	$\Rightarrow$ derive theories from the 
	\item full stack of understanding \\ 
	$\Rightarrow$ understand the implementation under the deep learning framework
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Basic Neutral Network}

\subsection{Advantages}
\subsubsection{Large/Big Data}
\begin{itemize}
\item Larger Maximum Capability
	\begin{itemize}
	\item Curve given Amount of Data
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/background-largedata".png}
	\end{figure}
	\item Reasons
		\begin{itemize}
		\item the scale of data (labeled)
		\item the scale of neural network (computability)
		\item the scale of efficiency: e.g. ReLu, faster parallel algorithm
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Flexibility}
\begin{itemize}
\item Different Structures for Different Tasks
	\begin{itemize}
	\item Same Data \& Task
		\begin{itemize}
		\item changing settings/structures of deep learning model can make a difference \\
		(v.s. SVM, etc.)
		\end{itemize}
	\end{itemize}

\item Ability to Choose Basis Functions
	\begin{itemize}
	\item Functional View
		\begin{itemize}
		\item $\displaystyle y(\mathbf {x}, \mathbf w)=f(\mathbf w^T\phi(\mathbf x)), \text{ where } \phi \text{ is basis function }, f(\cdot) \text{ is net as a function}$
		\end{itemize}
	\item Learning $\phi$: choose embedding $\Rightarrow$ choose basis function
	\item Learning $\mathbf w$: choose which feature / basis functions more useful
	\end{itemize}
	
\item Solving Bias-Variance Trade-off
	\begin{itemize}
	\item Complexity + Data/Regularization
		\begin{itemize}
		\item easy complexity via depth, size \\
		$\Rightarrow$ reduce bias, without hurting variance by utilizing big data
		\item easy regularization via L$2$ ant etc.\\ 
		$\Rightarrow$ prevent high variance without hurting bias much in a deep/big net
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Power of Depth}
\begin{itemize}
\item Deep Representation
	\begin{itemize}
	\item Low-level $\rightarrow$ High-level
		\begin{itemize}	
		\item multiple layers to choose \& combine useful information (creating new feature/basis) \\
		$\Rightarrow$ next layer use chosen/combined simple basis to build more complex one
		\item $\Rightarrow$ an hierarchy from low-level information to high-level information
		\end{itemize}
	\end{itemize}

\item Circuit Theory
	\begin{itemize}
	\item Power of Combination
		\begin{itemize}
		\item functions that can be compactly represented by a depth $k$ architecture might require an exponential number of computational nodes using a depth $k-1$ architecture \\
		(from the perspective of factorization)
		\end{itemize}
	\end{itemize}
\textbf{Yet, start from the SHALLOW (logistic regression) before trying the deep}
\end{itemize}

\subsection{Problem}
($n$ units in one hidden layer)

\subsubsection{Weight-space Symmetries} 
\begin{itemize}
\item Symmetries in Activation Function
	\begin{itemize}
	\item $\mathcal{O}(2^n)$, e.g. $\arctan(-x) = -\arctan(x) \Rightarrow$ changing signs of all input \& output has the same mapping (reduce effective data)
	\end{itemize}
\item Positional Combination in One Layer
	\begin{itemize}
	\item $\mathcal{O}(n!)$ exchange unit with each other (together with their input output weights) $\Rightarrow$ mapping stay the same
	\end{itemize}
\end{itemize}
$\Rightarrow \mathcal O(n!2^n)$ overall weight-space symmetries

\subsubsection{High-Dimension Search Space}
\begin{itemize}
\item Multiple Critical Points
	\begin{itemize}
	\item Symmetries
		\begin{itemize}
		\item at least $\mathcal O (n!2^n)$ critical points ($\nabla E(w) = 0$), where $E(w)$ is error function \\
		due to weight-space symmetries
		\end{itemize}
	\item Saddle Points
		\begin{itemize}
		\item both the bottom (in one dimension) and the top for another
		\item due to high-dimension weight space \\ 
		$\Rightarrow$ more likely to have functions being convey/convex in different dimensions
		\end{itemize}
	\item Local Optima
		\begin{itemize}
		\item less then saddle points in amount, due to high-dimension weight space \\ 
		e.g. usually $\ge 10^4$-D for modern deep nets
		\end{itemize}
	\end{itemize}
\item Plateaus
	\begin{itemize}
	\item a large flat region where gradient $\rightarrow 0$ \\
	$\Rightarrow$ gradient descent slowly down the flat surface (before exiting)
	\item $\Rightarrow$ slow down gradient descent significantly
	\end{itemize}
\item Expensive in Finding Critical Point
	\begin{itemize}
	\item expensive for even local optima with gradient decent
	\item as expensive as $\mathcal O(n^3)$ if using Laplace approximation
	\end{itemize}	
\end{itemize}

\subsubsection{Gradient Vanishing/Exploding}
\begin{itemize}
\item Gradient Vanishing
	\begin{itemize}
	\item Saturated Function
		\begin{itemize}
		\item sigmoid/tanh function: gradient $\rightarrow 0$ when input $\rightarrow \pm \infty$
		\end{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item with depth $L$, each activation (e.g. tanh) output $a^l < 1$ and weight $\mathbf w^l <1$ \\ 
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'<1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $<1$ \\
		$\Rightarrow$ gradient exponentially decayed in back-prop
		\end{itemize}
	\end{itemize}
\item Gradient Exploding
	\begin{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item similarly, each activation (e.g. ReLU) output $a^l>1$ and weight $\mathbf w^l > 1$ \\
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'>1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $>1$ \\
		$\Rightarrow$ gradient exponentially augmented in back-prop
		\end{itemize}
	\end{itemize}
\item Possible Solutions
	\begin{itemize}
	\item Random Initialization
		\begin{itemize}
		\item \hyperref[DL_Init_Xavier]{Xavier Initialization}: for gradient vanishing \& exploding
		\end{itemize}
	\item Activation 
		\begin{itemize}
		\item \hyperref[DL_Act_ReLU]{ReLU}: for gradient vanishing
		\end{itemize}
	\item Skip/Concat Connection
		\begin{itemize}
		\item \hyperref[DL_Block_Res]{residual block}
		\item 
		\end{itemize}
	\end{itemize}	
\end{itemize}

\subsection{Learning}
\subsubsection{Forward-Backward Propagation}
\begin{itemize}
\item Representation
	\begin{itemize}
	\item Layers
		\begin{itemize}
		\item input layer
		\item hidden layer(s): layer with NO ground truth (for the associated weights) available \\
		note: input \& hidden layers have associated biases as well (usually)
		\item output layer
		\end{itemize}
	\item Neuron (Unit)
		\begin{itemize}
		\item $s_l$: num of units in layer $l$
		\item $w^l$: weight matrix of mapping from layer $l$ to $l+1$, with shape of $\left( s_{l+1}, s_l + 1 \right)$
		\item $h(\cdot)$: activation function (usually shared)
		\item $a_j^l$: activation output of unit $j$ at layer $l$
		\item $z_j^l$: output of unit $j$ at layer $l$ \\ 
		(represent parameterized basis, also the input for layer $l+1$)
		
		\end{itemize}
	\item Intuition
		\begin{itemize}
		\item all stacked vertically (vertical vector) \\
		$\Rightarrow$ horizontally for different examples; vertically for different units
		\end{itemize}
	\end{itemize}
	
\item Forward Propagation (Inference)
	\begin{itemize}
	\item Activation $a^{j+1} = w^j \cdot [z_0^j, ..., z_{s_j}^j]^T, \text{ with } z_0=1$
	\item Unit Output $z^{j+1} = h(a^{j+1}) = [z_1^{1}, ..., z^{j+1}_{s_{j+1}}]^T$
	\end{itemize}

\item Backward Propagation
	\begin{itemize}
	\item Loss $\mathcal L(W) = $
	\end{itemize}

\item Practice of Back Prop
	\begin{itemize}
	\item Caching Intermediate Result
		\begin{itemize}
		\item naturally cached: input $a^0=x$, weights matrix $w$ and bias $b$
		\item activation input/output $a/z$ \\
		(since will be used in back-prop)
		\end{itemize}
	\item Auto Difference
		\begin{itemize}
		\item achievement: calculate the derivatives along the forward prop \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Operations \& Layers Structure}
\subsection{Operations in Network}
\subsubsection{Activations}
\begin{itemize}
\item Sigmoid $a=\sigma(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mapping to $(0,1)$, with $\sigma(0)=0.5$
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item gradient vanishing: $\displaystyle \sigma(z)' = \sigma(z)(1-\sigma(z)) \Rightarrow \lim_{z\rightarrow \pm \infty} \sigma(z)' \rightarrow 0$ \\
		(as the gradient passed through (via chain rule) $=\frac{a}{z} \frac{z}{w}$)
		\end{itemize}
	\end{itemize}
\item Tangent $a=\tanh(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item empirically, almost always better than sigmoid (in hidden layers)
		\item maps to $(-1,1)$, with $\tan(0)=0 \Rightarrow$ help centering data ($0$-mean) \\ $\Rightarrow$ make the learning of next layer easier
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item still, gradient vanishing when $z\rightarrow \pm \infty$
		\end{itemize}
	\end{itemize}
\item Rectified Linear Unit (ReLU) $\max(0, z)$ \label{DL_Act_ReLU}
	\begin{itemize}
	\item Derivation: approximated by a stack of sigmoid
		\begin{itemize}
		\item 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate gradient vanishing: $\forall z>0, a=z \Rightarrow$ learn much faster
		\end{itemize}
		$\Rightarrow$ the default choice!
	\item Cons
		\begin{itemize}
		\item undefined behavior at $x=0$ (actually, gradient becomes the sub-gradient)
		\item gradient totally vanished for $x<0$
		\item $\Rightarrow$ dead units: weights learned/initialized to always output negatives \\ 
		$\Rightarrow$ activation always output $0$ \\
		$\Rightarrow$ the unit always output $0$
		\end{itemize}		
	\end{itemize}
\item Leaky Relu $a=\max(\alpha z, z), \alpha \rightarrow 0^+$ (e.g. $\alpha=0.01$)
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate the gradient vanishing problem for $(-\infty, +\infty)$
		\item avoid dead units problem
		\end{itemize}
		(yet not that popular as ReLU)
	\end{itemize}

\item Piecewise Linear Unit (PLU) $a=\max(\alpha(z+\beta)-\beta, \min(\alpha(z-\beta)+\beta, z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item hybrid of tanh \& ReLU: three linear pieces approximating tanh in a given range
		\item more expressive than ReLU: more nonlinear, better to fit smooth nonlinear function
		\item mitigate gradient vanishing problem: due to linearity
		\end{itemize}
	\item Cons
	\end{itemize}

\item Linear (Identity) Activation $a=z$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item used in regression to output real number $\in (-\infty, +\infty)$
		\item used in compression net
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item stacked units with linear activation $\Leftrightarrow$ single linear transformation
		\item logistic regression with linear activation in hidden layer is NO more expressive than logistic regression with no hidden layer \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Normalization in Network}
\begin{itemize}
\item Batch Normalization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for an activation in hidden layer with input $z$, a batch with size $N_b$
		\item calculate the mean of current batch $\displaystyle \mu=\frac 1 {N_b} \sum_n z_n$, where $z_n$ for the $n^{th}$ example 
		\item calculate the deviation of current batch $\displaystyle \sigma = \sqrt{\frac 1 {N_b} \sum_n(z_n-\mu)^2}$
		\item normalize to be $z'_n = \frac {z_n-\mu}{\sigma}$
		\item allow model to recover/manipulate original distribution: $\hat z_n=\gamma z'_n + \beta$, \\
		where $\gamma, \beta$ being trainable (updated by optimizer using gradients)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item preferred to apply batch norm on $z$ (before activation), instead of after it
		\item for math stability, $z'_n=\frac {z_n-\mu}{\sigma+\epsilon}$, with $\epsilon\rightarrow 0+$
		\item (usually) with mini-batch, calculate the mean \& variance from only the mini-batch
		\item with batch norm, original bias $b$ in calculating $z=wx+b$ becomes pointless \\
		$\Rightarrow$ integrated into the $\beta$ in batch norm
		\item at test time ($1$ example a time): need an estimation for $\mu, \sigma$ \\
		$\Rightarrow$ exponentially weighted average over $\beta, \sigma$ in training time
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item normalize the intermediate data to have $0$ mean, unit variance \\
		$\Rightarrow$ to speed up the training from some hidden layers (as normalization does)
		\item remain the ability to transfer the data to have other mean \& variance \\
		(controlled by $\gamma,\beta$)
		\item control the distribution of data in hidden layer \\ 
		$\Rightarrow$ suppress the change of input data distribution for the layer after it \\
		$\Rightarrow$ increase robustness for later layers, against covariate shift \\ 
		(from both the weight update in early layers and the input data change)
		\item regularize the net by adding noise to the input data of hidden layer \\ 
		(due to computing mean/variance only on mini-batch) \\
		$\Rightarrow$ enforce robustness against noise, hence unintended slight regularization effect
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Operations on Network}
\subsubsection{Initialization}
\begin{itemize}
\item Random Initialization for Weights
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item weights initialized to a random variable in a small range e.g. $(-0.03, 0.03)$
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid symmetry problem: \\
		if identical initialization for weights $\Rightarrow$ units in same layer computing exactly same function \\
		$\Rightarrow$ get the same learning step propagated back \\
		$\Rightarrow$ then always compute exactly the same function (by induction)
		\item avoid gradient vanishing: especially for gradient of sigmoid/tanh activation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item NOT concern various nets: sampling in a fixed range may not work for all nets
		\end{itemize}
	\end{itemize}
\item Xavier Initialization for Weights \label{DL_Init_Xavier}
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item set $\forall l\in [1,L], \text{Var}(w^l) = \frac 1{n_l}$ for tanh, $\frac 2{n_l}$ for ReLU, \\
		where $n_l$ is the number of unit in layer $l$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item draw random variable $r\sim \mathcal N(0,1)$
		\item set each of $w^l=r\cdot \sqrt{\frac 2 {n_l}}$ for ReLU, $r\cdot \sqrt{\frac 2 {n_l}}$ for tanh \\ 
		or $r\cdot \sqrt{\frac 2 {n_{l-1}+n_l}}$ proposed by 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item theoretically justified to initialized weights to be around $\pm 1$ \\
		$\Rightarrow$ mitigate gradient vanishing\& exploding problem statistically 
		\end{itemize}
	\end{itemize}
\item Zero Initialization for Bias
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item default to use $0$ bias \\
		(can NOT used for weights as explained)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Regularization}
\begin{itemize}
\item $L2$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $\Rightarrow$ also called "weight decay" \\
		(as in gradient decent, weight is multiplied by a $<1$ number due to L2 term)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item forcing weights to be smaller
			\begin{itemize}
			\item single node has smaller effect
			\item input of activation closer to $0$ \\
			$\Rightarrow$ activation becomes more linear-alike (e.g. sigmoid, tanh) \\
			$\Rightarrow$ layers perform more linear-alike transformation
			\end{itemize}
		$\Rightarrow$ simpler network, less able to fit extreme curly decision boundary \\
		(hence less able to overfit)

		\end{itemize}
	\end{itemize}
	
\item $L1$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each weight ww we add the term $\lambda \abs w$ to the objective. It is possible to combine the L1 regularization with the L2 regularization: $\lambda_1 \abs w + \lambda_2 w^2$ (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Dropout Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each of selected units, set a drop probability \\
		i.e. for each forward/back-prop, nodes are "dropped" according to the probability \\
		$\Rightarrow$ for each time, a randomly reduced net is trained
		\end{itemize}
	\item Implementation: Inverted Dropout
		\begin{itemize}
		\item set a keep prob $k$ instead of drop prob, for a selected layer
		\item generate random numbers for all units \& turned into a boolean "keep" vector $\mathbf k$
		\item dropped activation $\mathbf d = \mathbf a \times \mathbf k$ (element-wise), \\
		 where $\mathbf a$ is original activation output vector from the layer
		\item $\Rightarrow$ activation becomes $0$ for dropped units in $\mathbf d$
		\item scaling up by dividing the keep prob: $\mathbf d / k$ \\
		$\Rightarrow$ so that expected output value of each activation remains the same
		\item test time: no dropout $\Rightarrow$ no random output \& consider all robust features learned\\
		(randomness in training, mitigated by big data)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item can NOT rely on any one feature $\Rightarrow$ have to spread out weights \\
		$\Rightarrow$ results in shrinking the squared norm of weights (as $L2$)
		\item used on layers with enormous features as input (e.g. computer vision) \\
		$\Rightarrow$ reduce the chance of relying on small set of features
		\end{itemize}
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/regularization-dropout".jpeg}
	\end{figure}
	\item Cons
		\begin{itemize}
		\item training loss may have bigger glitch $\Rightarrow$ harder to debug \\
		(make sure loss decreasing before introduced dropout)
		\end{itemize}
	\end{itemize}

\item Max norm constraints
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $w^l_n$ of every neuron to satisfy (). Typical values of cc are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Early Stopping
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item stop the training at lowest validation loss (with training loss decreasing) \\
		$\Rightarrow$ at the start point of overfitting
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate both train \& val loss, saving models along the way \\
		$\Rightarrow$ use the model corresponding to the start of overfitting
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item at relatively early stage, weights are still relatively small \\ 
		(due to random initialization in $[0^-, 0^+]$)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item couples task of optimizing loss and task of not overfitting \\
		$\Rightarrow$ no longer one task at a time
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Optimization}
\begin{itemize}
\item Batch Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate on entire training set; then update weights
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item largest optimization every time
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item greedy optimizing
		\item slow \& memory demanding on large dataset
		\end{itemize}
	\end{itemize}

\item Stochastic Gradient Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item shuffle data to have training set $X_\text{train}$, further split into $X_\text{train}^{1}, ..., X_\text{train}^{T}$
		\item train the net iteratively with $\forall t\in[1,T], X_\text{train}^t$ \\
		i.e. one mini-batch for a gradient descent (weights update)
		\item after training through all $T$ batches, an epoch of training is finished \\
		$\Rightarrow 1$ epoch = $1$ full scan of training set
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item faster: more weight upgrade over the same amount of data
		\item better chance to reach global change: not greedy anymore
		\item more affordable for training in GPU memory
		\end{itemize}
		$\Rightarrow$ preferred choice
	\item Cons
		\begin{itemize}
		\item observing noisy loss: not monotonically decreasing (but overall decreasing)
		\end{itemize}
	\end{itemize}
	
\item Gradient Descent with Momentum
	\begin{itemize}
	\item Definition: exponentially weighted average
		\begin{itemize}
		\item calculate the gradient for weight update: $dW'_t = \beta dW'_{t-1} + (1-\beta) dW_t$, \\ 
		where $dW$ the original gradient
		\item $\Rightarrow$ average over past gradients with exponentially decaying weight, \\
		$\Rightarrow$ for past $k\in[0,K]$ gradient, coefficient becomes $\beta(1-\beta)^k$ \\ 
		(with $k=0$ denoting current gradient)
		\item bias correction: avoid slow start \\
		(due to: gradient $dW_0$ initialized to $0$ \& not enough gradients for averaging) \\
		$\Rightarrow$ set $dW_t=\frac {dW_t}{1-\beta^t}$ in the early stage \\
		(after starting stage, bias correction $\rightarrow 0$ for large $t$)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item approximation: weighted average over past $K=\frac 1 {1-\beta}$ gradients \\
		due to $(1-\epsilon)^{1/\epsilon} \approx \frac 1 e$, recognized as small enough \\
		$\Rightarrow$ discard gradients with further exponentially small weights
		\item apply element-wise multiplication on gradients and pre-calculated coefficient
		\item sum up to be the gradient for weight update \\ 
		(include bias correction term if necessary, yet often omitted)
		\item note: $dW'_t = \beta dW'_{t-1}+dW_t$ is another version, yet discouraged \\
		(coupling momentum $\beta$ with learning rate $\alpha$, as $\alpha$ needs to cooperate)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item averaging/smoothing out the regular oscillation in stochastic gradient descent \\
		$\Rightarrow \beta$ popularly chosen to be $0.9$ (averaging over last $10$ gradients) 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid some regular oscillation (slowing down the training \& not true randomness)
		\end{itemize}
	\end{itemize}
	
\item Root Mean Square Propagation (RMS prop)
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $S_t = \beta S_{t-1} + (1-\beta) dW_t^2$ ($S_0$ initialized to $0$),\\ 
		where $dW^2$ the original gradient being element-wisely squared\\
		$\Rightarrow$ exponentially weighted square of gradients
		\item calculate the gradient for weight update $dW'_t=\frac {dW_t}{\sqrt{S_t}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item calculate $S_t$ similarly (as an exponentially weighted average)
		\item $\sqrt{S_t}$ becomes $\sqrt{S_t+\epsilon}$, where $\epsilon\rightarrow 0^+$ for mathematical stability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item for gradients with large variance in training $\Rightarrow S_t$ large $\Rightarrow \frac 1{\sqrt{S_t}}$ small \\ 
		$\Rightarrow$ weighted less, hence stabilized (as it should be noisy \& taking smaller step)
		\item for gradients with small variance \\ 
		$\Rightarrow$ weighted more, encouraged (as it should be on the "trend" towards optimum)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item recognize trend from noise via variance of their gradient $\Rightarrow$ speedup training
		\item auto-fixing learning rate for each weight given the recorded behavior \\ 
		(protect learning process from a too large learning rate)
		\end{itemize}
	\end{itemize}
	
\item Adaptive Momentum (Adam) Optimization Optimization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $M_t = \beta_1 M_{t-1} + (1-\beta_1) M_t$ as momentum
		\item compute $S_t = \beta_2 S_{t-1} + (1-\beta_2) dW_t^2$ as root mean square
		\item apply bias correction on both: $M'_t=\frac {M_t}{1-\beta_1^t}, S'_t=\frac {S_t}{1-\beta_2^t}$
		\item $\Rightarrow$ calculate gradient for update $dW'_t=\frac {M'_t} {\sqrt{S'_t+\epsilon}}$, where $\epsilon\rightarrow 0^+$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item implement $M_t,S_t$ as momentum and root mean square \\
		(popular choice: $\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}$)
		\item do implement bias correction
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item combine momentum with root mean square \\
		$\Rightarrow$ for each weight
			\begin{itemize}
			\item smooth out regular oscillation
			\item encourage the trend \& adapt learning rate given history record
			\end{itemize}
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item effective for a large range of problem
		\end{itemize}
	\end{itemize}

\item Learning Rate Decay
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item update learning rate $\alpha = \frac 1 {1+r\cdot e}$, where $r$ the decay rate, $e$ the epoch number
		\item other decay formula:
			\begin{itemize}
			\item exponential decay: $\alpha=r^e\cdot\alpha_0$, where $\alpha_0$ the base learning rate
			\item $\alpha=\frac k {\sqrt{e}} * \alpha_0$, where $k$ a constant
			\end{itemize}
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item set learning rate for each epoch, or after some global steps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fast learning at the beginning, more cautious when approaching the optimum \\ 
		$\Rightarrow$ in order to finally converge
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Cost}
\subsubsection{Probabilistic Cost}
\begin{itemize}
\item Log Maximum Likelihood / Posterior
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item convert the logits into probability-alike prediction \\ 
		$\Rightarrow$ then interpreted as predicted likelihood $p(\mathbf y|\mathbf w, \mathbf x)$
		\item bayesian regression $\displaystyle L = -\frac 1 2\sum_{\mathbf y\in \mathbf Y} (\mathbf y - \hat {\mathbf y})^2$ \\
		(for $\mathbf y$ real number vector label, $\hat {\mathbf y}$ real number vector prediction)
		\item classification with logistic assumption $\displaystyle L = -\sum_{\mathbf y\in\mathbf Y}(\mathbf y^T \cdot \log \hat {\mathbf y})$ \\
		($t$ one-hot encoded label, $\hat y$ one-hot encoded prediction)
		\item to use posterior with Gaussian distribution: add $L2$ regularization term
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Layers}

\subsubsection{Prediction}
\begin{itemize}
\item Sigmoid

\item Softmax
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item arbitrary input $\mathbf z^{L}$ being logits, containing multiple multi-class predictions $z^{L}$ \\
		$\Rightarrow$ each prediction being the same dimension as one-hot encoded label
		\end{itemize}
	\item Output
		\begin{itemize}
		\item probabilistic-alike prediction $\mathbf a^L$, with the same shape as the input (logits)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item for $K$ classes to predict $\Rightarrow$ $\dim (z^L)=K$
		\item for each dimension $k\in[1,K]$, compute $\displaystyle a^L_k=\frac{e^{(z^L_k)}}{\displaystyle \sum_{k=1}^K e^{(z^L_k)}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item vecotrize the exponential computation $\hat z^L = \exp{(z^L)}$
		\item compute normalization $N=\displaystyle \sum_{k=1}^K {\hat z^L_k}$
		\item normalize as $a^L=\frac 1N \hat z^L$
		\item maximum likelihood with softmax: $\displaystyle L = \frac 1N \sum_{\mathbf Y}-\mathbf y^T \cdot \log \hat {\mathbf y}$, \\
		where $\mathbf y$ the one-hot encoded label, $\hat {\mathbf y}$ the prediction \\
		$\Rightarrow$ easy gradients: $dz^L = \hat{\mathbf y} - \mathbf y$, where $z^L$ the logits (vector)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item contrasting the hard-max function (non differentiable): $a_k = 1 \text{ if } \displaystyle \arg\max_k(z); \text{ else } 0$
		\item exponentially normalizing the output of arbitrary net into probabilistic form \\
		(reduced to logistic for binary class i.e. $K=2$) \\
		$\Rightarrow$ generalize logistic prediction to $K$-class prediction
		\item for maximum likelihood loss, only the gap with true class generate gradients \\ 
		(due to one-hot encoding) \\ 
		$\Rightarrow$ trying to predict the class true with higher probability
		\end{itemize}
	\end{itemize}

\item Hierarchical Softmax 
	\begin{itemize}
	\item uses a binary tree representation of the output layer with the W words as
	its leaves and, for each node, explicitly represents the relative probabilities of its child nodes \\ 
	$\Rightarrow$ a random walk that assigns probabilities to classes
	\end{itemize}

\item Normalization
\end{itemize}

\subsubsection{Convolution Layer}
\begin{itemize}
\item Convolution $2D$
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item spatially $2D$ feature maps, usually with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given hyperparameter: kernel/filter size, stride, padding
		\item kernel (weights) structured as a matrix (for each input channel)
		\item input maps padded if required
		\item an element-wise weighted sum on the spatially corresponding position
		\item sum across channels: sum over kernel output from each channel $+$ optional bias
		\item kernel strides spatially across the image, with stride along each axis defined \\
		$\Rightarrow$ to calculate $\mathbf 1$ channel in the output feature maps \\
		$\Rightarrow$ for multi-channels output: multiple sets of kernels
		\begin{figure}[ht]
		\centering
		\begin{subfigure}{.7\linewidth}
		\includegraphics[width=.75\linewidth, right]{"./Deep Learning/layer-conv2d exp".png}
		\end{subfigure}%
		\begin{subfigure}{.3\linewidth}
		\includegraphics[width=.5\linewidth, center]{"./Deep Learning/layer-conv2d kernel movement".png}
		\end{subfigure}%
		\end{figure}
		\item activation then taken after convolution operation, element-wisely
		\end{itemize}
	\item Padding
		\begin{itemize}
		\item reason
			\begin{itemize}
			\item prevent output feature maps from spatially shrinking
			\item prevent info lost on the edge\&corner of image \\ 
			(compared to the central part of feature map, multiplied less with the kernel)
			\end{itemize}
		\item convention: $0$-padding on both directions of an axis
		\item valid conv: no padding
		\item same conv: pad so that output size same as input size
		\end{itemize}
	\item Kernel Size
		\begin{itemize}
		\item odd square matrix: avoid asymmetric padding \& a central pixel for filter location \\
		(even becomes a convention)
		\end{itemize}
	\item Stride
		\begin{itemize}
		\item the step for kernel to move its location as striding over feature maps
		\item kernel striding over the edge (after padding): NOT convoluted
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a feature maps with defined output channel \& size \\
		(output channel depends on the number of sets of kernels)
		\item on a $2D$ square feature map, with padding at each edge $p$, stride on all axises $s$ \\ 
		kernel size $k\times k$ , input size $i\times i$, output size $o\times o$ \\
		$\displaystyle \Rightarrow o = \left\lfloor\frac {i+2p-k} {s} \right\rfloor + 1$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item computer vision filter: learn the weights in filters, instead of hand design \\ 
		$\Rightarrow$ guided by data statistics
		\item weights in the $l^\text{th}$ conv layer: $n_c^{l-1} \times k\times k \times n_c^{l}$, where $n_c$ the channel number \\
		(to output $n_c^l$ channels with $n_c^{l-1}$ input channels from previous layer) \\
		$\Rightarrow$ invariant to the input size (number of trainable variables fixed on design) \\
		$\Rightarrow$ less weights (then dense layer), more generalizability, hence less overfitting
		\item sharing weights spatially: apply same weights over the whole space \\
		$\Rightarrow$ NO need for special design at each location \\
		$\Rightarrow$ as need to handle spatial variance in processing images
		\item sparse connection: output connected only to the local input $\Rightarrow$ as a high-pass bandwidth \\
		$\Rightarrow$ robust to spatial variance
		\end{itemize}
	\item Back Propagation
	\item Implementation
		\begin{itemize}
		\item implement cross-correlation instead of convolution \\ 
		(skip the flipping operation: as their results are symmetric) \\
		yet, convolution is associative, due to the flipping
		\end{itemize}
	\end{itemize}

\item $1\times1$ Convolution
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item multi-dimension feature maps with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item integrate channels at each spatial location together \\
		(a weighted sum with bias, as conv definition)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature maps with same dimensions, but different channels
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item shrink the number of channels
		\item add more non-linearity \& info combination (more representability)
		\item 
		\end{itemize}
	\end{itemize}

\item Atrous Convolution
\item Deconvolution
\end{itemize}

\subsubsection{Pooling Layer}
\begin{itemize}
\item Normal Pooling
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item feature maps
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given hyperparameter: kernel size, stride (usually no padding)
		\item compute the max/average of the elements covered by kernel
		\item kernel strides along each axis over the feature maps (like conv) \\
		$\Rightarrow$ does NOT change the channel \\
		$\Rightarrow$ one kernel per channel (compared to conv)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a downsampled feature maps
		\item given a $2D$ feature map with kernel size $k \times k$, strides along each axis $s\times s$ \\
		input size $i\times i$, output size $o\times o$ \\
		$\Rightarrow o = \left\lfloor \frac {i-k} {s} \right\rfloor + 1$ (same as conv)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item downsampling the feature maps (NO weights to learn) \\ 
		$\Rightarrow$ if desired features detected anywhere, represent it by local max/average
		\end{itemize}
	\end{itemize}
\item Unpooling
\item Spatial Pyramid Pooling (SPP)
\item Region of Interest Pooling (RoI Pooling)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps from CNN
		\item RoIs i.e. proposal region (from selective search etc.) projected on feature map
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item divide each RoI with grid of desired size (proportional to the RoI size)
		\item max pooling from each cell
		\end{itemize}
		$\Rightarrow$ single-size SPP for each RoI
	\item Output
		\begin{itemize}
		\item a fixed size feature maps for each RoI
		\end{itemize}
	\end{itemize}
\item Probabilistic Max Pooling
\end{itemize}

\subsubsection{Residual Block} \label{DL_Block_Res}
\begin{itemize}
\item Structure
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for the $l+2$ layer, $a^{l+2} = g(z^{l+2} + a^l)$, \\
		where $g(\cdot)$ the activation, $z^{l+2} = W^{l+2}a^{l+1}+b^{l+2}$ from layer $l+1$ 
		\end{itemize}
	\item Main Path
		\begin{itemize}
		\item the usual passing of $a^l$ to layer $l+1$, then the $z^{l+2}$ at layer $l+2$
		\end{itemize}
	\item Skip Connection (Shortcut)
		\begin{itemize}
		\item the passing of $a^{l}$ directly to the layer $l+2$
		\end{itemize}
	\item Join
		\begin{itemize}
		\item for different size $a^l, a^{l+2}$: adjust $a^l$ by linear transformation or padding \\
		(can be implemented as matrix/conv with trainable or fixed weights)
		\item add the result from two paths $\Rightarrow a^{l+2}=g(z^{l+2}+a^l)$
		\end{itemize}
	\end{itemize}

\item Understanding
	\begin{itemize}
	\item Guaranteed Baseline
		\begin{itemize}
		\item with ReLU as activation, easy to learn identity function \\ 
		$\Rightarrow$ layer $l+2$ only need to make $z^{l+2}=0$ (as weights \& bias initialized near $0$)
		\item $\Rightarrow$ deeper net can easily guarantee to be at least as good as its shallow version \\
		(then search for luck to surpass baseline)
		\end{itemize}
	\item More Gradient
		\begin{itemize}
		\item gradient more easily passed to the early layers \\ 
		(as shortcuts not attenuating gradients)
		\item $\Rightarrow$ early layers settle down faster $\Rightarrow$ late layers get a more consistent input
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Inception Block}
\begin{itemize}
\item Basic Inception
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item op1 = $1\times1$ conv
		\item op2 = $1\times1$ conv, $3\times3$ conv with same padding
		\item op3 = $1\times1$ conv, $5\times5$ conv with same padding
		\item op4 = max pooling with same padding and stride $1$, $1\times1$ conv
		\item channel concate: concatenate the output channel from each op \\
		(as output size ensured to be the same in each op)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature maps with the same spatial size as input
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $1\times1$ conv to shrink
			\begin{itemize}
			\item less computation for afterwards larger kernel (e.g. $3\times3, 5\times5$)
			\item prevent output of other ops from being overwhelmed by pooling
			\end{itemize}
		\item enable network to learn the desired combination of info \\
		(instead of predefined hyperparamter) \\
		$\Rightarrow$ more representability
		\end{itemize}
	\end{itemize}

\item Xception
	\begin{itemize}
	\item 
	\end{itemize}	
\end{itemize}

\subsubsection{RNN Layer}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item sequence data: includes time/precedence (conventionally arrived from left to right)
		\item for each time step, data can be vector, feature maps, etc...
		\end{itemize}
	\item RNN Cell
		\begin{itemize}
		\item consume the input of current time step \& the hidden state from last time step \\
		(hidden state usually initialized to $\mathbf 0$)
		\item calculate a hidden state at each time step
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item RNN cell at time $t$, calculate hidden state (activation) $h^t = g_h(w_{h}[h^{t-1},x^t] + b_h)$, \\ 
		where $g_h(\cdot)$ the activation function, $g_h=\tanh$ by convention \\
		$[h^{t-1}, x^t]$ the concat of $h^{t-1}$ (hidden state of time $t-1$), $x^t$ (input at time $t$)
		\item expose its hidden state at each time steps
		\item calculate its output $y^t=g_y(w_{y}h^t+b_y)$, where $g_y=\sigma, softmax$ or $identity$
		\end{itemize}
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/layer-rnn unrolled".png}
		\end{figure}
	\item Types of RNN Mapping
		\begin{itemize}
		\item many-to-one: encoder scans through the input, only the last output considered
		\item one-to-many: decoder with single input $x^1$, take $x^t=y^{t-1}$, till $y^{t'}=$ stop
		\item many-to-many: a many-to-one encoder, followed by a one-to-many decoder \\
		$\Rightarrow$ able to map between various length
		\end{itemize}
	\item Back Propagation through Time
		\begin{itemize}
		\item unroll the recurrent operation into a sequential network with length $T$
		\item given the loss for each time step $L^1,...,L^T \Rightarrow L = \sum_{t=1}^T L^t$
		\item for time $\displaystyle t=1,...,T-1, \frac {\partial} {\partial a^t} L = \frac {\partial} {\partial a^t} L^t + \sum_{t'=t+1}^T\frac {\partial L^{t'}} {\partial a^{t+1}} \frac {\partial a^{t+1}} {\partial a^t} = \sum_{t'=t}^T \frac {\partial} {\partial a^t} L^{t'}$
		\end{itemize}
	\item Truncated Back Propagation through Time
	\item Challenge
		\begin{itemize}
		\item bad at modeling longterm dependency due to gradient vanishing problem \\
		$\Rightarrow$ loss at late time needs to go through multiple activations to the early time \\
		(similar to the deep plain net, after unrolled) \\
		$\Rightarrow$ loss at late time are hard to affect weights when evaluated at early time \\
		(i.e. larger the $t'$, smaller the $\frac {\partial} {\partial a^t} L^{t'}$) \\
		$\Rightarrow$ hard to find out error in late time due to observation in early time \\ 
		$\Rightarrow$ hard to represent longterm dependency 
		(e.g. Car...is fast vs. Cars...are fast)
		\item easily affect by local dependency (as longterm dependency lost)
		\item gradient exploding, due to multiple / too many updates on the same weights \\
		(solved by gradient clipping)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item sharing weight across time: same weights used on each time step \\
		$\Rightarrow$ solve various length input by applying weights recurrently on each of them
		\item information early in the sequence reserved \& passed through in the hidden state
		\end{itemize}
	\end{itemize}

\item Long Short Term Memory (LSTM)
	\begin{itemize}
	\item Memory Cell
		\begin{itemize}
		\item $c^t$ the memory maintained by LSTM cell at time $t$
		\end{itemize}
	\item Gates
		\begin{itemize}
		\item forget gate $G_f=\sigma(w_f[h^{t-1}, x^t] + b_f)$
		\item input gate $G_i = \sigma(w_i[h^{t-1}, x^t] + b_i)$ \\
		$\Rightarrow$ together control the memory update (how past\&current info fused)
		\item output gate $G_o = \sigma(w_o[h^{t-1}, x^{t}] + b_o)$ \\
		$\Rightarrow$ control the generation of hidden state
		\end{itemize}
	\item Fusing Info
		\begin{itemize}
		\item propose candidate $\hat{c}^{t}=\tanh(w_c[c^{t-1}, x^{t}]+b_c)$ for memory update
		\item update memory as $c^t = G_f c^{t-1} + G_i \hat c^t$
		\end{itemize}
	\item Hidden State (Activation)
		\begin{itemize}
		\item generate as $h^t = G_o \cdot \tanh(c^t)$
		\end{itemize}
	\begin{minipage}[r]{0.5\linewidth}
	\includegraphics[width=\linewidth, center]{"./Deep Learning/layer-rnn lstm".png}
	\end{minipage}
	\begin{minipage}[l]{\linewidth}
	where, \\ 
	joint arrows denotes a concatenation \\ 
	$c_{t-1}, \widetilde c_t, c_t$ the previous/candidate/current memory; \\
	$h_{t-1},h_t$ the previous/current hidden state; \\ 
	$f_t, i_t, o_t$ the forget/input/output gate
	\end{minipage}

	\item Understanding
		\begin{itemize}
		\item easy to learn an identity mapping $c^{t-1}\rightarrow c^t$ \\
		$\Rightarrow$ memory (info) generated early can last for long term \\
		$\Rightarrow$ better model the long-term dependency
		\end{itemize}

	\end{itemize}

\item Gated Recurrent Unit (GRU)
	\begin{itemize}
	\item Memory Cell
		\begin{itemize}
		\item $c^t$ the memory maintained by GRU cell at time $t$
		\end{itemize}
	\item Gates
		\begin{itemize}
		\item relevance gate $G_r=\sigma(w_r[c^{t-1}, x^t]+b_r)$, a mask $\in [0,1]$ \\ 
		$\Rightarrow$ control how memory candidate proposed (fusion of last memory \& input)
		\item update gate $G_u=\sigma(w_u[c^{t-1}, x^t]+b_u)$, a mask $\in [0,1]$ \\
		$\Rightarrow$ control how memory update happen (fusion of last memory \& candidate)
		\end{itemize}
		(two gates computed with the same input, though with different weights)
	\item Fusing Info
		\begin{itemize}
		\item propose memory candidate $\hat{c}^{t}=\tanh(w_c[G_r c^{t-1}, x^{t}]+b_c)$ for the update \\
		$\Rightarrow$ decide whether the previous memory useful (relevant) with the context $x^t$
		\item update memory as: $c^{t+1}=G_u \hat{c}^{t+1} + (1-G_u)c^{t-1}$ \\ 
		$\Rightarrow$ decide how the memory updated \& remained
		\end{itemize}
	\item Hidden State (Activation)
		\begin{itemize}
		\item $h^t = c^t$
		\end{itemize}
		
	\begin{minipage}[r]{0.5\linewidth}
	\includegraphics[width=\linewidth, center]{"./Deep Learning/layer-rnn gru".png}
	\end{minipage}
	\begin{minipage}[l]{\linewidth}
	where, \\ 
	joint arrows denotes a concatenation \\ 
	$h_{t-1}, \widetilde h_t, h_t$ the previous/candidate/current memory; \\ 
	$r_t, z_t$ the relevance/update gate
	\end{minipage}
	
	\item Understanding
		\begin{itemize}
		\item single gate control the generation of current memory
		\item memory directly as hidden state
		\item $\Rightarrow$ less weights, simpler structure $\Rightarrow$ faster \\
		(basic logic inherent from LSTM $\Rightarrow$ similar performance)
		\end{itemize}
	\end{itemize}


\item Bidirectional RNN (BRNN/BiRNN)
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item one RNN layer scanning as $t=1\rightarrow T$, hidden state exposed as $h_1^t$
		\item another RNN layer scanning from $t=T\rightarrow 1$, hidden state exposed as $h_2^t$
		\item generate output $y^t = g(w_y[h_1^t, h_2^t] + b_y)$ \\ 
		(concat all hidden states at the same time step from each RNN)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item account for the info from both previous \& latter time step \\
		$\Rightarrow$ global context info acquired
		\item cons: need the entire input sequence before processing \\
		$\Rightarrow$ NOT the case in real-time speech recognition etc.
		\end{itemize}
		
	\begin{figure}[!h]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/layer-rnn bidirection".png}
	\caption{(using LSTM cell)}
	\end{figure}
	
	\end{itemize}

\item Convolutional LSTM (ConvLSTM)
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architectures}

\subsection{Convolutional Networks}
\subsubsection{Classic Convolutional Networks}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Convolution Part
		\begin{itemize}
		\item one or multiple "same" conv layer(s) with stride $s=1$
		\item followed by a max (rarely average) pooling
		\item apply on input \& repeat on feature maps afterwards \\
		$\Rightarrow$ usually decreasing spacial size (width, height), increasing channel
		\end{itemize}
	\item Fully Connected (Dense) Part
		\begin{itemize}
		\item apply flattening / average pooling on last feature maps \\
		$\Rightarrow$ generate a single vector as input
		\item apply fully connected layers \& repeat for $1~2$ times \\
		$\Rightarrow$ usually with decreasing size
		\item finally output prediction probability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item trainable weights are mostly from dense layer \\
		$\Rightarrow$ conv layers, though large in number, contains far less weights
		\item conv stride $s=1$ (compare to $s>1$) \\
		$\Rightarrow$ decouple downsampling into the pooling \\
		$\Rightarrow$ account for more info/possibility before downsampling \\
		(also trade-off between required computability)
		\end{itemize}
	\end{itemize}

\item Receptive Fields
	\begin{itemize}
	\item 
	\end{itemize}

\item VGG-16
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item fixed conv: $3\times3$ kernel, stride $s=1$, same padding, ReLU activation
		\item fixed pooling: $2\times2$ kernel, stride $s=2$, max pooling
	\item Notation
		\begin{itemize}
		\item{} [conv64] to denote a conv layer with 64 output channels
		\item pool to denote max pooling
		\item{} [fc4096] to denote a fully connected layer with output vector of length 4096
		\end{itemize}
	\item Structure
		\item input image: $224\times224\times3$
		\item{} ([conv64]$\times$2, pool) $\rightarrow$ ([conv128]$\times$2, pool) $\rightarrow$ ([conv256]$\times$3, pool) $\rightarrow$ ([conv512]$\times$3+pool)$\times$2
		\item last feature maps ($7\time7\times512$) flatten into a input vector of length 4096
		\item{} [fc4096] $\rightarrow$ [fc4096] $\rightarrow$ logits $\rightarrow$ softmax prediction for $1000$ categories
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fixed conv \& pooling operation \\
		$\Rightarrow$ few hyperparameters, yet large in parameters ($\sim$138 million)
		\item decrease spatial size by $2$ \& increase channel by $2$ on each step \\ 
		(till small or deep enough e.g. 7$\times$7 size, 512 channels)
		\end{itemize}
	\end{itemize}

\item ResNets
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item residual block
		\item batchnorm applied in the conv(s) inside residual block
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stack of residual block, for hundreds, even thousands of layers
		\end{itemize}
	\end{itemize}
	
\item Inception Net
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item inception block
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stack of inception blocks
		\item two auxiliary loss from hidden layers
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Fully Convolutional Networks (FCN)} \label{DL_Arch_FCN}
\begin{itemize}
\item FCN for Classification \& Regression
	\begin{itemize}
	\item Convolution as Flattening
		\begin{itemize}
		\item given input feature maps $i\times i\times c$, with $c$ channels
		\item perform valid conv with kernel $i\times i \times o$ \\ 
		$\Rightarrow$ output size $1\times1 \times o$
		\end{itemize}
	\item $1\times1$ Convolution as Fully Connected Layer
		\begin{itemize}
		\item for flatten vector $1\times1\times i$, perform $1\times1$ conv with kernel size $1\times1\times 0$ \\
		$\Rightarrow$ output size $1\times1\times o$, with same computation as fc layer
		\end{itemize}
	\item Use Case
		\begin{itemize}
		\item \hyperref[DL_CV_Objdet]{object detection}
		\end{itemize}
	\end{itemize}

\item 
\end{itemize}

\subsection{Recurrent Neural Network}
\subsubsection{Classic RNN}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Stacked RNN Layers
		\begin{itemize}
		\item the output of layer $l$ becomes the input for layer $l+1$ \\
		$\Rightarrow$ RNN scanning the output of previous RNN layer
		\item $3\sim5$ layer considered deep: as RNN can be unrolled into a deep plain net
		\end{itemize}
	\item Encoder-Decoder RNN
		\begin{itemize}
		\item encoder RNN scans the input sequence, last hidden layer as sequence encoding
		\item decoder RNN takes encoding as initial hidden state, unrolled the output sequence
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Attention}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item overcome long-term dependency, via considering all input with different attention
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item a bi-RNN encode rich input context at each hidden state: $h^1,...,h^{T_x}$
		\item global context after attention at time $t$: $\displaystyle c^t=\sum_{i=1}^{T_x}\alpha^t_i h^i = [h^1,...,h^{T_x}]\alpha^t$, \\
		where hidden states $h^1,...,h^{T_x}$ and attention $\alpha^t$ column vectors \\
		$\Rightarrow$ a weighted sum over all hidden states as context
		\item a small (1-layer) net mapping $[c^{t-1}, h^{i}] \xrightarrow[]{\text{dense}} \text{logits} \xrightarrow[]{\text{softmax}} \alpha^t_i$ \\
		$\Rightarrow$ attention on $h^i$ depends on previous global context $c^{t-1}$ \& $h^i$ itself \\
		(softmax to ensure $\sum\alpha^t=1$)
		\item a decoder RNN taking $c^{t}$ as input, until stop sign generated \\
		(as $c^t$ can be calculated infinite times)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item use the sequence of hidden states  as encoding \\ 
		(as sentence encoded in last hidden states can be bias \& have info lost) \\
		$\Rightarrow$ account input sequence (with different attention at different places) when generating output sequence
		\item quadratic cost: attention $\alpha$ a $T_x\times T_y$ matrix to be calculated 
		\end{itemize}
	\end{itemize}
\item Attention on Image
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item focus on correct spatial context when generating corresponding caption/sequence
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item calculate weights for a sum over all spatial location of the image encoding \\
		$\Rightarrow$ calculate a 2-D mask over a 3-D tensor
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Transformer}

\subsection{Encoder-Decoder Architecture}
\subsubsection{Basic}
\begin{itemize}
\item Encoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item downsample/encode input into rich feature maps/vectors
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual input: CNN backbone
		\item natural expression input: RNN backbone
		\end{itemize}
	\end{itemize}
\item Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item upsample/decode rich feature maps back to the original size
		\item actually, impose requirement onto the encoder
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual output: CNN backbone
		\item natural expression output: RNN backbone
		\end{itemize}
	\end{itemize}
\item Connection
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item combine high level information with low level information
		\item image $\rightarrow$ image: outline refinement ...
		\item language $\rightarrow$ language: sentence style capturing
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item concatenation
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Extension}
\begin{itemize}
\item Multiple Encoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item project different information into the same space
		\item combine those information via some shared layers at the end
		\end{itemize}
	\end{itemize}
\item Multiple Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item impose multiple requirements to the encoder (via auxiliary loss)
		\end{itemize}
	\end{itemize}

\item Variational Auto-Encoder
\end{itemize}

\subsection{Generative Network}
\subsubsection{Generative Adversarial Network}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Vision}

\subsection{Objects Detection} \label{DL_CV_Objdet}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Spatial Information
		\begin{itemize}
		\item image maps as $3$-D tensor: width, height, channel (rgb, etc.)
		\item multi/stereo -images for $3$-D detection
		\end{itemize}
	\end{itemize}

\item Goal
	\begin{itemize}
	\item Bounding Box Prediction
		\begin{itemize}
		\item localization as regression task on box attributes ($x,y,h,w$)
		\item classification on object classes \& object existence
		\end{itemize}
	\item Landmark (Key Point) Prediction
		\begin{itemize}
		\item landmarks: the coordinates of key points (of  each type) in image
		\item label of landmark should be consistent across image
		\item output real number as regression task \\
		$\Rightarrow$ used in pose detection, bounding box detection, etc.
		\end{itemize}
	\end{itemize}

\item Understanding
	\begin{itemize}
	\item multi-object localization \& classification
	\end{itemize}
\end{itemize}

\subsubsection{Common Postprocessing}
\begin{itemize}
\item Non-max Suppression \label{DL_CV_Objdet_nonmax}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item multiple predicted bounding boxes, with probability of existence ($p_e$) relatively large
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item clean up \& account for duplicate bounding boxes on the same object \\
		$\Rightarrow$ for each (predicted) object, finalize prediction to be a single bounding box
		\end{itemize}
	\item Naive Operation
		\begin{itemize}
		\item choose the bounding boxes with highest (maximal) $p_e$ (for each object classes)
		\item remove those bounding boxes whose IoU with it are large \\
		$\Rightarrow$ remove (suppress) bounding boxes with large overlap
		\item repeat until all bounding boxes have a low IoU with each other \\
		$\Rightarrow$ remaining boxes are the final predictions
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approaches}
\begin{itemize}	
\item Sliding Window Detection
	\begin{itemize}
	\item Bounding Box Proposal
		\begin{itemize}
		\item slide the window with various size, across the image \\
		$\Rightarrow$ propose bounding boxes with predefined size \& location
		\item feed the window into CNN for classification \\
		$\Rightarrow$ CNN as classifier, window as bounding box
		\end{itemize}
	\item Fast Implementation: Sliding Window as Convolution
		\begin{itemize}
		\item implement CNN classification as \hyperref[DL_Arch_FCN]{FCN} \\
		$\Rightarrow$ as conv independent from input size
		\item run directly the CNN on the input image (instead of on each sliding window) \\
		$\Rightarrow$ output size $m\times n \times k$, \\ 
		where $m\times n$ the total number of sliding windows on the image, $k$ the class number
		\item $\Rightarrow$ one times running for all sliding windows \\ 
		(as sliding window essentially is a crop from image)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item though with fast implementation, still not promising regarding result \\
		$\Rightarrow$ sliding window propose a fixed set of window \\
		$\Rightarrow$ fail if not matching window size; or missing location/rotation
		\end{itemize}
	\end{itemize}

\item You Only Look Once (YOLO)
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item grid the input image into disjoint cells \\
		(apply usually fine grid, e.g. $19\times19$ girds)
		\item define a serious of anchor box: covering most of the interested objects \\
		(e.g. tall\&thin box for pedestrian, low\&fat box for car)
		\item assign the objects to a tuple (cell, anchor box)
			\begin{itemize}
			\item assign to cell by its central point $\Rightarrow$ ensure maximal $\mathbf 1$ object in a grid
			\item assign to one from $B$ anchor boxes depending on its IoU with all $B$ boxes
			\end{itemize}
		\end{itemize}
	\item Anchor Box Selection
		\begin{itemize}
		\item run K-means on all  clabel box; use cluster center as the anchor box
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $p_e$ probability of existence for current anchor box in a cell
		\item $b_x, b_y$ the box location offset relative to the top-left coord of the cell \\
		$\Rightarrow$ easier to learn, as dense layer not confused by varying locations
		\item $b_h, b_w$ the box size relative to the size of the cell (allowed to go outside the grid) \\
		$\Rightarrow$ cope with various girding strategy
		\item $p_1,...,p_C$ the classification for object inside current box
		\item $\Rightarrow a_b=[p_e, b_x, b_y, b_h, b_w, p_1,...,p_C]$ the anchor box $b$ \\
		(note: localization\&classification considered only when $p_e=1$)
		\item $\Rightarrow [a_1,...,a_B]^T$ the prediction/label in each cell \\
		(hence, able to match multiple anchor boxes at the same time in one cell)
		\end{itemize}
	\item Prediction
		\begin{itemize}
		\item each cell predict bboxes with their $p_e$
		\item each cell predict conditional probability $p(\text{class}|\text{obj})$ \\
		i.e. the class probability of object, given there is (part of) an object in the cell
		\item final prediction of each bbox in each cell: $p(class, obj) = p_e\times p(\text{class}|\text{obj})$
		\end{itemize}
	\item Bounding Box Prediction
		\begin{itemize}
		\item apply FCN on each cell for classification\&localization\\ 
		(implemented as fast sliding window) \\
		$\Rightarrow$ output spatial shape = grid shape; channel = bounding box len
		\item get some predicted bounding boxes in each cell (e.g. fixed to $n$ box per cell)
		\item non-max suppression used to finalize predicted bounding box (for each object class)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item passthrough layer (skip connection) as downsampling \\
		$\Rightarrow$ early layer ($26\times26\times512$) stacks its spatially adjacent features together 
		$\Rightarrow$ form a $13\times13\times2048$ feature map; then concat to late layer \\
		(a modest $1\%$ performance increase)
		\item 
		\end{itemize}
	\item Training
		\begin{itemize}
		\item optimize the sum-squared error (due to its simplicity)
		\item emphasize the label containing box (object) \\ 
		$\Rightarrow$ avoid large num of labels without object pushing weights to $0$
		\item predict the $\sqrt{b_w},\sqrt{b_h}$, instead of $b_w, b_h$ \\ 
		$\Rightarrow$ reflect that small deviations in large boxes matter less than in small boxes
		\item for each bbox predicted by a cell
			\begin{itemize}
			\item increase the confidence \& adjust coords for box overlapping the most with label
			\item decrease the confidence for box not overlapping with any label
			\end{itemize}
		\item data augmentation: mix classification \& detection dataset \\ 
		$\Rightarrow$ train the same backbone with different prediction branch
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multiple anchor boxes: able to predict multiple boxes
		\item decoupled anchor boxes \& object class: able to predict multiple same-type objects \\
		(have to use different anchor box)
		\item $\Rightarrow$ able to handle: multiple objects, each in a different anchor box in one cell
		\item yet, NOT able to handle: multiple objects in same-type anchor box in one cell \\
		(NOR when objects more than total anchor boxes) \\
		$\Rightarrow$ suffer from small objects in group (e.g. birds)
		\item griding: to increase the chance of predicting \\ 
		$\Rightarrow$ avoid the failed case (i.e. multiple object in same-type anchor box in one cell)
		\item with FCN implementation, receptive fields not restricted by grid \\
		$\Rightarrow$ global knowledge utilized for prediction corresponding to each cell
		\item one scan to have predictions for all cell $\Rightarrow$ \textbf{fast} \& enable real-time application
		\end{itemize}
	\end{itemize}

\item Region Proposal
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item effectively propose a various bounding boxes on the image for potential focused object \\
		$\Rightarrow$ recall all focused object with preferably fewer boxes (e.g. $\sim 1\text{k}$)
		\end{itemize}
	\item Input
		\begin{itemize}
		\item segmentation mask by a segmentation network \\
		$\Rightarrow$ to place bboxes on high-probability blobs
		\end{itemize}
	\end{itemize}

\item R-CNN
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item region proposal by selective search
		\item CNN to predict class \& a bounding box (to refine)
		\end{itemize}
	\end{itemize}

\item Fast R-CNN
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item based on r-cnn
		\item convolution implementation to predict all proposed regions \\
		(similar to fast sliding window)
		\end{itemize}
	\end{itemize}

\item Faster R-CNN
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item based on fast r-cnn
		\item use CNN for segmentation \& region proposal (faster than selective search)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Research Direction}
\begin{itemize}
\item Faster YOLO
	\begin{itemize}
	\item Dynamic Grid
		\begin{itemize}
		\item similar to spatial pyramid pooling: griding with respect to input image size \\
		$\Rightarrow$ larger image more grid
		\item $\Rightarrow$ ultimately, pixel grid: each pixel as a grid cell
		\end{itemize}
	\item $\Rightarrow$ Segmentation as Detection
		\begin{itemize}
		\item for output mask, giving classification ($p_e$) + localization (regression) \\
		(instead of giving class probability as per-pixel classification)
		\item at most as many objects as pixel number \\ 
		$\Rightarrow$ not possible to lose detection due to griding
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Face Recognition}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item image from camera
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Identity Recognition
		\begin{itemize}
		\item recognize the identity of the face in image
		\item refuse to recognize if the face does NOT belongs to any stored identity
		\end{itemize}
	\item Liveness Detection
		\begin{itemize}
		\item make sure the face in image are from a live human \\
		(instead of picture etc.)
		\end{itemize}
	\end{itemize}
\item Face Verification
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image from camera \& identity
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item true/false, regarding whether the image content belongs to the identity
		\end{itemize}
	\end{itemize}
	
\item Challenge
	\begin{itemize}
	\item One-shot Learning
		\begin{itemize}
		\item given only single (at most, few) face-identity pair for each identity
		\item still, need to build a robust system for recognition task
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Siamese Network as Encoder
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item CNN + dense layer to encode the input image $x^i$ as a vector $f(x^i)$
		\end{itemize}
	\item Learning Goal
		\begin{itemize}
		\item minimize $\norm{f(x^i)-f(x^j)}^2$ if $x^i, x^j$ from same identity
		\item maximize $\norm{f(x^i)-f(x^j)}^2$ if $x^i, x^j$ from different identity
		\item $\Rightarrow$ learning encoding given a fixed distance function $d(x_1, x_2) \ge 0$\\
		(here, $d(x_1, x_2)=\norm{x_1-x_2}^2$)
		\end{itemize}
	\item Triplet Loss
		\begin{itemize}
		\item given an anchor image $A$ representing the identity $I$
		\item take a positive image $P$ from identity $I$; an negative image $N$ not from identity $I$
		\item $\Rightarrow L(A,P,N) = \max\left( d(f(A), f(P)) - d(f(A), f(N)) + \alpha, 0 \right)$, where \\
		$\alpha$ an hyperparamter to make sure the net differentiate them by a margin; \\
		$\max()$ to make the loss $=0$ as long as the requirement satisfied
		\end{itemize}
	\item Training: Hard Negative Mining
		\begin{itemize}
		\item due to large variance in the dataset $\Rightarrow$ $d(A,P) << d(A,N)$ in most case
		\item due to large number of identities $\Rightarrow$ permutation explosion
		\item $\Rightarrow$ evaluate current net on dataset, use mistakes for the next epoch
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn a encoder towards a selected distance function \\
		$\Rightarrow$ use permutation to have more training examples
		\item able to precomputing the encoded vector for fast recognition
		\end{itemize}
	\end{itemize}

\item Encoder with Binary Classification
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item still, CNN + dense layer to encode input image
		\end{itemize}
	\item Learning Goal
		\begin{itemize}
		\item given two encoded vectors, another net (or logistic regression) to perform binary classification \\
		$\Rightarrow$ $1$ for two image has same identity; $0$ for different identities
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item still, utilize permutation for larger training set \\ 
		(use pair, instead of triplet)
		\item learn the similarity function as well: output directly the result of comparison
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Image Style Transfer}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Content Image $C$
		\begin{itemize}
		\item the image containing the spatial info (content)
		\end{itemize}
	\item Style Image $S$
		\begin{itemize}
		\item the image containing the style of presenting the content
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Generated Image $G$
		\begin{itemize}
		\item a image with content from $C$ drawn in style of $S$
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/topic-image style transfer nst overview".png}
		\end{figure}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Neural Style Transfer
	\begin{itemize}
	\item Learning Goal
		\begin{itemize}
		\item given input $C,S$ with output $G$, loss $L = \sum_{l} \left[\alpha L_\text{content}(C,G) + \beta L_\text{style}(C,S)\right]$, \\
		where $l$ is sum over chosen hidden layers of the CNN
		\item $\Rightarrow$ minimize content \& style difference
		\end{itemize}
	\item Content
		\begin{itemize}
		\item given input, the activations from a set of (hidden) layers of the net
		\item $\Rightarrow$ similarity of $C, G$ measured as $\sum_l d(a^{l(C)}, a^{l(G)})$, \\ 
		where $a^{l(\cdot)}$ the feature maps at layer $l$ given the input, $d(\cdot)$ a distance function \\
		(e.g. $d(x_1,x_2) = \norm{x_1 - x_2}^2$)
		\end{itemize}
	\item Style
		\begin{itemize}
		\item given input, the correlation between activations across channels, for chosen layers \\
		$\Rightarrow$ correlation matrix across feature map at each channel as style matrix \\
		(actually, gram matrix)
		\item let $a_{i,j,k}^l$ the activation at a $h\times w\times c$ conv kernel location $i,j,k$ in layer $l$ \\ 
		$\Rightarrow$ style (gram) matrix $M^l_{k,k'} = \sum_{i,j}a^l_{ijk}\cdot a^l_{ijk'}$, for all $k,k'\in\{1,...,c\}$
		\item $\Rightarrow$ similarity of $S, G$ measured as $\sum_l \left[ \frac 1 {(2 h^l w^l c^l)^2} d(M^{l(S)}, M^{l(G)}) \right] $ \\
		where $M^{l(\cdot)}$ the gram matrix at layer $l$ given the input, $d(\cdot)$ a distance function, with normalization term $\frac 1 {(2 h^l w^l c^l)^2}$ \\
		(e.g. $d(x_1, x_2) = \norm{x_1-x_2}^2_F$, the euclidean norm between matrices)
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/topic-image style transfer nst style matrix".png}
		\end{figure}	
		\end{itemize}
	\item z
	\end{itemize}
\end{itemize}

\subsection{Point Cloud Recognition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Natural Language Processing}
\subsection{Language Representation} \label{DL_NLP_Langrep}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Language Token/Corpus
		\begin{itemize}
		\item words, sentences, paragraphs, ... $\Rightarrow$ can be language at various level
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Distributed Vector Representations as Embedding Matrix $E_{M\times N} = [e_1,...,e_N]$
		\begin{itemize}
		\item $e$ the column vectors, $M$ the desired embedding length, $N$ the total tokens num \\
		$\Rightarrow$ look up for the desired embedding \\
		(NOT using matrix multiplication due to sparsity from one-hot encoding)
		\item distributed representation: decomposed yet meaningful \\
		$\Rightarrow$ fight the curse of dimensionality
		\end{itemize}
	\item $\Rightarrow$ Meaningful Vector
		\begin{itemize}
		\item able to measure the (dis-)similarity of between tokens (words) \\
		$\Rightarrow$ semantic meaning: "Germany"-"Berlin" \& "France"-"Paris" \\
		$\Rightarrow$ syntactic meaning: "quick"-"quickly" \& "slow"-"slowly" \\
		(e.g. $e_\text{man} - e_\text{woman} \approx e_\text{king} - e_\text{queen}$, where $e_\text{text}$ the embedding for word "text") 
		\item $\Rightarrow$ allow NLP model to be more robust \& generalize better
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Problem of Bias
		\begin{itemize}
		\item word embedding reflect biases of text used to train the model \\
		e.g. "father-doctor" as "mother-nurse" $\Rightarrow$ gender bias
		\item $\Rightarrow$ can cause discrimination when making decision
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Overview}
\begin{itemize}
\item Character Embedding
	\begin{itemize}
	\item One-hot Encoding
		\begin{itemize}
		\item a one-hot vector with length $26$
		\end{itemize}
	\end{itemize}
\item Word Embedding
	\begin{itemize}
	\item Word Dictionary
		\begin{itemize}
		\item a collection of high-frequency word, embedded as one-hot vector
		\item special token \textless{}UNK\textgreater{} for unknown word
		\end{itemize}
	\item Features from Rule Model
		\begin{itemize}
		\item number at each vector location denotes the score for the word matching a rule \\
		(e.g. location for "is\textunderscore{}food" contains score $s\rightarrow1$ for "apple", $s\rightarrow0$ for "man")
		\end{itemize}
	\item Part-of-Speech (POS) Tag
		\begin{itemize}
		\item 
		\end{itemize}
	\item \hyperref[DL_NLP_Langrep_word2vec]{Word2Vec Embedding}
		\begin{itemize}
		\item construct supervised learning from UNlabeled corpus
		\end{itemize}
	\item \hyperref[DL_NLP_Langrep_GloVe]{Global Vector for Word Embedding (GloVe)}
		\begin{itemize}
		\item linear model with simple optimization goal
		\end{itemize}
	\item RNN Encoder
		\begin{itemize}
		\item apply RNN model as encoder on characters in the word \\
		$\Rightarrow$ no more <UNK> or unknown word
		\end{itemize}
	\end{itemize}
\item Sentence/paragraph Embedding
	\begin{itemize}
	\item RNN Encoder
		\begin{itemize}
		\item apply RNN model as encoder on words in the sentence \\ 
		(last hidden layer as encoding)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Word2Vec Embedding} \label{DL_NLP_Langrep_word2vec}
\begin{itemize}
\item N-gram Model
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item simple network to predict the $N+1^\text{th}$ word given previous $N$ words as input \\
		(e.g. using single softmax layer)
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item $E$ randomly initialized \& all words in corpus encoded in one-hot vector
		\item forward prop: word in one-hot $\rightarrow$ lookup $E$ $\rightarrow$ linear layer $\rightarrow$ softmax to predict
		\item training: update linear layer parameters \& matrix $E$ as weights \\ 
		(with cross-entropy loss)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn $P(t|c)$, where $t$ the target word, $c$ the previous $N$ context words
		\item setup a even larger training set from a large corpus
		\end{itemize}
	\item Generalization
		\begin{itemize}
		\item more context: take input from both previous and after words
		\item less \& close context: take only the last word as input $\Rightarrow$ 1-gram model
		\end{itemize}
	\end{itemize}
\item Skip-gram Model
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item choose only $1$ single word as context word \\
		$\Rightarrow$ balance sampling w.r.t. word frequency (e.g. prevent tons of "the", "a", ...)
		\item randomly choose other word(s) in the sentence as target word(s)
		\item $\Rightarrow$ to predict target word(s) given only context word as input \\ 
		$\Rightarrow$ learn word vector representations that are good at predicting the nearby words
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item same as N-gram model
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item harder supervised learning task, yet goal is to learn $E$
		\item better reflect the statistic: similar word appear in similar context \\
		(e.g. "soviet"-"union" appears much more often than "soviet"-"sasquatch") \\
		$\Rightarrow$ embedding for similar target word adjusted with similar gradients \\
		$\Rightarrow$ lie closer in vector space
		\item cons: softmax over large word dict $\Rightarrow$ low computation \\
		$\Rightarrow$ mitigated by hierarchical softmax, noise contrastive estimation (NCE)
		\end{itemize}
	\end{itemize}
\item Skip-gram with Negative Sampling
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item choose a pair of context and target word $(c,t)$ as positive example
		\item generate $k$ negative examples by: same context word $c$ \& random word $t'$ as target
		\item given a pair of words, binary classification: is a (context, target) pair? \\
		$\Rightarrow$ distinguish valid target word from $k$ draws from noise distribution
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item detect meaningful $(c,t)$ pair/phrase by heuristic method \\ 
		e.g. if $c,t$ co-appear within 10-words distance more than a threshold, etc.
		\item $k=5-20$ for small train set; $k=2-5$ for large train set \\
		(larger noise to avoid overfitting)
		\item sample random word $t'$ from modified uniform distribution $\frac 1 Z U(t)^{3/4}$ over words
		\item subsample frequent words: sampled $t'$ discarded by probability $P(t') = 1-\sqrt{\frac {thr} {f(t')}}$ \\
		where $thr$ a threshold, $f(t')$ the frequency of $t'$ in corpus \\
		(to avoid meaningless words like "the", "a", etc.)
		\item $E$ randomly initialized \& all words in corpus encoded in one-hot vector \\
		(forward prop similarly)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn $P(y=1|c,t)$ via logistic regression \\ 
		$\Rightarrow$ much computationally affordable compared to giant softmax (less weights) \\
		$\Rightarrow$ much simpler approach than hierarchical softmax \& NCE
		\item non-linear model (logistic reg) also prefers linear structure of word embedding \\
		$\Rightarrow$ cosine distance still measures (dis-)similarity
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Global Vector for Word Embedding (GloVe)} \label{DL_NLP_Langrep_GloVe}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Context-Target Matrix $X$
		\begin{itemize}
		\item $x_{ij}$: the count of times word $w_i$ appear in the context of word $w_j$ \\
		(context definition can be non-symmetric)
		\end{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item given embedding matrix $E$, minimize $\sum_{i,j}f(x_{ij})(\theta_i^Te_j-\log x_{i,j})^2$, \\
		where $e_j$ the embedding for $w_j$, $\theta_i$ the weights associated with $w_i$
		\item $f(x_{ij})$ a weighting term to balance infrequent-frequent words \\
		($f(x_{ij})$ for $x_{i,j}=0$, preventing $-\inf$ from $\log0$)
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item gradient decent directly optimize the simple objective
		\item final embedding for word $w, w_e = \frac 1 2 (e_w+\theta_w)$ \\
		$\Rightarrow$ as $\theta_w, e_w$ in objective interchangeable
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item directly model the linear structure in word representation \\ 
		(project input $e$ directly to output $\theta^T e$)
		\item final linear structure probably NOT align with human interpretable axis \\
		$\Rightarrow$ yet probably a combination of them (from a higher view)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Addressing Bias in Word Embedding}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item a trained word embedding
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item identify the bias in embedding
		\item eliminate the bias if it appears in undesired places
		\end{itemize}
	\item Identify Bias Direction
		\begin{itemize}
		\item singular value decomposition to identify the axises where biases lie \\
		(similar to a PCA)
		\item e.g. principle component of $e_\text{man}-e_\text{woman}, e_\text{male}-e_\text{female}, ...$
		\end{itemize}
	\item Neutralize
		\begin{itemize}
		\item for all NOT definitional word (where bias should NOT appear) \\
		$\Rightarrow$ project to axises orthogonal to bias axises (to get rid of bias)
		\item e.g. project $e_\text{doctor}$ to the axises to reduce component in bias axises
		\end{itemize}
	\item Equalize Pairs
		\begin{itemize}
		\item for all definitional word (where bias should appear) \\
		$\Rightarrow$ adjust their distance towards non-definitional word to be the same \\
		(may train/handpick all definitional words, which is only a small set)
		\item e.g. make sure $d(e_\text{boy},e_\text{doctor}) = d(e_\text{girl},e_\text{doctor})$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Language Modeling}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item a word with sequence of characters
		\item a sentence with sequence of words/characters
		\item a paragraph with sequence of sentences/words/characters
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Probability Distribution
		\begin{itemize}
		\item model the appearance probability of the sequence $P(\text{z}^1,...,\text{z}^{t})$, \\
		where $z^t$ the token at time $t$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item at each time, output token distribution conditional on previous token(s) \\
		$\Rightarrow y^t = p(z^t|z^1,...,z^{t-1})$, where $z^t$ the token at time $t$
		\item $\Rightarrow$ sequence probability $\displaystyle P(z^1,...,z^{t}) = P(z^1)P(z^2|z^1)...P(z^T|z^1,...,z^{T-1})= \prod^T_{t}y^t$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item $\mathbf 0$ vector as both (initial) hidden state \& input at time $0$ \\
		$\Rightarrow$ estimate $y^1=p(z^1)$, the distribution for being the $1^\text{st}$ token
		\item for time $t=2,...,T$, take input $x^2,...,x^T$ with hidden state $h^1,...,h^{T-1}$ \\ 
		$\Rightarrow$ estimate each conditional distribution (conditioning by passing hidden state)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item for time $1$, input $x^1=\mathbf 0$, previous hidden state $h^0=\mathbf 0$
		\item for time $t=2,...,T$, input $x^t = {z^*}^{t-1}$ the true token of $t-1$ in the given sequence
		\end{itemize}
	\item Generative Model: Sampling New Sequence
		\begin{itemize}
		\item sampling the first token $\hat{z}^1$ according to the distribution $y^1$
		\item for $t=2,...$, take input $x^t=\hat{\text{z}}^{t-1}$, the token sampled from $y^{t-1}$
		\item until $t>T$ or the end signal sampled (e.g. the period "." in a sentence)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Masked Language Model}

\subsection{Name-Entity Recognition}
\subsubsection{}
\begin{itemize}
\item 
\end{itemize}

\subsection{Sentiment Classification}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item a sentence / paragraph
	\end{itemize}
\item Goal
	\begin{itemize}
	\item predict the degree of positive/negative attitude
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item small training set
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Many-to-one RNN
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item each word encoded by word embedding
		\item RNN scanning through paragraphs
		\item last hidden layer as paragraph representation \& used to classify/regress
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Neural Machine Translation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence 
		\begin{itemize}
		\item typically sentence, can be also multiple sentences (paragraph)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item generated sentences in desired language
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN Encoder-Decoder
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item an RNN encodes the input sequence by its last hidden layer
		\item input encoding used as initial hidden state for decoder RNN
		\item decoder RNN generates (conditional) distribution over words at each step \\
		$\Rightarrow$ for time $t, y^t = p(z^t|x^1,...,x^{T_x}, \hat{z}^1,...,\hat{z}^{t-1})$, \\
		where $z^t$ the token at time $t$, $\hat{z}^1,...,\hat{z}^{t-1}$ the tokens chosen from $y^1,...,y^{t-1}$ \\
		($z^t$ a random variable, $\hat{z}^t$ a concrete assignment, $y^t$ a conditional distribution)
		\item unroll until stop sign generated
		\end{itemize}
	\item Understanding: Conditional Language Model
		\begin{itemize}
		\item decoder functions like language modeling, only different in its initial hidden state
		\item $\Rightarrow$ measure the conditional distribution $\displaystyle p(z^1,...,z^{T_y}|x^1,...,x^{T_x})=\prod_{t=1}^{T_y}y_t$, \\
		where $z^1,...,z^{T_y}$ the generated sequence, $x^1,...,x^{T_x}$ the input sequence
		\end{itemize}
	\item Improvement
		\begin{itemize}
		\item combined with attention model
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Choosing Output Sequence}
\begin{itemize}
\item Greedy Search
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item choose the words of highest (conditional) probability at each time step
		\end{itemize}
	\end{itemize}
\item Beam Search
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item with a vocabulary size of $N$, a bean with size $b$, input sequence $\mathbf x = x^1,...,x^{T_x}$
		\item to start, choose top-$b$ tokens (among $N$ tokens) at the $1^\text{st}$ step
		\item for step $t$, input each previous stored $b$ tokens to have $b$ conditional distributions
		\item choose the top-$b$ token pairs (among $b\times N$ pairs) regarding joint probability \\
		$\Rightarrow P(z^1,...,z^t|\mathbf x) = P(z^t|z^1,...,z^{t-1},\mathbf x)P(z^1, ..., z^{t-1}|\mathbf x)$ 
		\end{itemize}
	\item Normalization by Length
		\begin{itemize}
		\item reason: short sequence with less $y^t\in[0,1] \Rightarrow$ larger in general
		\item choose $t^\text{th}$ pair regarding the normalized probability $\frac 1 t P(z^1,...,z^t|\mathbf x)$
		\item $\Rightarrow$ more numerically stable
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item approximately search the sequence with highest joint (conditional) probability \\
		$\Rightarrow$ try to maximize $P(z^1,...,z^{T_y}|x^1,...x^{T_x})$
		\item similar to viterbi algorithm in HMM $\Rightarrow b = 1$ reduce to greedy search
		\item $B$ usually chosen in $10$ in research, $>1000$ in commercial system \\
		(still faster than BFS/DFS, yet no guarantee on finding best result)
		\end{itemize}
	\end{itemize}
\item 
\end{itemize}

\subsection{Speech Recognition}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item an audio sample, with each frame as a time step
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item text (words/sentences) corresponding to the audio
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Variable Timing
		\begin{itemize}
		\item output (letter/words) usually has much less time steps than input (audio frames) \\
		$\Rightarrow$ multiple input time steps corresponding to same output time step
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Learning Objective}
\begin{itemize}
\item Connectionist Temporal Classification (CTC) Loss
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item avoid learning boundaries and timings
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item two sequences considered equivalent if they differ only in alignment, ignoring blanks \\
		$\Rightarrow$ remove duplicate token (e.g. letters) from both sequence before comparison
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN Encoder-Decoder
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item encoder scan through audio frames \& decoder output letter/punctuation/"blank"/"space"
		\item "blank": no symbol v.s. "space": delimiter for letters $\rightarrow$ words
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Trigger Word Detection}
\begin{itemize}
\item Goal
	\begin{itemize}
	\item trigger word: a specific predefined audio signal to invoke system (e.g. xiaodu xiaodu)
	\item detect where trigger word included in an audio (if any)
	\end{itemize}
\item Train Set Setup
	\begin{itemize}
	\item Basic
		\begin{itemize}
		\item $0$ for frames not corresponding to trigger word; $1$ for frames consisting trigger words
		\end{itemize}
	\item upsampling
		\begin{itemize}
		\item upsampling positive example: extends $1$ label a few frames after the trigger words \\
		(as trigger word often appears once in an interaction with system)
		\end{itemize}
	\end{itemize}
\item Classic Approach
	\begin{itemize}
	\item RNN Encoder-Decoder
		\begin{itemize}
		\item encoder scan through audio \& decoder output 0-1 classification at each step
		\end{itemize}
	\item Conv RNN
		\begin{itemize}
		\item a fixed window to better capture context for detecting trigger word
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Machine Reading Comprehension}
\subsubsection{RNN with Attention}
\subsubsection{Convolution with Self-attention - QAnet}

\subsection{Image Caption}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input as the target of description
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Natural Expression
		\begin{itemize}
		\item description of the image in natural language, e.g. English
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Neural Image Caption
	\begin{itemize}
	\item Visual Information
		\begin{itemize}
		\item encoded by CNN backbone into a $1$-D vector
		\end{itemize}
	\item Word Information
		\begin{itemize}
		\item a set of word selected beforehand
		\item word embedding performed
		\end{itemize}
	\item Language Generation
		\begin{itemize}
		\item generated by an LSTM decoder
		\item combining info: visual encoding as initial state of LSTM
		\item process: LSTM gives each word a to-be-selected probability at each time step
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item sampling: sample each word according to the distribution given by LSTM
		\item beam search: iteratively consider extending $k$ best sentence of length $t$ to $t+1$ \\
		$\Rightarrow$ select $k$ best sentence of length $t+1$ from all resulted sentences
		\end{itemize}
		(beam search selected in the paper)
	\end{itemize}
\end{itemize}


\subsection{Referring Segmentation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input for segmentation
		\end{itemize}
	\item Natural Language Expression
		\begin{itemize}
		\item expression to denote the interested object(s)/stuff(s)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Segmentation Mask of Referred Object(s)
		\begin{itemize}
		\item currently (till early 2019), mostly binary segmentation
		\end{itemize}
	\end{itemize}
\item Related Area
	\begin{itemize}
	\item NLP + CV
		\begin{itemize}
		\item referring localization
		\item image caption
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Segmentation from Natural Language Expressions
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item FCN-32s to encode the image into $2$-D feature maps (the last conv layer)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM to encode the sentence into $1$-D vector (the last hidden state)
		\end{itemize}
	\item Combining Info and Output
		\begin{itemize}
		\item per-pixel info: concat [coordinates of current pixel (coord info), language info]
		\item tile the per-pixel info into a feature map, then concat to the spatial info \\
		(per-pixel info concatenated at every pixel of spatial info)
		\item followed by a series of conv and finally a deconv for upsampling
		\end{itemize}
	\item Training
		\begin{itemize}
		\item per-pixel cross-entropy loss
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item special spatial info: coord of each pixel
		\item standard info combination: concatenation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no powerful spatial info encoder: FCN-32s instead of Resnet/Unet...
		\item weak upsampler, compared to encoder-decoder architecture
		\item language info comes late: after downsampling
		\item weak language info: only integrated once
		\end{itemize}
	\end{itemize}

\item Recurrent Multimodal Interaction for Referring Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder (Resnet as backbone, with atrous conv)
		\item then tiled (concat at every pixel) by coord info (coordinate of current pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item word embedding $w_t$ for $t=1,...,T$
		\item LSTM scanning the sentence, with hidden state $h_t$ at time $t$
		\item language info $l_t=$ concat [$h_t$, $w_t$]
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item $l_t$ tiled to spatial info, at each time step \\
		$\Rightarrow$ creating combined feature maps $F_t$ (of shape $[\text{height}, \text{wide}, \text{channel}])$
		\item combined feature maps $F_1,...,F_T$ fed to an convolutional LSTM, \\
		where the ConvLSTM shares weight over both space and time \\
		$\Rightarrow$ feature vector of $F_t[i,j]$ is the input of the ConvLSTM at time $t$ \\
		$\Rightarrow$ conv in ConvLSTM implemented as $1\times1$ conv
		\item a series of conv following the last hidden state of the ConvLSTM
		\end{itemize}
	\item Output
		\begin{itemize}
		\item bilinear interpolated to original input size
		\item optionally post-processed by dense CRF, using pydensecrf (hence inference only)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item more powerful spatial info extractor: DeepLab-101
		\item better language info: integrated at every time step, maintained by an ConvLSTM
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item weak architecture for spatial info: still no upsampling (blur segmentation)
		\item no spatial relation considered in ConvLSTM (?)
		\item weak language representation \\ 
		(better with pos tag, word2vec, word dict, biLSTM, and maybe even attention)
		\item language info still comes late: still after downsampling
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Current State-of-The-Art (early 2019)}
\begin{itemize}
\item Key-Word-Aware Network for Referring Expression Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder for comparability
		\item then tiled by coord info (coordinate of each pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, each hidden state as word info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item attention mask from combined info (spatial info with language info tiled) \\
		(at each time step)
		\item attention weighting over spatial info at each time step \\
		$\Rightarrow$ an $1$-D global encoding for each time step (via weighted mean over space) \\
		$\Rightarrow$ filling feature maps: global encoding if attention here $>$ threshold; else $\mathbf 0$ \\
		$\Rightarrow$ summing filled feature maps over time for the global spatial maps $c$
		\item attention weighting over tiled language info at each time step, correspondingly \\
		$\Rightarrow$ tiled language info maps summed over time for the global language maps $q$
		\item concat [spatial info, $c$, $q$], followed by $1\times1$ conv
		\end{itemize}
	\item Output
		\begin{itemize}
		\item upsampling performed
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item attention introduced: from combined info
		\item better combination: attention masked interact with both spatial \& language info
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item blur segmentation: no encoder-decoder architecture
		\item attention mask obtained sequentially: only last mask has complete language info
		\item language info comes late: after downsampling
		\end{itemize}
	\end{itemize}
\item Referring Image Segmentation via Recurrent Refinement Networks
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLabl ResNet-101 as encoder
		\item last feature maps tiled (concat at each pixel) with coord info
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, generating word info at each time step
		\item last hidden layer as language info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item combined info $=$ spatial info tiled with language info
		\item selecting set of feature maps from downsampling stages
		\item all selected feature maps resized and fed to $1\times1$ conv \\ 
		$\Rightarrow$ to match the dimensions of combined info
		\item convolutional LSTM applied to refine the combined info \\ 
		(with matched selected feature maps as input at each time step)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a conv after final hidden state of ConvLSTM for segmentation
		\item upsampled to original image size
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item ConvLSTM integrating info at dowsampling stage $\Rightarrow$ segmentation refined
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no upsampling: blur segmentation, mitigated by ConvLSTM though \\
		(yet no language info introduced in refinement)
		\item CNN fixed during training: relying on ConvLSTM
		\item single info combination: only by tiling \\
		(though, currently performing the best in all dataset)
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsubsection{Research Direction}
\begin{itemize}
\item Integrating Encoder-Decoder Architecture
	\begin{itemize}
	\item Upsampling
		\begin{itemize}
		\item similar to Unet, concat low-level spatial info
		\item introduce language info as well \\
		(e.g. early combination, explicit introducing, ...)
		\end{itemize}
	\end{itemize}
\item Early Info Combination
	\begin{itemize}
	\item Tiling at First Conv
		\begin{itemize}
		\item downsampling more responsible for language info processing \\
		$\Rightarrow$ hopefully get more fine-tuning alone with conv filters
		\item can be used with pre-trained net: \\ 
		$ReLU(conv_1*X_1 + conv_2*X_2) = ReLU( [conv_1,conv_2]*[X_1,X_2] )$
		\end{itemize}
	\item Multiple Entries
		\begin{itemize}
		\item combining info at different stages of downsampling / upsampling
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Attention from Combined Info
		\begin{itemize}
		\item as key-word-aware net
		\end{itemize}
	\item Attention on Language Info
		\begin{itemize}
		\item $1$-D spatial pyramid pooling / attention mask on the sentence encoding
		\end{itemize}
	\end{itemize}
\item Language Info Throughout Network
	\begin{itemize}
	\item Encoder-Decoder for Language Info
		\begin{itemize}
		\item network asked to recover language info after processing combined info \\ 
		(potentially via a separate branch only at training time) \\ 
		$\Rightarrow$ auxiliary loss
		\end{itemize}
	\item Language as Conv Filter
		\begin{itemize}
		\item Language Info, through a subnet, becoming a set of conv filters \\
		$\Rightarrow$ then imposed in downsampling, tunnel, upsampling or bridge stage(s)
		\end{itemize}
	\end{itemize}
\item Data Augmentation
	\begin{itemize}
	\item Translation Module
		\begin{itemize}
		\item using the same image
		\item expression translated to a middle language and then back to English \\
		$\Rightarrow$ language info trained more finely
		\end{itemize}
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Special Learning}
\subsection{Transfer Learning} \label{DL_Learning_Transfer}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Source Data
		\begin{itemize}
		\item a large amount of labeled data
		\item having different distribution then the desired target data
		\end{itemize}
	\item Target Data
		\begin{itemize}
		\item a small amount of labeled data, with a large amount of unlabeled data \\
		(due to hardness of labeling, etc.)
		\item from the distribution where model need to handle
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Model Performance
		\begin{itemize}
		\item good performance on val\&test set (containing target data)
		\item good generalization ability on the target distribution
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Standard Baseline}
\begin{itemize}
\item Pre-training \& Fine-tunning
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item distribution of source \& target data share some common features \\
		$\Rightarrow$ different task shares some common knowledge
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item train network on source data only
		\item swap/modify the last few layers (including prediction layer)
		\item retrain the last layer (limited target data) / all net (enough target data)
		\end{itemize}
	\item Guideline
		\begin{itemize}
		\item small dataset: freeze pretrained network \& use it as fixed feature extractor \\ 
		$\Rightarrow$ only train the last prediction layer
		\item medium dataset: freeze fewer layers, design some own last layers
		\item exceptionally large dataset with large computation budget: train from scratch \\
		$\Rightarrow$ pretrained weights as initialization (nor preferred in most cases)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item sharing weights/structure: low level feature extraction useful for both \\
		$\Rightarrow$ based on model ability
		\end{itemize}
	\end{itemize}
\item Transfer Ada Boost (trAdaBoost)
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item distribution of target \& source overlap more or less \\
		$\Rightarrow$ able to extract helpful guides from source data
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item setup train set with mixed target \& source data
		\item weighting example from target \& source differently: \\ 
		for source data weight $= \frac 1 {N_\text{source}}$; target data weight $= \frac 1 {N_\text{target}}$ \\
		$\Rightarrow$ target data more important (as smaller in number)
		\item for each weight-update iteration (may contain multiple epochs), update the weight: \\
		$\Rightarrow$ shift the weight (importance) towards target data \& normalize all the weight
		\end{itemize}
		$\Rightarrow$ based on data distribution
	\item Understanding
		\begin{itemize}
		\item learn the shared feature/knowledge with the help of source data
		\item focus more on target as making progress
		\end{itemize}
	\end{itemize}
\item Feature Projection
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item few or NO overlap between source \& target (as data examples directly)
		\item source \& target can be mapped onto a shared feature space, where overlap can be discovered
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item project/map the source \& target data onto the same feature space
		\item transfer learning in the shared space
		\end{itemize}
		$\Rightarrow$ based on distribution transformation
	\item Understanding
		\begin{itemize}
		\item try discover common feature through transformation \\ 
		(may need a decoder to map back to desired output space)
		\end{itemize}
	\item Example
		\begin{itemize}
		\item \hyperref[DL_NLP_Langrep]{word embedding}: learn word relation as unsupervised learning \\
		$\Rightarrow$ a shared feature space to represent word
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Multi-task Learning}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Multi-labeled Data
		\begin{itemize}
		\item one input data corresponds to multiple desired outputs
		\item $\Rightarrow$ require similarity/common knowledge in different tasks \\
		(e.g. object detection for multiple object types)
		\end{itemize}
	\item Partial-labeled Data
		\begin{itemize}
		\item desired outputs may not be all labeled in the input \\ 
		(i.e. some may be missed)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item General Solution to Multi-task
		\begin{itemize}
		\item give all desired outputs from a single network
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Standard Baseline}
\begin{itemize}
\item Single Networks with Multiple Predictions
	\begin{itemize}
	\item Sharing
		\begin{itemize}
		\item shared low-level layers to extract features from the input
		\item shared loss as a sum over all prediction for corresponding label
		\item shared training as back-prop computed as a single network
		\item shared input data as trained together \\
		$\Rightarrow$ shared knowledge discovered when training on data for other tasks
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item each task help each other, by contributing to the common knowledge
		\item overcome data shortage: augmented by data for other tasks
		\item partial labeled still useful: help train the shared layers
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{K-shot Learning}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Training Set
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

