\chapter{Deep Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research}

\subsubsection{Knowledge Embedding}
\begin{itemize}
\item BP
	\begin{itemize}
	\item psychology view: knowledge in vectors
	\item semantic AI: knowledge graph
	\item BP algorithm can interpret \& convert between feature vector and graph representation (with some embedding)
	\end{itemize}
\item Boltzmann Machine
	\begin{itemize}
	\item Leaning Algorithm on Density Net
		\begin{itemize}
		\item same information in forward \& backward propagation to learn feature embedding
		\end{itemize}
	\end{itemize}
\item Restricted Boltzmann Machine (RBM)
	\begin{itemize}
	\item ways of learning in deep dense net with fast inference
	\item iterative learning (adding layer after the above trained)
	\item ReLU $\Leftrightarrow$ a stack of sigmoid functions (approximately) in RBM
	\item ReLU units initialized to identity for efficient learning
	\end{itemize}
\item EM
	\begin{itemize}
	\item EM with Approximate E Step
	\end{itemize}
\item vs. Symbolic AI
	\begin{itemize}
	\item Symbolic AI: symbolic logic-like expression to do reasoning
	\item yet, maybe state vector to represent knowledge
	\end{itemize}
\end{itemize}

\subsubsection{Brain Science}
\begin{itemize}
\item Brain: Nets Implemented by Evolution
	\begin{itemize}
	\item trying to train without BP
	\item doing BP (get derivatives) with re-construction error (auto-encoder)
	\end{itemize}
\end{itemize}

\subsubsection{Memory in Nets}
\begin{itemize}
\item Fast Weights for Short-term Memory
\item Capsule Net
	\begin{itemize}
	\item structured knowledge representation in each unit (feature with sets of property)
	\item $\Rightarrow$ enable nets to vote rather than filtering - thus better generalization
	\item now working: published in \textbf{2017 NIPS}
	\end{itemize}
\end{itemize}

\subsubsection{Unsupervised Learning}
\begin{itemize}
\item Importance
	\begin{itemize}
	\item better than human eventually (as supervised learning has limited maximum)
	\item GAN as a breakthrough
	\end{itemize}
\end{itemize}

\subsubsection{"Slow" Feature}
\begin{itemize}
\item Non-linear Transform to Find Linear Transform
	\begin{itemize}
	\item find a latent representation containing linear transform to do the work
	\item e.g. change viewpoints: pixels $\rightarrow$ coordinates $\rightarrow$ linear transform $\rightarrow$ back to pixels
	\end{itemize}
\end{itemize}

\subsubsection{Relations between Computers}
\begin{itemize}
\item showing computer data to work
	\begin{itemize}
	\item instead of programming it to work
	\end{itemize}
\end{itemize}


\subsubsection{Deep Reinforcement Learning}
\begin{itemize}
\item Overall Challenge 
	\begin{itemize}
	\item Representation
	\item Exploration Problem
	\item Credit Assignment
	\item Worst Case Performance
	\end{itemize}
\item Advantage (Deep Nets in RL)
	\begin{itemize}
	\item network capturing the representation (state vector)
	\end{itemize}
\item Question in DRL
	\begin{itemize}
	\item how to learn safely
	\item how to keep learning (under small negative samples) e.g. better than human
	\item can we learn the reinforcement learning program (RL in the RL)
	\item long time horizon
	\item use experience across tasks
	\end{itemize}
\item Success of DRL
	\begin{itemize}
	\item simulated robot inventing walking... $\Rightarrow$ single general algorithms to learn
	\end{itemize}
\end{itemize}


\subsubsection{Generative Adversarial Networks}
\begin{itemize}
\item Generative Models
	\begin{itemize}
	\item Resembling
		\begin{itemize}
		\item trained to optimized the distribution behind training data \\ 
		(then sampled from that distribution to get more imaginary training data)
		\item $\Rightarrow$ produce data to resemble the training data
		\end{itemize}
	\item Usage
		\begin{itemize}
		\item semi-supervised learning
		\item data augmentation
		\item simulating scientific experiment
		\end{itemize}
	\item Previous Ways
		\begin{itemize}
		\item Boltzmann Machine
		\item Sparse Coding
		\end{itemize}
	\item Now: Generative Adversarial Networks (GANs)
	\item Future
		\begin{itemize}
		\item increase reliability of GANs (stabilizing)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Yoshua Bengio Thoughts}
\begin{itemize}
\item Fallacy
	\begin{itemize}
	\item Smoothness in Nonlinearity
		\begin{itemize}
		\item to ensure non-zero gradients every where
		\end{itemize}
	\end{itemize}
\item Surprising Fact
	\begin{itemize}
	\item ReLU in Deep Net
		\begin{itemize}
		\item inspired initially by biological connection
		\end{itemize}
	\end{itemize}
\item \textbf{Distribution v.s. Symbolic Representation}
	\begin{itemize}
	\item Distributed Representation 
		\begin{itemize}
		\item distributed in lots of units, instead of a symbolic representation in a single cell \\
		(agree on Geoffrey Hinton)
		\end{itemize}
	\item Curse of Dimensionality
		\begin{itemize}
		\item neural net's distributed representation for joint distribution over random variables
		\end{itemize}
	\item $\Rightarrow$ Word Embedding
		\begin{itemize}
		\item generalized to joint distribution over sequence of words
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Yoshua Bengio Works}
\begin{itemize}
\item Piecewise Linear Activation (PLU)
\item Unsupervised Learning
	\begin{itemize}
	\item Focus
		\begin{itemize}
		\item Denoising auto-encoder
		\item GANs
		\end{itemize}
	\item Importance
		\begin{itemize}
		\item human ability: self-teaching, building world-model from perception
		\end{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item underlying concept across two fields: machine can learn through interactions \\
		$\Rightarrow$ learning "good" representation (yet, what is "good")
		\end{itemize}
	\item Possible Directions
		\begin{itemize}
		\item loss function: not even defined for each task \\ 
		(not knowing which is good for what?)
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Machine Translation (Founder)
	\item Generalized into Other Fields
	\end{itemize}
\item Back-prop in Brains (Neural Science
	\begin{itemize}
	\item Reasons for Efficiency of Backprop
	\item Larger Family behind Credit Assignment
	\end{itemize})
\end{itemize}

\subsubsection{Human Benchmark}
\begin{itemize}
\item Programming by Showing
	\begin{itemize}
	\item Requirement
		\begin{itemize}
		\item input + output as specification
		\item metric as goal
		\end{itemize}
	\item Writer
		\begin{itemize}
		\item the optimizer
		\end{itemize}
	\end{itemize}
\item Understanding Importance of Benchmark
	\begin{itemize}
	\item importance to do better given the current performance on the dataset \\
	(as important increase after passing human error)
	\end{itemize}
\item Understanding Network Behavior
	\begin{itemize}
	\item compared to the process of human decision
	\end{itemize}
\end{itemize}
\subsubsection{Transfer Learning}
\begin{itemize}
\item Image Task
	\begin{itemize}
	\item feature extractor + fine tune/modification onto various task
	\end{itemize}
\end{itemize}

\subsubsection{Restricted Boltzmann Machine}
\begin{itemize}
\item Auto Encoder
	\begin{itemize}
	\item Encoding All Kind of Data
		\begin{itemize}
		\item from digit to face, document, etc...
		\item deeper and deeper structure
		\end{itemize}
	\end{itemize}
\item Training Boltzmann Machine
	\begin{itemize}
	\item Pretraining
		\begin{itemize}
		\item increase the low boundary by training the previous layer
		\item then add another layer to train, ...
		\end{itemize}
	\item Direct Training (with GPU)
		\begin{itemize}
		\item similar, or better result
		\end{itemize}
	\end{itemize}
\item Boltzmann Machine Ability
	\begin{itemize}
	\item Generative Model
		\begin{itemize}
		\item model coupling distributions in data \\
		$\Rightarrow$ scalable (more scalable than current model\&operation)
		\item only way to train the model in the early age
		\end{itemize}
	\end{itemize}
\item Progress on Generative Model
	\begin{itemize}
	\item probabilistic max pooling
	\item variational encoder
	\item deep energy model
	\item semi-supervised Model
	\end{itemize}
\end{itemize}

\subsection{Directions}
\subsubsection{Topics}
\begin{itemize}
\item Point Cloud
	\begin{itemize}
	\item Bounding Box Directly from Points: no voxel
		\begin{itemize}
		\item clustering + regression on each cluster?
		\end{itemize}
	\end{itemize}
	
\item Unsupervised Learning
	\begin{itemize}
	\item Deep Belief Nets
	\end{itemize}

\item Reinforcement Learning
	\begin{itemize}
	\item Deep Reinforcement Learning
		\begin{itemize}
		\item scalable system
		\item communicative\& cooperating agents
		\end{itemize}
	\end{itemize}

\item One-shot / Transfer Learning
	\begin{itemize}
	\item Learning the Ability to Learn
	\end{itemize}

\item General AI
	\begin{itemize}
	\item Structure for General Task
		\begin{itemize}
		\item neural network or other structure, shared for multiple tasks \\
		(instead of breaking down to different parts like segmentation, detection, etc.) \\
		(instead of the split of cv, nlp, planning, etc.)
		\item $\Rightarrow$ a full agent (instead of decomposed function) \\
		$\Rightarrow$ optimization method/objective need to be carefully defined
		\end{itemize}
	\item Attempt for General AL
		\begin{itemize}
		\item scaling up supervised learning: imitating human
		\item unsupervised learning: AIXI, artificial evolution, etc.
		\end{itemize}
	\end{itemize}

\item AI Security
	\begin{itemize}
	\item Anti Inducing
		\begin{itemize}
		\item NOT to be fooled/induced to do unappropriated things \\
		(even if algorithm is right)
		\end{itemize}
	\item Built-in Security
	\end{itemize}
\item Fairness in AI
	\begin{itemize}
	\item Dealing Societal Issue
	\item Reflecting Preferred Bias
	\end{itemize}
\item Auto Optimization (Hyperparameter Tunning)
	\begin{itemize}
	\item Swarm Optimization
	\item Expectation Maximization
		\begin{itemize}
		\item target variable $\theta=$ hyperparameters
		\item hidden variable $Z=$ weights of network
		\item data $X=$ dataset
		\end{itemize}
	$\Rightarrow$ 
		\begin{itemize}
		\item E-step: evaluate $\displaystyle \mathbb E_{Z|\theta_n,X}(\ln P(Z,X|\theta))$
			\begin{itemize}
			\item $\ln P(Z, X|\theta)$: log likelihood of hyperparam $\theta$ (for weights \& data to be observed)
			\item $P(Z|\theta_n, X)$: posterior of weights $Z$ 
			\end{itemize}
		$\Rightarrow$ evaluate (approximate) the expectation of the log likelihood of hyperprarm $\theta$ \\
		(from a functional view, train with $\theta_0-\theta_N$, evaluate model $M$ times in training, thus with weights $Z_{00}-Z_{NM}$) \\
		$\Rightarrow$ a matrix with $n$ as row entry, $m$ as column entry, mapping to both $\ln P(Z, X|\theta), P(Z|\theta_n, X)$
		$\Rightarrow$ then marginalize (taking the expectation) over $Z$, to get a (sampled) function over $\theta$
		\item M-step: maximize the result function from E-step
			\begin{itemize}
			\item fit a curve \& maximize w.r.t hyperparams $\theta$
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item World Understanding: after perception
	\begin{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item machine learns from interactions
		\item machine builds a representation of world (like human ability, without fine label)
		\end{itemize}
	$\Rightarrow$ building world-model from perception
	\item Causality Mining
	\end{itemize}
\item Model Interpretation
	\begin{itemize}
	\item Logical Formalization
		\begin{itemize}
		\item deep learning can be understood logically \\
			e.g. what make deep net training harder? understand the limit of current algorithm/model and \textbf{why}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Advises}
\begin{itemize}
\item Learning Direction
	\begin{itemize}
	\item Math
		\begin{itemize}
		\item statistic
		\item linear algebra
		\item calculus
		\item optimization
		\end{itemize}
	\end{itemize}
\item Reading
	\begin{itemize}
	\item read a little bit \& find somewhere intuitively not right
	\begin{itemize}
		\item good intuition: eventually work; \\ 
		bad intuition: not working no matter what it is doing
		\item if other doubts your idea as bullshit $\Rightarrow$ a sign for real good result
	\end{itemize}
	\item a supervisor with similar belief
	\item PhD vs. Company
		\begin{itemize}
		\item amount of mentoring
		\item faster if dedicated supervisor available
		\item resource
		\end{itemize}
	\end{itemize}

\item Practice
	\begin{itemize}
	\item open-source learning resource
	\item open source contribution
		\begin{itemize}
		\item contribute to open source framework (e.g. conv on sparse matrix in TF)
		\item implement the paper, the open source it (as a tool for other)
		\item work on a projected and open source it \\
		$\Rightarrow$ the stage (e.g. github) will bring people to you
		\end{itemize}
	\item implement the tools: to find out how \& why it works \\
	$\Rightarrow$ derive theories from the 
	\item full stack of understanding \\ 
	$\Rightarrow$ understand the implementation under the deep learning framework
	\end{itemize}
\end{itemize}

\subsubsection{Direction - Segmentation}
\begin{itemize}
\item  Multi-scale in Segmentation
	\begin{itemize}
	\item Per-pixel Classification
		\begin{itemize}
		\item not only skipping from encoder to decoder, but also skipping from tunnel to decoder final output layer, by e.g. tiling.
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Direction - Detection}
\begin{itemize}
\item Faster One-stage
	\begin{itemize}
	\item Segmentation as Detection
		\begin{itemize}
		\item for output mask, giving classification ($p_e$) + localization (regression) \\
		(instead of giving class probability as per-pixel classification)
		\item at most as many objects as pixel number \\ 
		$\Rightarrow$ not possible to lose detection due to griding
		\item one-step further from ExtremeNet: \\
		directly segmentation, then use boundary pixel to re-organize into bbox
		\end{itemize}
	\end{itemize}
\item Two-stage in One-stage
	\begin{itemize}
	\item Attention
		\begin{itemize}
		\item attention map as 1st box proposal, as extreme net (instead of RoI)
		\end{itemize}
	\end{itemize}
\item \textbf{No Non-Maximum Suppression}
	\begin{itemize}
	\item Auto Filtering
		\begin{itemize}
		\item regress threshold of objectness as well \\
		$\Rightarrow$ objectness under threshold not considered by encoder-decoder \\
		(under the GAN framework ?) \\
		(under multi-tasking ?)
		\end{itemize}
	\item CRF
		\begin{itemize}
		\item still not end2end, yet trainable
		\end{itemize}
	\item RNN Encoder-Decoder
		\begin{itemize}
		\item encode all bbox / spatial feature, then decode (generate) till end-of-sequence \\
		(as YOLOv1 still use dense layer...)
		\end{itemize}
	\end{itemize}
\item Directly Maximize IoU
	\begin{itemize}
	\item seems to have been done in some works...
	\end{itemize}
\end{itemize}

\subsubsection{Direction - Tracking}
\begin{itemize}
\item Current State - 2019
	\begin{itemize}
	\item NN in Data Association
		\begin{itemize}
		\item pred association prob of prediction-detection in JPDA
		\item pred prob of detection being true / false alarm (the existence prob for bbox), used in MHT
		\end{itemize}
	\item Tracking Initiation \& Deletion
		\begin{itemize}
		\item currently, by rules... \\
		e.g. 3 observations in 4 consecutive frames; observation missed for 4-7 frames
		\end{itemize}
	\item Prediction Encoding
		\begin{itemize}
		\item crop the image according the predicted area (gating)
		\end{itemize}
	\item Deep Backbone
		\begin{itemize}
		\item deeper wider siamrpn
		\end{itemize}
	\end{itemize}
\item NN in Gating
	\begin{itemize}
	\item Region Proposal Network
		\begin{itemize}
		\item propose the search region (gating) by RPN \\ 
		(instead of using rule based on current track location) \\
		$\Rightarrow$ predict a large region with RPN based on track info \\
		(down by siamRPN?)
		\end{itemize}
	\end{itemize}
\item End-to-end / unified framework
	\begin{itemize}
	\item End-to-End Regression for Data Association in MOT
		\begin{itemize}
		\item cite: Online Multi-Target Tracking Using Recurrent Neural Networks
		\item directly output association decision
		\item input: raw image (semantic), current detection, track prediction, track history
		\item track-level info: extended Siamese for relation of: track - current detection \\
		(instead of only det-det in $2$ frames)
		\item fully NN approach for JPDA \\
		$\Rightarrow$ track - all det: concat (tile) track+pred to each det $\Rightarrow$ use fcn, then pooling to regress to a bbox for update \\
		(direct regress the box, instead of Siamese net + weighted sum) \\
		(cite: Data-Driven Approximations to NP-Hard Problems)
		\item $\Rightarrow$ fast JPDA: single RNN encode all detections as $D$, then regress for each track-$D$ for update \\
		(instead of num of track $\times$ num of det, now only num of track + num of det)
		\item fully NN approach for MHT ??? \\
		trace-back available ... encoding past detection as well? (in the tracker?) \\ 
		N-scan pruning?
		\end{itemize}
	\item Track Initiation/Deletion
		\begin{itemize}
		\item directly output decision result \\
		$\Rightarrow$ regress the decision boundary as well (instead of universal $0.5$)
		\end{itemize}
	\item \underline{Interesting Predicted Bounding Box Encoding}: attention instead of cropping
		\begin{itemize}
		\item direct encoding: x,y,w,h, o, + encoding of full image
		\item mask: bounding box plot onto a separate mask, concat to the (encoding of) full image \\
		further, attention mechanism \\
		(soft-crop ???) \\
		$\Rightarrow$ naturally develop into \textbf{instance tracking}, even tracking in point cloud data
		\item $\Rightarrow$ \textbf{crop with extension} can significantly draw back the wall time \\
		(due to: 1. crop in cpu \& for each object; 2. crop is not generally gpu accelerated) \\
		especially, when object is large in the image $\Rightarrow$ need to pad a lot \\
		(e.g. baidu field-end tracking: car emerging/leaving right under the camera)
		\item two masks: one for all bbox from detector, one for current track
		\end{itemize}
		$\Rightarrow$ enable global track for multiple obj track (single RNN for all track in an image)
	\item \underline{Scale Invariance}
		\begin{itemize}
		\item current siamRPN++: trained with a fixed-scale resized image crop as input \\
		$\Rightarrow$ if object is non-rigid: pixel size can change significantly between frames \\
		$\Rightarrow$ crop image according to last frame would contain crop out far more/less background than expected \\
		$\Rightarrow$ wrong percentage of fore-/back-ground in the image crop \\
		$\Rightarrow$ model input (as resized to fixed size) has different object scale than training \\
		$\Rightarrow$ failed to track \& fall back to only detection (as containing RPN) \\
		(reproduced by having different context\_amount in inference-training)
		\item siamFC: in test time, extract template at multiple (e.g. 3) scales for each target
		\item HENCE, try to have scale invariance to address abrupt size change in target
		\end{itemize}
	\item SOT
		\begin{itemize}
		\item directly use cnn + conv-lstm
		\end{itemize}
	\item \underline{Tracking by Detection}
		\begin{itemize}
		\item use last-frame track box as proposal in 2-stage detector \\
		or, incorporate with siamRPN: siam cascade-RPN with track box as proposal for $\ge$2nd stage RPN
		\end{itemize}
	\item single Obj Tracking for MOT
		\begin{itemize}
		\item common cnn encodes image $t$ as $f_t$, detected bbox as $D_t$ (given or trained) \\
		(prefer to train, as auxiliary loss for multi-tasking ?)
		\item encode each detection $d_t \in D_t$ as $c(d_t) \in c(D_t)$ \\
		empty detection always $\in$ $D_t$ \\
		$\Rightarrow$ track may always have NO compatible det
		\item one rnn tracker for each $c(d_t)$
		\item rnn takes in encoded previous prediction $c(p_{t-1})$ (produced by itself), with $f_t$, and each encoded det $c(d_t) \in c(D_t)$ \\
		$\Rightarrow$ produce as many track as $D_t$, each with a probability \\ 
		(control the track's death) \\
		$\Rightarrow$ only the highest remains, others deleted
		\item remaining track rnn regress pred bbox $p_{t+1}$ (an encoded bbox as well) \\
		(a decoder at $t$ trying to induce the $d_{t+1}$) \\
		(GAN ?)
		\end{itemize}
	\item MOT as SOT
		\begin{itemize}
		\item attention mask contain multiple interest \\
		$\Rightarrow$ not able to handle birth/death of independent object \\
		(as modeled all as a whole)
		\end{itemize}
	\item End-to-End MOT
		\begin{itemize}
		\item design requirement
			\begin{itemize}
			\item arbitrary start of track
			\item arbitrary num of track
			\item arbitrary end of track
			\end{itemize}
		\end{itemize}
	\item End-to-end MOT with States (Markov Decision Process / Deterministic Finite Automaton)
		\begin{itemize}
		\item action at each state given by NN
		\item history of track in RNN $\Rightarrow$ provided when making decision \\
		$\Rightarrow$ RNN modeling all the state ??? \\ 
		(update with different set of weights on different chosen actions ?) \\
		(design transition instead of states: states as the closure of all actions ?)
		\item design state for each challenge scene separately (out-of scene, occlusion, long-term lost, etc.) \\
		$\Rightarrow$ directly tackle each scene \\
		(state growing: auto-discover state ???)
		\item RNN for actions ? \\
		output score for all actions, only legal actions (given current state) considered\&selected, then transfer state accordingly
		\item trained with RL
		\end{itemize}
	\end{itemize}
\item Training
	\begin{itemize}
	\item MDP as Hard-Negative Mining \\
		\begin{itemize}
		\item yet, hard negative discovered in DaSiam / siam cascaded-RPN \\
		(in terms of data imbalance)
		\end{itemize}
	\item Sparse Detection
		\begin{itemize}
		\item feed only a few detection, demand NN to fill up using track history\&prediction
		\end{itemize}
	\item Other NLP Training
		\begin{itemize}
		\item training of n-gram model used on rnn traker ?
		\end{itemize}
	\end{itemize}
\item Fixable Track
	\begin{itemize}
	\item Fixing After Lost
		\begin{itemize}
		\item backward rnn for re-associated track after lost \\
		$\Rightarrow$ to fix the previous prediction given current observation \\
		(then recompute the forward rnn for consecutive tracking)
		\end{itemize}
	\end{itemize}
\item \underline{SiamRPN Tracking}
	\begin{itemize}
	\item further branches after xcorr?
		\begin{itemize}
		\item as currently, xcorr to directly produce regression \\ 
		(followed by a 1x1 conv to adjust channel for classification) \\
		$\Rightarrow$ equip with a further conv pipe? (including 3x3) to further utilize semantic similarity? \\
		(enable deep power after xcorr)
		\end{itemize}
	\item padding removal for deep power
		\begin{itemize}
		\item siamRPN++: cropping for template branch \& data augmentation for search branch
		\item siamDW: crop-inside unit: as early as where the padding is introduced
		\item $\Rightarrow$ crop before xcorr \& before final regression
		\end{itemize}
	$\Rightarrow$ try to break the 20-layer limits in siamDW \\
	(20-layer limit not happened in siamRPN++, possibly due to cropping???) \\
	(or, is it data augmentation always demanded to embrace deep power???) \\
	\item $\Rightarrow$ \textbf{review another factor for central bias}
		\begin{itemize}
		\item data aug really solves: central bias is the thing
		\item crop NOT solve completely $\Rightarrow$ other factors causing central bias
		\item perhaps due to the intrinsic central bias in conv ???
		\end{itemize}
	\item $\Rightarrow$ avg padding (use channel-wise mean, instead of 0, to pad)
		\begin{itemize}
		\item conv contains intrinsic central bias: center pixel get more convolution \\
		$\Rightarrow$ padding actually \textbf{reserve} border info \\
		$\Rightarrow$ keep the upside \& remove the downsid of padding \\
		i.e. NOT introduce location signal (0), but channel-wise avg
		\item if it works: proof that central bias is introduced in deep tracker:
			\begin{itemize}
			\item intrinsically from conv
			\item by 0-signal from 0-padding \& mostly centralized label box
			\end{itemize}
		\end{itemize}
	\item Removing Post-process
		\begin{itemize}
		\item cosine window: seems to be solved in siamrpn++ (by deep power?)
		\item score penalty: to be tested
		\end{itemize}
	\item \underline{GAN to Improve Transformation Learning}
		\begin{itemize}
		\item auxilary loss in siam tracker to distinguish from true search target, or a fake search region
		\item may train the image patch generator first with a simple classifier
		\item then append the classifier to the siam-tracker $\Rightarrow$
			\begin{itemize}
			\item use the gradient from GAN classifier branch to boost siam performance
			\item use the generated data for more diverse training data
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Direction - Point Cloud}
\begin{itemize}
\item Voxel Grouping + Per-Point Ops
	\begin{itemize}
	\item PointNet++: multiple random sampling for local grouping in inference
	\item VoxelNet: voxelization as one-off grouping \& 3D conv for local context mining
	\item Voxel-Point:
		\begin{itemize}
		\item one-off grouping by voxelization
		\item local context by concat local max-ppoling \\ 
		$\Rightarrow$ hierarchical max-pooling, instead of 3D conv / repeated sampling \\
		(implemented as 3D spp at different stage?)
		\item after concat local feature, still per-point ops to process
		\item $\Rightarrow$ faster, better aligned with detection from point cloud? \\
		(propose/regress box from each point / local feat)
		\end{itemize}
	\end{itemize}
\item Bird-View
	\begin{itemize}
	\item Flexible Voxelization
		\begin{itemize}
		\item voxelize based on multiple resolution - spp \\
		(minimal resolution = 1/2 minimal object size)
		\item point net for each voxel
		\end{itemize}
	\end{itemize}
\item Unsupervised Learning?
\end{itemize}

\subsubsection{Direction - Referring Seg}
\begin{itemize}
\item Integrating Encoder-Decoder Architecture
	\begin{itemize}
	\item Upsampling
		\begin{itemize}
		\item similar to Unet, concat low-level spatial info
		\item introduce language info as well \\
		(e.g. early combination, explicit introducing, ...)
		\end{itemize}
	\end{itemize}
\item Info Early Fusion
	\begin{itemize}
	\item Tiling at First Conv
		\begin{itemize}
		\item as siamese net for joint input
		\item downsampling more responsible for language info processing \\
		$\Rightarrow$ hopefully get more fine-tuning alone with conv filters
		\item can be used with pre-trained net: \\ 
		$ReLU(conv_1*X_1 + conv_2*X_2) = ReLU( [conv_1,conv_2]*[X_1,X_2] )$
		\end{itemize}
	\item Multiple Entries
		\begin{itemize}
		\item combining info at different stages of downsampling / upsampling
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Attention from Combined Info
		\begin{itemize}
		\item as key-word-aware net
		\end{itemize}
	\item Attention on Language Info
		\begin{itemize}
		\item $1$-D spatial pyramid pooling / attention mask on the sentence encoding
		\end{itemize}
	\end{itemize}
\item Language Info Throughout Network
	\begin{itemize}
	\item Encoder-Decoder for Language Info
		\begin{itemize}
		\item network asked to recover language info after processing combined info \\ 
		(potentially via a separate branch only at training time) \\ 
		$\Rightarrow$ auxiliary loss
		\end{itemize}
	\item Language as Conv Filter
		\begin{itemize}
		\item Language Info, through a subnet, becoming a set of conv filters \\
		$\Rightarrow$ then imposed in downsampling, tunnel, upsampling or bridge stage(s)
		\end{itemize}
	\end{itemize}
\item Data Augmentation
	\begin{itemize}
	\item Translation Module
		\begin{itemize}
		\item using the same image
		\item expression translated to a middle language and then back to English \\
		$\Rightarrow$ language info trained more finely
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Direction - Tactile Perception}
\begin{itemize}
\item Close-range Camera as Tactile Sensor
	\begin{itemize}
	\item directly location mapping between camera \& tactile sensor via IMU \\
	$\Rightarrow$ jointly process in cnn
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Basic Neutral Network}

\subsection{Overview}
\subsubsection{Advantages}
\begin{itemize}
\item Larger Maximum Capability
	\begin{itemize}
	\item Curve given Amount of Data
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/background-largedata".png}
	\item Reasons
		\begin{itemize}
		\item the scale of data (labeled)
		\item the scale of neural network (computability)
		\item the scale of efficiency: e.g. ReLu, faster parallel algorithm
		\end{itemize}
	\end{itemize}
\item Flexibility
	\begin{itemize}
	\item Different Structures for Different Tasks
		\begin{itemize}
		\item changing settings/structures of deep learning model can make a difference \\
		(v.s. SVM, etc.) 
		\end{itemize}
	\item Ability to Choose Basis Functions
		\begin{itemize}
		\item functional view: $\displaystyle y(\mathbf {x}, \mathbf w)=f(\mathbf w^T\phi(\mathbf x))$, where $\phi$ the basis function, $f(\cdot)$ the network \\
		$\Rightarrow$ learn $\phi$ to choose embedding (basis function) \\
		$\Rightarrow$ learn $\mathbf w$ to choose which feature / basis functions more useful
		\end{itemize}
	\item Solving Bias-Variance Trade-off
		\begin{itemize}
		\item easy complexity via depth, size
		$\Rightarrow$ reduce bias, without hurting variance by fitting to big data
		\item easy regularization via L$2$ ant etc.\\ 
		$\Rightarrow$ prevent high variance without hurting bias much in a deep/big net
		\end{itemize}
	\end{itemize}
\item Power of Depth
	\begin{itemize}
	\item Deep Representation
		\begin{itemize}
		\item multiple layers to choose \& combine useful information (creating new feature/basis) \\
		$\Rightarrow$ next layer use chosen/combined simple basis to build more complex one
		\item $\Rightarrow$ an hierarchy from low-level information to high-level information
		\end{itemize}
	\item Circuit Theory (Power of Combination)
		\begin{itemize}
		\item functions that can be compactly represented by a depth $k$ architecture might require an exponential number of computational nodes using a depth $k-1$ architecture \\
		(from the perspective of factorization)
		\item \textbf{Yet, start from the SHALLOW, e.g. logistic regression, before trying the deep} \\
		(Occam's scissor: prefer simplicity, as it generalizes better)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Challenge}
($n$ units in one hidden layer)
\begin{itemize}
\item Weight-space Symmetries
	\begin{itemize}
	\item Symmetries in Activation Function
		\begin{itemize}
		\item $\mathcal{O}(2^n)$, e.g. $\arctan(-x) = -\arctan(x) \Rightarrow$ changing signs of all input \& output has the same mapping (reduce effective data)
		\end{itemize}
	\item Positional Combination in One Layer
		\begin{itemize}
		\item $\mathcal{O}(n!)$ exchange unit with each other (together with their input output weights) $\Rightarrow$ mapping stay the same
		\end{itemize}
	\end{itemize}
	$\Rightarrow \mathcal O(n!2^n)$ overall weight-space symmetries
	\item High-Dimension Search Space
		\begin{itemize}
		\item Multiple Critical Points
			\begin{itemize}
			\item symmetries: at least $\mathcal O (n!2^n)$ critical points ($\nabla E(w) = 0$), where $E(w)$ is error function \\
			due to weight-space symmetries
			\item saddle points: both the bottom (in one dimension) and the top for another \\
			$\Rightarrow$ more likely to have functions being convey/convex in different dimensions \\
			(due to high-dimension weight space)
			\item local optima: less then saddle points in amount \\ 
			(due to high-dimension weight space e.g. usually $\ge 10^4$-D for modern deep nets)
			\end{itemize}
		\item Plateaus
			\begin{itemize}
			\item a large flat region where gradient $\rightarrow 0$ \\
			$\Rightarrow$ gradient descent slowly down the flat surface (before exiting)
			\item $\Rightarrow$ slow down gradient descent significantly
			\end{itemize}
		\item Expensive in Finding Critical Point
			\begin{itemize}
			\item expensive for even local optima with gradient decent
			\item as expensive as $\mathcal O(n^3)$ if using Laplace approximation
			\end{itemize}	
		\end{itemize}
\item Gradient Vanishing/Exploding
	\begin{itemize}
	\item Saturated Function
		\begin{itemize}
		\item sigmoid/tanh function: gradient $\rightarrow 0$ when input $\rightarrow \pm \infty$
		\end{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item with depth $L$, each activation (e.g. tanh) output $a^l < 1$ and weight $\mathbf w^l <1$ \\ 
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'<1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $<1$ \\
		$\Rightarrow$ gradient exponentially decayed in back-prop
		\item similarly, each activation (e.g. ReLU) output $a^l>1$ and weight $\mathbf w^l > 1$ \\
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'>1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $>1$ \\
		$\Rightarrow$ gradient exponentially augmented in back-prop
		\end{itemize}
	\item Possible Solutions
		\begin{itemize}
		\item initialization: \hyperref[DL_Init_Xavier]{Xavier Initialization}
		\item activation: \hyperref[DL_Act_ReLU]{ReLU}: for gradient vanishing
		\item structure: \hyperref[DL_Block_Res]{residual block}
		\end{itemize}	
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Operations \& Layers Structure}
\subsection{Operations in Network}
\subsubsection{Activations}
\begin{itemize}
\item Sigmoid $a=\sigma(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mapping to $(0,1)$, with $\sigma(0)=0.5$ \\
		$\Rightarrow$ naturally maps into prob
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item gradient vanishing: $\displaystyle \sigma(z)' = \sigma(z)(1-\sigma(z)) \Rightarrow \lim_{z\rightarrow \pm \infty} \sigma(z)' \rightarrow 0$ \\
		(as the gradient passed through (via chain rule) $=\frac{a}{z} \frac{z}{w}$)
		\item NOT zero-centering, but always maps input into positive regime \\
		$\Rightarrow$ for weights afterawrds, can have only all positive/negative gradient \\
		$\Rightarrow$ inefficient update
		\item expensive expotential ops
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item do NOT use in hidden layer, yet can be used for prediction (at last layer)
		\end{itemize}
	\end{itemize}

\item Tangent $a=\tanh(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item empirically, almost always better than sigmoid (in hidden layers)
		\item maps to $(-1,1)$, with $\tan(0)=0 \Rightarrow$ help centering data ($0$-mean) \\ $\Rightarrow$ make the learning of next layer easier
		\item zero-centering (v.s. sigmoid)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item still, gradient vanishing when $z\rightarrow \pm \infty$
		\end{itemize}
	\end{itemize}
\item Rectified Linear Unit (ReLU) $\max(0, z)$ \label{DL_Act_ReLU}
	\begin{itemize}
	\item Derivation: approximated by a stack of sigmoid
		\begin{itemize}
		\item 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate gradient vanishing: $\forall z>0, a=z \Rightarrow$ converge much faster
		\item efficient ops
		\end{itemize}
		$\Rightarrow$ the default choice!
	\item Cons
		\begin{itemize}
		\item undefined behavior at $x=0$ (actually, gradient becomes the sub-gradient)
		\item gradient totally vanished for $x<0$ \\
		$\Rightarrow$ dead units: weights learned/initialized to always output negatives \\ 
		(thus the unit always output $0$, due to ReLu)
		\item again, not zero-centering
		\end{itemize}		
	\end{itemize}

\item Leaky Relu $a=\max(\alpha z, z), \alpha \rightarrow 0^+$ (e.g. $\alpha=0.01$)
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate the gradient vanishing problem for $(-\infty, +\infty)$
		\item avoid dead units problem
		\end{itemize}
	\end{itemize}

\item Piecewise Linear Unit (PLU) $a=\max(\alpha(z+\beta)-\beta, \min(\alpha(z-\beta)+\beta, z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item hybrid of tanh \& ReLU: three linear pieces approximating tanh in a given range
		\item more expressive than ReLU: more nonlinear, better to fit smooth nonlinear function
		\item mitigate gradient vanishing problem: due to linearity
		\end{itemize}
	\item Cons
	\end{itemize}

\item Linear (Identity) Activation $a=z$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item used in regression to output real number $\in (-\infty, +\infty)$
		\item used in compression net
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item stacked units with linear activation $\Leftrightarrow$ single linear transformation
		\item logistic regression with linear activation in hidden layer is NO more expressive than logistic regression with no hidden layer \textbf{!}
		\end{itemize}
	\end{itemize}
	
\item Maxout Activation $a=\max (Wz+b, \text{axis=0})$
	\begin{itemize}
	\item Operation
		\begin{itemize}
		\item $a/z$ the col vector for input/output with dimension of $d_i/d_o$ \\
		($k$ a hyper parameter) \\
		$\Rightarrow W_{[k\times d_o\times d_i]}$ \& $b_{k\times d_o}$ \\
		$\Rightarrow W\cdot z + b \rightarrow [k,d_o]$
		\item finally, take element-wise max over $k$ candidate
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item multi-linear approximation \\
		$\Rightarrow$ use $k$ linear functions to approximate convex function \\
		(due to the max ops)
		\item i.e. can learn a piecewise linear, convex function with up to k pieces
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item more params \& hyper param
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item actually, append one more linear layer
		\item in conv ops: a channel-wise maxpool as the activation
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Normalization in Network}
\begin{itemize}
\item Batch Normalization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for an activation in hidden layer with input $z$, a batch with size $N_b$
		\item calculate the mean of current batch $\displaystyle \mu=\frac 1 {N_b} \sum_n z_n$, where $z_n$ for the $n^{th}$ example 
		\item calculate the deviation of current batch $\displaystyle \sigma = \sqrt{\frac 1 {N_b} \sum_n(z_n-\mu)^2}$
		\item normalize to be $z'_n = \frac {z_n-\mu}{\sigma}$
		\item allow model to recover/manipulate original distribution: $\hat z_n=\gamma z'_n + \beta$, \\
		where $\gamma, \beta$ being trainable (updated by optimizer using gradients)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item preferred to apply batch norm on $z$ (before activation), instead of after it \\
		$\Rightarrow$ to control the active regime of the activation
		\item for math stability, $z'_n=\frac {z_n-\mu}{\sigma+\epsilon}$, with $\epsilon\rightarrow 0+$
		\item (usually) with mini-batch, calculate the mean \& variance from only the mini-batch
		\item with batch norm, original bias $b$ in calculating $z=wx+b$ becomes pointless \\
		$\Rightarrow$ integrated into the $\beta$ in batch norm
		\item at test time ($1$ example a time): need an estimation for $\mu, \sigma$ \\
		$\Rightarrow$ exponentially weighted average over $\beta, \sigma$ in training time
		\end{itemize}
	\item Implementation - Conv
		\begin{itemize}
		\item one mean \& variance for an entire feature map (before activation) \\
		$\Rightarrow$ preserve relativity across spatial location
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item normalize the intermediate data to have $0$ mean, unit variance \\
		$\Rightarrow$ equip init network with desired properties \\
		($0$ mean \& same unit variance for all layers) \\
		$\Rightarrow$ easier to train
		\item remain the ability to transfer the data to have other mean \& variance \\
		(controlled by $\gamma,\beta$)
		\item control the distribution of data in hidden layer \\ 
		$\Rightarrow$ suppress the change of input data distribution for the layer after it \\
		$\Rightarrow$ increase robustness for later layers, against covariate shift \\ 
		(from both the weight update in early layers and the input data change)
		\item regularize the net by adding noise to the input data of hidden layer \\ 
		(noise are from computing mean/variance only on mini-batch) \\
		$\Rightarrow$ enforce robustness against noise, hence unintended slight regularization effect
		\item $\Rightarrow$ \underline{address gradient exploding \& vanishing} \\
		(as responses are controlled, thus backward gradients are also regularized accordingly)
		\end{itemize}
	\end{itemize}
\item Compare \\
\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/ops-norm compare".png}

\end{itemize}

\subsection{Operations on Network}
\subsubsection{Initialization}
\begin{itemize}
\item Random Initialization for Weights
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item weights initialized to a random variable in a small range e.g. $(-0.03, 0.03)$
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid symmetry problem: \\
		if identical initialization for weights $\Rightarrow$ units in same layer computing exactly same function \\
		$\Rightarrow$ get the same learning step propagated back \\
		$\Rightarrow$ then always compute exactly the same function (by induction)
		\item avoid gradient vanishing: especially for gradient of sigmoid/tanh activation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item NOT concern various nets: sampling in a fixed range may not work for all nets
		\item weights are all sampled from the same distribution with limited variance \\
		$\Rightarrow$ variance in deep layer output collapse into $0$ \\
		$\Rightarrow$ symmetric problem
		\item too small $W$: gradient vanishing, as multiplied by multiple small $W$ in backprop
		\item too large $W$ (large variance): gradient vanishing due to saturation (tanh, etc.)
		\end{itemize}
	\end{itemize}

\item Xavier Initialization for Weights \label{DL_Init_Xavier}
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item set $\forall l\in [1,L], \text{Var}(w^l) = \frac 1{n_l}$ for tanh, $\frac 2{n_l}$ for ReLU, \\
		where $n_l$ is the number of unit in layer $l$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item draw random variable $r\sim \mathcal N(0,1)$
		\item set each of $w^l=r\cdot \sqrt{\frac 2 {n_l}}$ for ReLU, $r\cdot \sqrt{\frac 2 {n_l}}$ for tanh \\ 
		or $r\cdot \sqrt{\frac 2 {n_{l-1}+n_l}}$ proposed by 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item theoretically justified to initialized weights to be around $\pm 1$ \\
		$\Rightarrow$ mitigate gradient vanishing \& exploding problem statistically 
		\item scale activation statistics correspondingly \\
		$\Rightarrow$ expect the output variance to be the same of input variance
		\item need to compensate for ReLu \\
		(as half of activation/gradients are mused)
		\end{itemize}
	\end{itemize}
\item Zero Initialization for Bias
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item default to use $0$ bias \\
		(can NOT used for weights as explained)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Regularization}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Complexity Constraint
		\begin{itemize}
		\item measure the model complexity (task dependent) \\
		$\Rightarrow$ penalize using too complex model (to fit the train set)
		\item trade-off between ability (complexity) \& generalization (simplicity)
		\end{itemize}
	\end{itemize}
\item $L2$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $\frac12 \lambda w^2$, also called "weight decay" \\
		(as in gradient decent, weight is multiplied by a $<1$ number due to L2 term)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item makes small number spread over weights (each weight has smaller effect)
		$\Rightarrow$ simpler network, less able to fit extreme curly decision boundary \\
		(better generalization)
		\end{itemize}
	\end{itemize}
	
\item $L1$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $+\lambda \abs{w}$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item encourage sparse weight i.e. very close to 0 \\
		$\Rightarrow$ end up using a sparse subset of layer input \\
		$\Rightarrow$ feature selection (recognize \& discard noisy input)
		\item default to $L2$ if not using explicit feature selection
		\end{itemize}
	\end{itemize}

\item Elastic Net regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $\lambda_1 \abs w + \lambda_2 w^2$ i.e. combine the L1 and L2 regularization
		\end{itemize}
	\end{itemize}

\item Maxnorm Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $\norm{w}_2 < \epsilon$ \\
		i.e. enforce an absolute upper bound on weights (for every neuron) \\
		$\Rightarrow$ clamp the weight to have $\norm{w}_2<\epsilon$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item bounded update to avoid exploding weight \\
		(even under improper learning rate)
		\end{itemize}
	\end{itemize}

\item Dropout Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each of selected units, set a drop probability \\
		i.e. for each forward/back-prop, nodes are "dropped" according to the probability \\
		$\Rightarrow$ for each time, a randomly reduced net is trained
		\end{itemize}
	\item Implementation: Inverted Dropout
		\begin{itemize}
		\item set a keep prob $k$ instead of drop prob, for a selected layer
		\item generate random numbers for all units \& turned into a boolean "keep" vector $\mathbf k$
		\item dropped activation $\mathbf d = \mathbf a \times \mathbf k$ (element-wise), \\
		 where $\mathbf a$ is original activation output vector from the layer
		\item $\Rightarrow$ activation becomes $0$ for dropped units in $\mathbf d$
		\item scaling up by dividing the keep prob: $\mathbf d / k$ \\
		$\Rightarrow$ so that expected output value of each activation remains the same
		\item test time: no dropout: no random output \& consider all robust features learned \\
		(randomness in training, mitigated by big data) \\
		yet, scale each activation by drop out rate $\Rightarrow$ match the expectation
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item can NOT rely on any one feature $\Rightarrow$ have to spread out weights \\
		$\Rightarrow$ results in shrinking the squared norm of weights (as $L2$)
		\item used on layers with enormous features as input (e.g. computer vision) \\
		$\Rightarrow$ reduce the chance of relying on small set of features
		\end{itemize}
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/regularization-dropout".jpeg}
	\end{figure}
	\item Cons
		\begin{itemize}
		\item training loss may have bigger glitch $\Rightarrow$ harder to debug \\
		(make sure loss decreasing before introduced dropout)
		\end{itemize}
	\end{itemize}

\item Max norm constraints
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $w^l_n$ of every neuron to satisfy (). Typical values of cc are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Early Stopping
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item stop the training at lowest validation loss (with training loss decreasing) \\
		$\Rightarrow$ at the start point of overfitting
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate both train \& val loss, saving models along the way \\
		$\Rightarrow$ use the model corresponding to the start of overfitting
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item at relatively early stage, weights are still relatively small \\ 
		(due to random initialization in $[0^-, 0^+]$)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item couples task of optimizing loss and task of not overfitting \\
		$\Rightarrow$ no longer one task at a time
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Optimization}
\begin{itemize}
\item Batch Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate on entire training set; then update weights
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item largest optimization every time
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item greedy optimizing
		\item slow \& memory demanding on large dataset
		\end{itemize}
	\end{itemize}

\item Stochastic Gradient Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item shuffle data to have training set $X_\text{train}$, further split into $X_\text{train}^{1}, ..., X_\text{train}^{T}$
		\item train the net iteratively with $\forall t\in[1,T], X_\text{train}^t$ \\
		i.e. one mini-batch for a gradient descent (weights update)
		\item after training through all $T$ batches, an epoch of training is finished \\
		$\Rightarrow 1$ epoch = $1$ full scan of training set
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item faster: more weight upgrade over the same amount of data
		\item better chance to reach global change: not greedy anymore
		\item more affordable for training in GPU memory
		\end{itemize}
		$\Rightarrow$ preferred choice
	\item Cons
		\begin{itemize}
		\item observing noisy loss: not monotonically decreasing (but overall decreasing) \\
		$\Rightarrow$ slow converge
		\item saddle point \& plateau region
		\end{itemize}
	\end{itemize}
	
\item Gradient Descent with Momentum
	\begin{itemize}
	\item Definition: exponentially weighted average
		\begin{itemize}
		\item calculate the gradient for weight update: $dW'_t = \beta dW'_{t-1} + (1-\beta) dW_t$, \\ 
		where $dW$ the original gradient \\
		$\Rightarrow$ update with $dW't$: momentum $dW'_{t-1}$ + current gradient $d W_t$
		\item i.e. average over past gradients with exponentially decaying weight, \\
		$\Rightarrow$ for past $k\in[0,K]$ gradient, coefficient becomes $\beta(1-\beta)^k$ \\ 
		(with $k=0$ denoting current gradient)
		\item bias correction: avoid slow start \\
		(due to: gradient $dW_0$ initialized to $0$ \& not enough gradients for averaging) \\
		$\Rightarrow$ set $dW_t=\frac {dW_t}{1-\beta^t}$ in the early stage \\
		(after starting stage, bias correction $\rightarrow 0$ for large $t$)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item approximation: weighted average over past $K=\frac 1 {1-\beta}$ gradients \\
		due to $(1-\epsilon)^{1/\epsilon} \approx \frac 1 e$, recognized as small enough \\
		$\Rightarrow$ discard gradients with further exponentially small weights
		\item apply element-wise multiplication on gradients and pre-calculated coefficient
		\item sum up to be the gradient for weight update \\ 
		(include bias correction term if necessary, yet often omitted)
		\item note: $dW'_t = \beta dW'_{t-1}+dW_t$ is another version, yet discouraged \\
		(coupling momentum $\beta$ with learning rate $\alpha$, as $\alpha$ needs to cooperate)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item use momentum $dW'$ to update
		\item averaging/smoothing out the regular oscillation in stochastic gradient descent \\
		$\Rightarrow \beta$ popularly chosen to be $0.9$ (averaging over last $10$ gradients) \\
		$\Rightarrow$ faster training \& hopefully overcome saddle and local minima
		\end{itemize}
	\end{itemize}

\item Nesterov Momentum
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item gradient for update $dW'_{t} = \beta dW'_{t-1} + (1-\beta)d(W_{t-1}-\beta dW'_{t-1})$, \\
		where $d(W_{t-1}-\beta dW'_{t-1})$ the gradient of network $W_{t-1}-\beta dW'_{t-1}$ \\
		i.e. update the network using purely last momentum \& calc the gradient of that \\
		$\Rightarrow$ adjust the current momentum
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item look ahead: account for momentum \& correct the gradient
		\end{itemize}
	\end{itemize}

\item Root Mean Square Propagation (RMS prop)
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $S_t = \beta S_{t-1} + (1-\beta) dW_t^2$ ($S_0$ initialized to $0$),\\ 
		where $dW^2$ the original gradient being element-wisely squared\\
		$\Rightarrow$ exponentially weighted square of gradients
		\item calculate the gradient for weight update $dW'_t=\frac {dW_t}{\sqrt{S_t}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item calculate $S_t$ similarly (as an exponentially weighted average)
		\item $\sqrt{S_t}$ becomes $\sqrt{S_t+\epsilon}$, where $\epsilon\rightarrow 0^+$ for mathematical stability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item for gradients with large variance in training $\Rightarrow S_t$ large $\Rightarrow \frac 1{\sqrt{S_t}}$ small \\ 
		$\Rightarrow$ weighted less, hence stabilized (as it should be noisy \& taking smaller step)
		\item for gradients with small variance \\ 
		$\Rightarrow$ weighted more, encouraged (as it should be on the "trend" towards optimum)
		\item modulate the learning rate for each weight based on its gradient magnitude \\
		$\Rightarrow$ equalizing the update scale
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item recognize trend from noise via variance of their gradient $\Rightarrow$ speedup training
		\item auto-fixing learning rate for each weight given the recorded behavior \\ 
		(protect learning process from a too large learning rate)
		\item align the pace of optimization along each axis (normalized by sqrt)
		\end{itemize}
	\end{itemize}
	
\item Adaptive Momentum (Adam) Optimization Optimization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $M_t = \beta_1 M_{t-1} + (1-\beta_1) M_t$ as momentum
		\item compute $S_t = \beta_2 S_{t-1} + (1-\beta_2) dW_t^2$ as root mean square
		\item apply bias correction on both: $M'_t=\frac {M_t}{1-\beta_1^t}, S'_t=\frac {S_t}{1-\beta_2^t}$
		\item $\Rightarrow$ calculate gradient for update $dW'_t=\frac {M'_t} {\sqrt{S'_t+\epsilon}}$, where $\epsilon\rightarrow 0^+$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item implement $M_t,S_t$ as momentum and root mean square \\
		(popular choice: $\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}$)
		\item do implement bias correction
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item combine momentum with root mean square \\
		$\Rightarrow$ for each weight
			\begin{itemize}
			\item smooth out regular oscillation
			\item encourage the trend \& adapt learning rate given history record \\
			(equalizing the update scale)
			\end{itemize}
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item effective for a large range of problem
		\end{itemize}
	\end{itemize}

\item Learning Rate Decay
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item update learning rate $\alpha = \frac 1 {1+r\cdot e}$, where $r$ the decay rate, $e$ the epoch number
		\item other decay formula:
			\begin{itemize}
			\item exponential decay: $\alpha=r^e\cdot\alpha_0$, where $\alpha_0$ the base learning rate
			\item $\alpha=\frac k {\sqrt{e}} * \alpha_0$, where $k$ a constant
			\end{itemize}
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item set learning rate for each epoch, or after some global steps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fast learning at the beginning, more cautious when approaching the optimum \\ 
		$\Rightarrow$ in order to finally converge
		\item used mostly with RMS/momentum; usually not with Adam \\
		(should consider search ing good learning rate first)
		\end{itemize}
	\end{itemize}
	
\item Second-Order Optimization (Vanilla)
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item second-order Taylor expansion $J(W) \approx J(W_0) + (W-W_0)^T \nabla J(W_0) + \frac12 (W-W_0)^TH(W-W_0)$, \\
		where $H$ the Hessian matrix \\
		$\Rightarrow$ update as $W_{t}=W_{t-1}-H^{-1}J(W_{t-1})$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item no learning rate to tune
		\item computing $H$ is $\mathcal O(n^2)$; $H^{-1}$ is $\mathcal O(n^3)$
		\end{itemize}
	\end{itemize}
\item Quasi-Newton (BFGS)
\item Limited-memory BFGS (L-BFGS)
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item NOT account for stochastic approach (mini-batch) \\
		$\Rightarrow$ require full-batch
		\item NOT good on non-convex problem
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Loss}
\subsubsection{Probabilistic Loss}
\begin{itemize}
\item Log Maximum Likelihood / Posterior
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item convert the logits into probability-alike prediction \\ 
		$\Rightarrow$ then interpreted as predicted likelihood $p(\mathbf y|\mathbf w, \mathbf x)$
		\item bayesian regression $\displaystyle L = -\frac 1 2\sum_{\mathbf y\in \mathbf Y} (\mathbf y - \hat {\mathbf y})^2$ \\
		(for $\mathbf y$ real number vector label, $\hat {\mathbf y}$ real number vector prediction)
		\item classification with logistic assumption $\displaystyle L = -\sum_{\mathbf y\in\mathbf Y}(\mathbf y^T \cdot \log \hat {\mathbf y})$ \\
		($t$ one-hot encoded label, $\hat y$ one-hot encoded prediction)
		\item to use posterior with Gaussian distribution: add $L2$ regularization term
		\end{itemize}
	\end{itemize}

\item Smooth L1 Loss
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsubsection{Metric Loss}
\begin{itemize}
\item Triplet Loss
\item Soft Label
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item contain more information than hard label \\
		$\Rightarrow$ involve similarity info: tips for generalization \\
		$\Rightarrow$ enable distillation
		\item as regularization
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item train net specialized on one sub-task \\
		(e.g. recognize only one class)
		\item ensemble all specialized net to create soft label
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Layers}

\subsubsection{Prediction}
\begin{itemize}
\item Sigmoid
z`
\item Softmax
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item arbitrary input $\mathbf z^{L}$ being logits, containing multiple multi-class predictions $z^{L}$ \\
		$\Rightarrow$ each prediction being the same dimension as one-hot encoded label
		\end{itemize}
	\item Output
		\begin{itemize}
		\item probabilistic-alike prediction $\mathbf a^L$, with the same shape as the input (logits)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item for $K$ classes to predict $\Rightarrow$ $\dim (z^L)=K$
		\item for each dimension $k\in[1,K]$, compute $\displaystyle a^L_k=\frac{e^{(z^L_k)}}{\displaystyle \sum_{k=1}^K e^{(z^L_k)}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item vecotrize the exponential computation $\hat z^L = \exp{(z^L)}$
		\item compute normalization $N=\displaystyle \sum_{k=1}^K {\hat z^L_k}$
		\item normalize as $a^L=\frac 1N \hat z^L$
		\item maximum likelihood with softmax: $\displaystyle L = \frac 1N \sum_{\mathbf Y}-\mathbf y^T \cdot \log \hat {\mathbf y}$, \\
		where $\mathbf y$ the one-hot encoded label, $\hat {\mathbf y}$ the prediction \\
		$\Rightarrow$ easy gradients: $dz^L = \hat{\mathbf y} - \mathbf y$, where $z^L$ the logits (vector)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item contrasting the hard-max function (non differentiable): $a_k = 1 \text{ if } \displaystyle \arg\max_k(z); \text{ else } 0$
		\item exponentially normalizing the output of arbitrary net into probabilistic form \\
		(reduced to logistic for binary class i.e. $K=2$) \\
		$\Rightarrow$ generalize logistic prediction to $K$-class prediction
		\item for maximum likelihood loss, only the gap with true class generate gradients \\ 
		(due to one-hot encoding) \\ 
		$\Rightarrow$ trying to predict the class true with higher probability
		\item always make the correct pred prob to be larger \\
		$\Rightarrow$ push the logits for correct/wrong pred to $+/-\infty$ \\
		(compared with hinge loss)
		\end{itemize}
	\end{itemize}

\item Hierarchical Softmax \label{DL_Layers_Hisoftmax}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item arbitrary input $\mathbf z^{L}$ being logits, considered as a flatten tree
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a tree structure, flatten into $1$-D array
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item for each node (if not leaf), a softmax to predict prob for its direct children \\
		$\Rightarrow$ multiple softmax connected to various input nodes in $\mathbf z^L$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item create a Bayesian network with a single root node (e.g. "obj") \\
		$\Rightarrow$ each node being a conditional probability conditioned on its direct parent
		\item for absolute probability of each class (represented by a node) \\ 
		$\Rightarrow$ apply sum rule \& product rule accordingly
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item NOT assuming mutual exclusion between class
		\item $\Rightarrow$ enable graceful degrade in classification \\
		e.g. can still recognize by $p(\text{animal})$, if failed with $p(\text{cat}), p(\text{dog})$, etc.
		\item enable joint training with multiple datasets (involving classification) \\
		e.g. \hyperref[DL_CV_Objdet_YOLOv2]{YOLOv2(YOLO9000)}
		\end{itemize}
	\end{itemize}

\item Normalization
\end{itemize}

\subsubsection{Convolution Layer}
\begin{itemize}
\item Convolution $1D$
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item $1D$ vector of size $i$, can have multiple channels at each location
		\end{itemize}
	\item Setting
		\begin{itemize}
		\item kernel size $k$: another 1D vector (weight vec)
		\item padding $p$: the size to pad at one direction of an axis \\
		(usually symmetric padding at both direction $\Rightarrow2p$ in total)
		\item stride $s$: the step for kernel to move its location as sliding across input
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item $\displaystyle s(t) = (f * g)(t) = \sum_{x=-\infty}^\infty f(x)g(t-x)$
		\item padding: to extend the $x$ range where $f(x)$ is defined, by filling default values
		\item strides: the step / gap between 2 consecutive kernel locations
		\item flip the kernel vector $g(x)$ \& slides it across input vector $f(x)$ with stride $s$
		\item for each position $t$: \\
		an element-wise weighted sum on the spatially corresponding position \\
		(overlap = range where $f(x), g(t-x)$ are both defined)
		\item sum across channels: sum over kernel output from each channel $+$ optional bias
		\item kernel strides spatially across the image, with stride along each axis defined \\
		$\Rightarrow$ to produce a $\mathbf 1$-channel output \\
		$\Rightarrow$ for multi-channels output: multiple sets of kernels
		\end{itemize}
	\item Output
		\begin{itemize}
		\item output size $o$ = possible placements of kernel on input \\
		(channel depends on the num of sets of kernels) \\
		$\displaystyle \Rightarrow o = \left\lfloor\frac {i+2p-k} {s} \right\rfloor + 1$
		\item for $s>1$, all input size $\{j=i+a | a = 0,...,s-1\}$ will produce same output size \\
		(complicate the analysis of transposed conv)
		\end{itemize}
	\item Implementation - Img-to-Col
		\begin{itemize}
		\item pre-calc the function $g(t-x)$ for all possible $t$ (as output size known) \\
		$\Rightarrow$ fill in the table that corresponds to input size: \\
		\begin{minipage}[r]{.3\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/layer-conv1d conv impl".png} \\
		\end{minipage}
		\begin{minipage}[l]{.4\linewidth}
		= \resizebox{\hsize}{!}{$\begin{pmatrix}
		w_1 & w_2 & w_3 & w_4 & 0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   \\
		0   & 0   & w_1 & w_2 & w_3 & w_4 & 0   & 0   & 0   & 0   & 0   & 0   \\
		0   & 0   & 0   & 0   & w_1 & w_2 & w_3 & w_4 & 0   & 0   & 0   & 0   \\
		0   & 0   & 0   & 0   & 0   & 0   & w_1 & w_2 & w_3 & w_4 & 0   & 0   \\
		0   & 0   & 0   & 0   & 0   & 0   & 0   & 0   & w_1 & w_2 & w_3 & w_4 \\
		\end{pmatrix}$}
		\end{minipage} \\
		where $\mathbf x = $ input $f(x)$ after padding (gray cells), $\mathbf y = s(t)$ the output, \\
		with $i=8, k=4, p=2, s=2$, thus $o=5$
		\item $\Rightarrow$ each row of the matrix $=$ kernel $g(t-x)$ with a given $t$ \\
		i.e. each row represents kernel at a specific position
			\begin{itemize}
			\item row num = possible kernel position = output size
			\item col num = input size (after padding)
			\end{itemize}
		\end{itemize}
	\item Back Propagation (TODO: study matrix derivatives)
		\begin{itemize}
		\item with above matrix (denoted as $C$): $\mathbf y = C \mathbf x$
		\item to update kernel weights: $\frac{\partial}{\partial C} L = \frac{\partial L}{\partial\mathbf y} \frac{\partial\mathbf y}{\partial C} =\frac{\partial L}{\partial\mathbf y} \cdot \mathbf x^T$\\
		as $\frac{\partial\mathbf y}{\partial C} =$\resizebox{0.1\hsize}{!}{$\begin{bmatrix} \text{---}&\mathbf x^T&\text{---} \\ &\vdots& \\ \text{---}&\mathbf x^T&\text{---} \end{bmatrix}$} \&
		$\frac{\partial L}{\partial C} =$\resizebox{0.12\hsize}{!}{$\begin{bmatrix} \text{---}& \frac{\partial L}{\partial\mathbf y}[1]\cdot\mathbf x^T&\text{---} \\ &\vdots& \\ \text{---}&\frac{\partial L}{\partial\mathbf y}[5]\cdot\mathbf x^T&\text{---} \end{bmatrix}$} \\
		$\Rightarrow$ equivalent to $\frac{\partial L}{\partial\mathbf y} \cdot \mathbf x^T$, as $\frac{\partial L}{\partial\mathbf y}$ a col vec as $\mathbf y$ \\
		(still, needs to sum up grad for the same weights)
		\item to backprop: $\frac{\partial}{\partial \mathbf x} L = \frac{\partial L}{\partial \mathbf y} \frac{\partial \mathbf y}{\partial \mathbf x} = C^T \cdot \frac{\partial L}{\partial \mathbf y}$, as $\frac{\partial \mathbf y_i}{\partial \mathbf x_j} = C_{i,j}$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item the basis for N-D conv: as the relations are analogy across axises
		\item kernel defines both forward \& backward pass \\
		$\Rightarrow$ depending on how to use the $C$ and $C^T$
		\end{itemize}
	\end{itemize}
\item Convolution $2D$
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item spatially $2D$ feature maps, usually with multiple channels
		\end{itemize}
	\item Setting
		\begin{itemize}
		\item kernel size $k$: a 2D matrix (weights) to be convoluted \\
		$\Rightarrow$ usually odd square matrix - even becomes a convention \\
		(to avoid asymmetric padding \& a central pixel for filter location) \\
		$\Rightarrow k\times k$ kernel
		\item padding $p$: as in 1D conv (usually, same $p$ for all axises)
		\item stride $s$: as in 1D conv (usually, same $s$ for all axises)
		\end{itemize}
	\item Operation (similar to 1D conv)
		\begin{itemize}
		\item $\displaystyle S(i,j)= (I*K)(i,j) = \sum_m\sum_n I(m,n)K(i-m, j-n)$ \\
		$\Rightarrow$ slide along both axises \& produce a 2D function \\ 
		(hence 2D conv)
		\item kernel (weights) $K$ structured as a matrix (for each input channel)
		\item slide \& sum on 2D spaces \\
		\begin{minipage}[r]{.7\linewidth}
		\includegraphics[width=.75\linewidth, center]{"./Deep Learning/plot/layer-conv2d exp".png}
		\end{minipage}
		\begin{minipage}[l]{.3\linewidth}
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/layer-conv2d kernel movement".png}
		\end{minipage}
		\item each kernel a input channel \& a set of channel for 1-channel output \\
		$\Rightarrow$ multiple sets of kernels for multi-channel output
		\item activation then taken after convolution operation, element-wisely
		\end{itemize}
	\item Output
		\begin{itemize}
		\item as in 1D conv: $\displaystyle o = \left\lfloor\frac {i+2p-k} {s} \right\rfloor + 1$ \\
		for $s>1$, all input size $\{j=i+a | a = 0,...,s-1\}$ will produce same output size 
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item implement cross-correlation instead of convolution \\
		(skip the flipping operation: as their results are symmetric) \\
		yet, convolution is associative, due to the flipping
		\item create a matrix with: each row = unrolled kernel at a specific location \\
		(similar to 1D conv) \\
		\begin{minipage}[l]{.15\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/layer-conv2d basic".png} \\
		\end{minipage}
		\begin{minipage}[l]{.85\linewidth}
		with $i=4, k=3, p=s=0$, construct matrix: \\ \\
		\resizebox{\hsize}{!}{$\begin{pmatrix}
w_{0,0} & w_{0,1} & w_{0,2} & 0       & w_{1,0} & w_{1,1} & w_{1,2} & 0       & w_{2,0} & w_{2,1} & w_{2,2} & 0       & 0       & 0       & 0       & 0       \\
0       & w_{0,0} & w_{0,1} & w_{0,2} & 0       & w_{1,0} & w_{1,1} & w_{1,2} & 0       & w_{2,0} & w_{2,1} & w_{2,2} & 0       & 0       & 0       & 0       \\
0       & 0       & 0       & 0       & w_{0,0} & w_{0,1} & w_{0,2} & 0       & w_{1,0} & w_{1,1} & w_{1,2} & 0       & w_{2,0} & w_{2,1} & w_{2,2} & 0       \\
0       & 0       & 0       & 0       & 0       & w_{0,0} & w_{0,1} & w_{0,2} & 0       & w_{1,0} & w_{1,1} & w_{1,2} & 0       & w_{2,0} & w_{2,1} & w_{2,2} \\
		\end{pmatrix}$}
		\end{minipage} \\
		\end{itemize}
	\item Back Propagation
		\begin{itemize}
		\item similar to 1D conv above
		\end{itemize}
	\item Understanding - Padding
		\begin{itemize}
		\item prevent output feature maps from spatially shrinking
		\item prevent info lost on the border \& corner of image \\
		(compared to the central part of feature map, multiplied less with the kernel) \\
		$\Rightarrow$ ensure border feature get convoluted by the same times as central feature
		\item YET, may introduce noise if using large kernel \\
		(as need to pad a lot \& introduce overwhelming useless info, e.g. $0$s)
		\item can also introduce location info to network \\
		(no more translation invariance after several padded conv layer) \\
		$\Rightarrow$ perform localization tasks
		\item convention: $0$-padding on both directions of an axis
		\item padding style:
			\begin{itemize}
			\item valid (no) padding: $p=0$
			\item half (same) padding for odd kernal $k=2n+1$ and $s=1$ \\
			$\Rightarrow p=n$ to have $o == i$
			\item full padding: $p=k-1$ to account all possible overlaps of kernel \& feature map
			\end{itemize}
		\end{itemize}
	\item Understanding - Kernel
		\begin{itemize}
		\item pattern matching on input as a linear classifier (activation afterwards) \\
		$\Rightarrow$ learn a spatially shared pattern
		\item each kernel/filter = a set of single-channel filter producing 1 output channel
		\item 3-layer 3x3 conv = 1-layer 7x7 conv regarding receptive fields \\
		(but less params, more non-linearity, more available pattern combination)
		\end{itemize}
	\item Understanding - Stride
		\begin{itemize}
		\item a form of subsampling (how much output is retained) \\
		$\Rightarrow$ subsample every 1 in $s$ locations from the stride-1 feature map along each axis
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item a linear transformation that preserves topological info \& ordering
		\item v.s. hand-designed filter: learn \& optimize the goal with the help of data statistics
		\item weights in the $l^\text{th}$ conv layer: $n_c^{l-1} \times k\times k \times n_c^{l}$, where $n_c$ the channel number \\
		(to output $n_c^l$ channels with $n_c^{l-1}$ input channels from previous layer) \\
		$\Rightarrow$ invariant to the input size (number of trainable variables fixed on design) \\
		$\Rightarrow$ less weights (then dense layer), more generalizability, hence less overfitting
		\item sharing weights spatially: apply same weights over the whole space \\
		$\Rightarrow$ NO need for special design at each location \\
		$\Rightarrow$ as need to handle spatial variance in processing images
		\item sparse connection: output connected only to the local input \\ 
		$\Rightarrow$ robust to spatial variance (as a high-pass bandwidth)
		\end{itemize}	
	\end{itemize}

\item $1\times1$ Convolution
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item multi-dimension feature maps with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item integrate channels at each spatial location together \\
		(a weighted sum with bias, as conv definition)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature maps with same dimensions, but different channels
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item shrink the number of channels
		\item add more non-linearity \& info combination (more representability)
		\item channel-wise interaction
		\end{itemize}
	\end{itemize}

\item Transposed Convolution
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature vec/map/volume, etc.
		\end{itemize}
	\item Setting
		\begin{itemize}
		\item consider only one axis \\
		(can generalize to N-D input/kernel with all axises sharing the same setting)
		\item original conv: $i, a, k, s, p, o$, where $i +2p - k \mod s = 0$ \& $a={0,...,s-1}$ \\
		(to unleash the constraint from $\floor{}$ ops)
		\item input size $i'$, output size $o'$
		\item kernel size $k'$, stride $s'$, padding at one direction $p'$
		\item $f$ the num of location to be inserted into 2 consecutive input location
		\item $a'$ the num of padding to add to $\mathbf 1$ side of the axis (additional padding)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given a direct conv with setting $i, k, s, p, o$ \\
		$\Rightarrow$ its transposed conv is the ops to have $i'=o$ and $o'=i$ \\
		i.e. to recover the original input shape \& maintain the correspondence
		\item assume $s'=1, k'=k$, then set $p'+p=k-1, a'=a, f=s-1$
		\item conv as usual on output $o=i'$ (after manipulated by $p', a', f$)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature with size of $o'=i$
		\end{itemize}
	\item Derivation - Mathematically
		\begin{itemize}
		\item assume $s'=s=1, k'=k$ \\
		$\Rightarrow \left\{\begin{alignedat}{4}
		&\floor{\frac{i+2p-k}{s}}+1 &&= i+2p-k+1 &&= o && \phantom{--}\text{ direct conv} \\
		&\floor{\frac{o+2p'-k}{s}}+1 &&= o+2p'-k+1 &&= i && \phantom{--}\text{ transposed conv}
		\end{alignedat}\right. $ \\
		$\Rightarrow p+p'=k-1$ \\
		thus, under special case of $s=1$:
			\begin{itemize}
			\item zero-full padding are transposed conv for each other
			\item same padding is transposed conv for itself (as shape does NOT change at all)
			\end{itemize}
		\item more general case: assume $s'=1, k'=k$ \\
		$\Rightarrow \left\{\begin{alignedat}{4}
		&\floor{\frac{i+a+2p-k}{s}}+1 &&= \frac{i+2p-k}{s}+1 &&= o && \phantom{-}\text{ direct conv} \\
		&\floor{\frac{o+2p'+a'+f(o-1)-k}{s'}}+1 &&= o+a'+2p'+f(o-1)-k+1 &&=i+a && \phantom{-}\text{ transposed conv}
		\end{alignedat}\right. $ \\
		$\displaystyle \Rightarrow \text{given }\begin{cases}
		2p'+ a'+f(o-1)=a+2k-2-2p+(s-1)(o-1) \\ 
		0 \le a \le s-1 \\
		0 \le p \le k-1
		\end{cases} \\
		\phantom{\Rightarrow} \text{ find } p',a',f \\
		\begin{alignedat}{3}
		& \phantom{\Rightarrow} \text{ s.t.} \phantom{-} && 0 \le p' &&\le k-1 \text{ to have valid overlap with kernel of transposed conv} \\
		& &&0 \le a' &&\le k-1 \\
		& &&0 \le p'+a' &&\le k-1 \\
		& &&0 \le f &&\le k-1
		\end{alignedat}$ \\
		$\Rightarrow$ one intuitive solution is thus: $p+p'=k-1, a'=a, f=s-1$
		\end{itemize}
	\item Derivation - Intuitively
		\begin{itemize}
		\item to maintain the correspondence of the original direct conv (in the opposite way):
		\item recover the original output $o_{s=1}=i+a+2p-k+1$
			\begin{itemize}
			\item stride $s$ as subsampling on $o_{s=1} \Rightarrow f=s-1$
			\item $a\le s-1$ as skipped input at the tail of an axis/dim of $o_{s=1} \Rightarrow a'=a$
			\end{itemize}
		(besides, $o_{s=1}=o+(s-1)(o-1)+a$)
		\item $\Rightarrow$ then, could have $s=1$ assumption
		\item $p$ as additional border info/mapping that is not required to account \\ 
		$\Rightarrow$ the border output (e.g. $o[0]=i'[0]$) corresponds to original $k-p$ input \\
		(while the border location recovers $\floor{\frac{p'+1}{s'}}=p'+1$ location via trans conv) \\
		$\Rightarrow$ $p'+1=k-p$, i.e. $p'+p=k-1$
		\end{itemize}
	\item Implementation - Conv Backward
		\begin{itemize}
		\item using the backward of a conv: transposed conv via $C^T$, \\
		where $C$ the constructed matrix
		\item given direct conv, where $x=i, y=o$ \\
		$\Rightarrow$ have its backward \& transposed conv with $x'=y, y'=x$ (in shape \& location) \\
		\begin{minipage}[l]{.3\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/layer-conv1d conv impl".png} \\
		\centering
		forward pass
		\end{minipage}
		\begin{minipage}[r]{.2\linewidth}
		\includegraphics[width=\linewidth, right]{"./Deep Learning/plot/layer-conv1d backward".png} \\
		\centering
		backward pass
		\end{minipage}
		\begin{minipage}[r]{.45\linewidth}
		\includegraphics[width=\linewidth, right]{"./Deep Learning/plot/layer-conv trans conv as fractionally strided conv".png} \\
		\centering
		transposed conv
		\end{minipage} \\
		$\begin{alignedat}{1}
		\text{where }&\text{gray in } x^T \text{ the padding,} \\
		&\text{gray in } y' \text{ the places to be cropped,} \\ 
		&\text{gray in } x'^T \text{ the compensated location \& padding} 
		\end{alignedat}$ \\
		\item $\Rightarrow$ each transposed conv equivalent to a conv backward \\
		(after a kernel re-arrange, actually a flip) \\
		$\Rightarrow$ \underline{implement as swapping back/forward-prop of normal conv} \\
		(as kernel is learnable)
		\end{itemize}
	\item Implementation - Conv Forward
		\begin{itemize}
		\item $r^d$-channel normal conv to perform single-channel transposed conv \\
		where $r$ the upsampling rate, $d$ the data dimension (2 for img) \\
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-conv trans conv as normal conv".png} \\ 
			\begin{itemize}
			\item weights are divided into classes by color i.e. yellow, purple, gree, blue
			\item weights within each class are activated at the same time
			\item weights between classes will NOT activated at the same time $\Rightarrow$ independent
			\end{itemize}
		\item able to restruct independent weights into stack of kernels \\
		$\Rightarrow$ obtain the same result by reconstruct the output feature map from normal conv
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item a learnable version of inverse conv \\
		$\Rightarrow$ to \underline{maintain the correspondence} between original input \& output \\
		$\Rightarrow$ also, to recover the shape of the original input of the direct conv \\
		(yet, not recovers the content, but learning a new mapping)
		\item can be viewed as scaling kernel on each output position by the weight, \\
		where output position = kernel position (region) in the original conv \\
		(sum on the overlapped region) \\
		$\Rightarrow$ inverse \& maintain the original correspondence
		\item checkerboard effect due to the overlapped kernel \\
		$\Rightarrow$ use trans conv corresponding to original non-overlap conv
		\end{itemize}
	\end{itemize}

\item Dilated/Atrous Convolution
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item 2D feature map
		\end{itemize}
	\item Setting
		\begin{itemize}
		\item size: input $i$, output $o$, kernel $k$, padding $p$, stride $s$
		\item input skipped after the last conv at each axis: $a=\{0,...,s-1\}$
		\item dilation rate $d$
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item insert $d-1$ spaces between two consecutive kernel elements \\
		$\Rightarrow$ actual kernel size become $k+(k-1)(d-1)$
		\item convolve with dilated conv kernel
		\end{itemize}
	\item Output
		\begin{itemize}
		\item $o=\floor{\frac{i+a+2p-k-(k-1)(d-1)}{s}}+1$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item subsampling the kernel \\
		$\Rightarrow$ increase receptive field without increasing learnable params
		\item whereas, deep net usually do NOT have enough receptive field for large obj on large img
		\end{itemize}
	\end{itemize}

\item Deformable Convolution \label{DL_Layers_DeformableConv}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature map
		\end{itemize}
	\item Setting
		\begin{itemize}
		\item $\mathbf w$ the kernel of a 2D conv
		\item $\mathcal R$ the offset of each kernel params to the conv center \\
		e.g. $\mathcal R=\{(-1,-1), (-1,0), ..., (1,1))\}$ for 3x3 conv
		\item $N=\abs{\mathcal R}$ the number of conv kernel params
		\item $\mathbf p_0$ the position of a conv:
			\begin{itemize}
			\item $\mathbf x(\mathbf p_0)$ the feature at conv center on input feature map
			\item $\mathbf y(\mathbf p_0)$ the resulting/corresponding feature on output map
			\end{itemize}
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item original conv: $\mathbf y(\mathbf p_0)=\sum_{\mathbf p_0\in\mathcal R} \mathbf w(\mathbf p_n)\cdot \mathbf x(\mathbf p_0+\mathbf p_n)$ \\
		(single channel)
		\item another same-setting conv kernel to produce offset field \\
		(same-size output feature map, but of $2N$ channels) \\
		$\Rightarrow N$ groups of x-y offset at each location \\
		$\Rightarrow$ each $\mathbf y(p_0)$ has a corresponding offset prediction $\mathbf y_p(\mathbf p_0)$
		\item deformable conv: $\mathbf y(\mathbf p_0)=\sum_{\mathbf p_0\in\mathcal R} \mathbf w(\mathbf p_n)\cdot \mathbf x(\mathbf p_0+\mathbf p_n+\Delta \mathbf p_n)$, where $\Delta \mathbf p_n \text{ the } n^\text{th} \text{ x-y offset in } \mathbf y_p(\mathbf p_0)$ \\
		(still, single channel) \\
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-conv deformable conv".png} \\
		\item multi-channel input: different offset for conv on each location of each channel
		\item bilinear interpolation to obtain $\mathbf x(\mathbf p_0+\mathbf p_n+\Delta \mathbf p_n)$ as $\Delta \mathbf p_n$ predicted by network
		\end{itemize}
	\item Back Propagation
		\begin{itemize}
		\item as $G(\mathbf q, \mathbf p)=g(q_x, p_x)\cdot g(q_y,p_y)$, with $g(a,b)=\max(0,1-\abs{a-b})$ \\
		$\Rightarrow$ $G$ the bilinear interpolation to have $\mathbf x(\mathbf p)=\sum_{\mathbf q} G(\mathbf q, \mathbf p)\cdot \mathbf x(\mathbf q)$
		\Item \begin{align*} \frac{\partial}{\partial \mathbf \Delta \mathbf p_n}\mathbf y(\mathbf p_0) &= \sum_{\mathbf p_n\in\mathcal R} \mathbf w(\mathbf p_n)\cdot \frac{\partial \mathbf x(\mathbf p_0+\mathbf p_n+\Delta\mathbf p_n)}{\partial \Delta \mathbf p_n} \\
		&= \sum_{\mathbf p_n\in\mathcal R} \left[ \mathbf w(\mathbf p_n)\cdot \sum_{\mathbf q}\frac{\partial G(\mathbf q, \mathbf p_0+\mathbf p_n+\Delta\mathbf p_n)}{\partial\Delta p_n} \mathbf x (\mathbf q)\right] \end{align*}
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item loosen the assumption of conv: learning the sample location for each conv \\
		(conv assume strict local correspondence prior) \\
		$\Rightarrow$ more effective receptive fields \\ 
		(as bbox-size receptive fields can contain lots of background) \\
		\begin{minipage}[l]{.5\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/layer-conv deformable conv vs normal conv".png}		
		\end{minipage}
		\begin{minipage}[r]{.4\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/layer-conv deformable conv on cat".png} \\		
		\resizebox{\linewidth}{!}{\begin{tabular}{@{}l@{}}
		red box: receptive field from deformable conv \\
  		(as it is preceded by normal conv)\end{tabular}}
		\end{minipage} \\
		$\Rightarrow$ better sampling on non-rigid object
		\item \underline{attention} by learning sampling location:
			\begin{itemize}
			\item conditioned on the input $\Rightarrow$ dynamically determination
			\item weighting the input by sampling times $\Rightarrow$ able to focus/ignore
			\end{itemize}
		$\Rightarrow$ each conv location has a simplified attention map (offset)
		\item i.e. dynamically determine the sample location for each conv at each location \\
		$\Rightarrow$ adaptive receptive fields (based on semantic)
			\begin{itemize}
			\item less background noise
			\item can have large enough receptive fields for background / large object \\
			(more freedom than dilated conv)
			\end{itemize}
		\item recognize spatial transformation / geometry variance based on semantic \\
		(no embedded prior \& conditioned on the input)
		\item effective dilation are correlated with target object size \\
		(evidence to have learned meaningful result)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Pooling Layer}
\begin{itemize}
\item Max/Average Pooling
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item feature maps
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given hyperparameter: kernel size, stride (usually no padding)
		\item compute the max/average of the elements covered by kernel
		\item kernel strides along each axis over the feature maps (like conv) \\
		$\Rightarrow$ does NOT change the channel \\
		$\Rightarrow$ one kernel per channel (compared to conv)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a downsampled feature maps
		\item given a $2D$ feature map with kernel size $k \times k$, strides along each axis $s\times s$ \\
		input size $i\times i$, output size $o\times o$ \\
		$\Rightarrow o = \left\lfloor \frac {i-k} {s} \right\rfloor + 1$ (same as conv)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item downsampling the feature maps (NO weights to learn) \\ 
		$\Rightarrow$ if desired features detected anywhere, represent it by local max/average \\
		$\Rightarrow$ summarize subregions
		\item invariance to small translations of input
		\item max ops as gradient router (muse the effect of its negative input) \\
		$\Rightarrow$ update the maximal part with greater gradient
		\end{itemize}
	\end{itemize}

\item Unpooling
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item recover the spatial size before the pooling
		\item nearest neighbor: for each region, copy the number
		\item max unpooling: only place the num to its original position before pooling \\
		$\Rightarrow$ require original location in corresponding feature map in downsampling stage \\
		i.e. require the corresponding switches (location info used in pooling backprop)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item an upsampling corresponds to the downsample pooling
		\end{itemize}
	\end{itemize}

\item Spatial Pyramid Pooling (SPP)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps from CNN
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item apply on feature map a series of grids with cell number predefined
		\item perform pooling on each cell, across all grids, then concat all output \\
		$\Rightarrow$ gird defined as a proportional slicing \\
		$\Rightarrow$ actual gird solved at runtime (w.r.t feature map size)
		\item example: three grids with different cell number; each cell a max pooling \\
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-pooling spp".png}
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a fixed size feature maps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item output shape of pooling layer pre-defined $\Rightarrow$ map arbitrary input size to fixed size \\
		$\Rightarrow$ no more crop/resize on input image \\
		$\Rightarrow$ conv layer need only to handle normal ratio (less burden) \\
		(decouple conv from requirement of dense layer)
		\end{itemize}
	\end{itemize}
	
\item Region of Interest Pooling (RoI Pooling) \label{DL_Layers_Pooling_roi}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps from CNN
		\item RoIs: proposals (from selective search, RPN etc.) of $[x,y,w,h]$ \\
		(translated back to be under image coordinates) \\
		$\Rightarrow$ floating-point bbox
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item project RoI to feature map \& quantize RoI to align with indexes \\
		$\Rightarrow x'=\floor{\frac{x}{s}}$, where $s$ the network stride (same applied to $[x, y, w, h]$)
		\item divide each RoI with grid of desired size (proportional to the RoI size)
		\item max pooling from each cell/bin
		\end{itemize}
		$\Rightarrow$ single-size SPP for each RoI
	\item Implementation
		\begin{itemize}
		\item can be "crop and resize" operation in tensorflow \\
		$\Rightarrow$ use bilinear-interpolation to resample the cropped region into desired size \\
		(still have quantization, compared with RoI Align, but no more per-bin pooling)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a fixed size feature maps for each RoI
		\end{itemize}
	\end{itemize}
	
\item Positional Sensitive RoI Polling (PS-RoI Pooling) \label{DL_Layers_Pooling_psroi}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item RoIs (floating-point bbox)
		\item $k^2n$-channel feature maps: divided into $k^2$ groups, each group $n$ channel, \\
		where $k^2$ the bin/cell num of RoI, $n$ related to downstream task
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item project \& quantize RoI onto feature map
		\item selective (over channel) RoI pooling: \\
		$\Rightarrow$ for $i$-th bin/cell $\in k\times k$ bins/cells, select $i$-th group of feature maps
		\item average pooling on the bin location of the selected group
		\end{itemize}
	\item Output
		\begin{itemize}
		\item $k\times k$ feature maps with $n$ channels
		\item $n$-d vector with a further average pooling \\
		e.g. $n = 4$ for $(x,y,w,h)$ regression \& $n=C+1$ for $C$-class classification
		\item example: $3\times 3$ bins RoI for $C$-class classification \\
		(average pooling as voting to produce final classification pred) \\
		\includegraphics[width=0.5\linewidth, left]{"./Deep Learning/plot/layer-pooling psroi".png}	
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item position sensitive: dedicated conv kernels \& feature maps for each bin location \\
		(each feature map group accounts for a spatial location of RoI bin)
		\item no need of post-RoI process: share all computation \\
		$\Rightarrow$ produces prediction from each bins before RoI \\
		(parallel proposal generation \& prediction in R-FCN)
		\item $\Rightarrow$ more robust prediction \\ 
		(as $k\times k$ prediction made for each proposals)
		\end{itemize}
	\end{itemize}

\item RoI Align
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature map \& RoI (floating-point bbox)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item project the RoI to feature map without quantization \\
		i.e. $x'=\frac{x}{s}$, where $s$ the network stride $\Rightarrow$ still floating-point number
		\item calculate all the desired sampling point for the RoI
		\item bilinear interpolate to obtain the feature at each point \\
		(using the 4 closest features)
		\item $\Rightarrow$ fetch out the RoI feature \& max/avg pool at each bin (as usual) \\
		\includegraphics[width=0.3\linewidth, center]{"./Deep Learning/plot/layer-pooling roi align".png}
		\end{itemize}
	\item Output
		\begin{itemize}
		\item fixed-size feature
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item \underline{no quantization} \\
		$\Rightarrow$ remove mis-alignment between RoI location \& extracted feature \\
		$\Rightarrow$ preserve exact localization (i.e. per-pixel spatial correspondence)
		\item RoI pooling v.s. align \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/layer-pooling roi pool vs align".png}
		\end{itemize}
	\end{itemize}

\item Probabilistic Max Pooling
\end{itemize}
\subsubsection{RNN Layer}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item sequence data: includes time/precedence (conventionally arrived from left to right)
		\item for each time step, data can be vector, feature maps, etc...
		\end{itemize}
	\item RNN Cell
		\begin{itemize}
		\item consume the input of current time step \& the hidden state from last time step \\
		(hidden state usually initialized to $\mathbf 0$)
		\item calculate a hidden state at each time step
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item RNN cell at time $t$, calculate hidden state (activation) $h^t = g_h(w_{h}[h^{t-1},x^t] + b_h)$, \\ 
		where $g_h(\cdot)$ the activation function, $g_h=\tanh$ by convention \\
		$[h^{t-1}, x^t]$ the concat of $h^{t-1}$ (hidden state of time $t-1$), $x^t$ (input at time $t$)
		\item expose its hidden state at each time steps
		\item calculate its output $y^t=g_y(w_{y}h^t+b_y)$, where $g_y=\sigma, softmax$ or $identity$
		\end{itemize}
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-rnn unrolled".png}
		\end{figure}
	\item Types of RNN Mapping
		\begin{itemize}
		\item many-to-one: encoder scans through the input, only the last output considered
		\item one-to-many: decoder with single input $x^1$, take $x^t=y^{t-1}$, till $y^{t'}=$ stop
		\item many-to-many: a many-to-one encoder, followed by a one-to-many decoder \\
		$\Rightarrow$ able to map between various length
		\end{itemize}
	\item Back Propagation through Time
		\begin{itemize}
		\item unroll the recurrent operation into a sequential network with length $T$
		\item given the loss for each time step $L^1,...,L^T \Rightarrow L = \sum_{t=1}^T L^t$
		\item for time $\displaystyle t=1,...,T-1, \frac {\partial} {\partial a^t} L = \frac {\partial} {\partial a^t} L^t + \sum_{t'=t+1}^T\frac {\partial L^{t'}} {\partial a^{t+1}} \frac {\partial a^{t+1}} {\partial a^t} = \sum_{t'=t}^T \frac {\partial} {\partial a^t} L^{t'}$
		\end{itemize}
	\item Truncated Back Propagation through Time
		\begin{itemize}
		\item starting from $t=0$, form a batch $[s_t,...,s_{t+b}]$, with initial state given \\
		(either the real initial state, or the hidden state from the last batch)
		\item compute batch gradient while consuming a truncat of the whole input sequence \\
		$\Rightarrow$ mini-batch sequentially truncated off the sequence
		\end{itemize}
	\item Challenge
		\begin{itemize}
		\item bad at modeling longterm dependency due to gradient vanishing problem \\
		$\Rightarrow$ loss at late time needs to go through multiple activations to the early time \\
		(similar to the deep plain net, after unrolled) \\
		$\Rightarrow$ loss at late time are hard to affect weights when evaluated at early time \\
		(i.e. larger the $t'$, smaller the $\frac {\partial} {\partial a^t} L^{t'}$) \\
		$\Rightarrow$ hard to find out error in late time due to observation in early time \\ 
		$\Rightarrow$ hard to represent longterm dependency 
		(e.g. Car...is fast vs. Cars...are fast)
		\item easily affect by local dependency (as longterm dependency lost)
		\item gradient exploding, due to multiple / too many updates on the same weights \\
		(if weight $>1$) \\
		yet, can be solved by gradient clipping
		\item gradient vanishing, similar to explosion, but weight $<1$ \\
		$\Rightarrow$ need LSTM/GRU
		\item hard to converge, due to fluctuating gradient in an unroll \\ 
		(noisy intermediate stage) \\
		$\Rightarrow$ initial short sequence to overcome plateaus, then long sequence for dependency
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item sharing weight across time: same weights used on each time step \\
		$\Rightarrow$ solve various length input by applying weights recurrently on each of them
		\item information early in the sequence reserved \& passed through in the hidden state
		\end{itemize}
	\end{itemize}

\item Long Short Term Memory (LSTM)
	\begin{itemize}
	\item Memory Cell
		\begin{itemize}
		\item $c^t$ the memory maintained by LSTM cell at time $t$
		\end{itemize}
	\item Gates
		\begin{itemize}
		\item forget gate $G_f=\sigma(w_f[h^{t-1}, x^t] + b_f)$
		\item input gate $G_i = \sigma(w_i[h^{t-1}, x^t] + b_i)$ \\
		$\Rightarrow$ together control the memory update (how past\&current info fused)
		\item output gate $G_o = \sigma(w_o[h^{t-1}, x^{t}] + b_o)$ \\
		$\Rightarrow$ control the generation of hidden state
		\end{itemize}
	\item Fusing Info
		\begin{itemize}
		\item propose candidate $\hat{c}^{t}=\tanh(w_c[c^{t-1}, x^{t}]+b_c)$ for memory update
		\item update memory as $c^t = G_f c^{t-1} + G_i \hat c^t$
		\end{itemize}
	\item Hidden State (Activation)
		\begin{itemize}
		\item generate as $h^t = G_o \cdot \tanh(c^t)$
		\end{itemize}
	\begin{minipage}[r]{0.5\linewidth}
	\includegraphics[width=\linewidth, center]{"./Deep Learning/plot/layer-rnn lstm".png}
	\end{minipage}
	\begin{minipage}[l]{\linewidth}
	where, \\ 
	joint arrows denotes a concatenation \\ 
	$c_{t-1}, \widetilde c_t, c_t$ the previous/candidate/current memory; \\
	$h_{t-1},h_t$ the previous/current hidden state; \\ 
	$f_t, i_t, o_t$ the forget/input/output gate
	\end{minipage}
	\item Understanding
		\begin{itemize}
		\item easy to learn an identity mapping $c^{t-1}\rightarrow c^t$ \\
		$\Rightarrow$ memory (info) generated early can last for long term \\
		$\Rightarrow$ better model the long-term dependency
		\item restricted modification to $c$, as sigmoid $\in (0,1)$m tanh $\in(-1,1)$ \\
		$\Rightarrow$ overcome the gradient explosion
		\item ease the gradient vanishing
			\begin{itemize}
			\item element-wise add/mul ops, instead of matrix ops
			\item different $f,i,\widetilde c$ at each step (v.s. same $W$ in vanilla RNN)
			\end{itemize}
		$\Rightarrow$ gradient flow (highway) for $c$ (then further pass to $W$)
		\end{itemize}
	\end{itemize}

\item Gated Recurrent Unit (GRU)
	\begin{itemize}
	\item Memory Cell
		\begin{itemize}
		\item $c^t$ the memory maintained by GRU cell at time $t$
		\end{itemize}
	\item Gates
		\begin{itemize}
		\item relevance gate $G_r=\sigma(w_r[c^{t-1}, x^t]+b_r)$, a mask $\in [0,1]$ \\ 
		$\Rightarrow$ control how memory candidate proposed (fusion of last memory \& input)
		\item update gate $G_u=\sigma(w_u[c^{t-1}, x^t]+b_u)$, a mask $\in [0,1]$ \\
		$\Rightarrow$ control how memory update happen (fusion of last memory \& candidate)
		\end{itemize}
		(two gates computed with the same input, though with different weights)
	\item Fusing Info
		\begin{itemize}
		\item propose memory candidate $\hat{c}^{t}=\tanh(w_c[G_r c^{t-1}, x^{t}]+b_c)$ for the update \\
		$\Rightarrow$ decide whether the previous memory useful (relevant) with the context $x^t$
		\item update memory as: $c^{t+1}=G_u \hat{c}^{t+1} + (1-G_u)c^{t-1}$ \\ 
		$\Rightarrow$ decide how the memory updated \& remained
		\end{itemize}
	\item Hidden State (Activation)
		\begin{itemize}
		\item $h^t = c^t$
		\end{itemize}
		
	\begin{minipage}[r]{0.5\linewidth}
	\includegraphics[width=\linewidth, center]{"./Deep Learning/plot/layer-rnn gru".png}
	\end{minipage}
	\begin{minipage}[l]{\linewidth}
	where, \\ 
	joint arrows denotes a concatenation \\ 
	$h_{t-1}, \widetilde h_t, h_t$ the previous/candidate/current memory; \\ 
	$r_t, z_t$ the relevance/update gate
	\end{minipage}
	
	\item Understanding
		\begin{itemize}
		\item single gate control the generation of current memory
		\item memory directly as hidden state
		\item $\Rightarrow$ less weights, simpler structure $\Rightarrow$ faster \\
		(basic logic inherent from LSTM $\Rightarrow$ similar performance)
		\end{itemize}
	\end{itemize}


\item Bidirectional RNN (BRNN/Bi-RNN)
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item one RNN layer scanning as $t=1\rightarrow T$, hidden state exposed as $h_1^t$
		\item another RNN layer scanning from $t=T\rightarrow 1$, hidden state exposed as $h_2^t$
		\item generate output $y^t = g(w_y[h_1^t, h_2^t] + b_y)$ \\ 
		(concat all hidden states at the same time step from each RNN)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item account for the info from both previous \& latter time step \\
		$\Rightarrow$ global context info acquired
		\item cons: need the entire input sequence before processing \\
		$\Rightarrow$ NOT the case in real-time speech recognition etc.
		\end{itemize}
		
	\begin{figure}[!h]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-rnn bidirection".png}
	\caption{(using LSTM cell)}
	\end{figure}
	
	\end{itemize}

\item Convolutional LSTM (ConvLSTM)
\end{itemize}

\subsubsection{Attention}
\begin{itemize}
\item Motivation
	\begin{itemize}
	\item Long-Term Dependency Modeling
		\begin{itemize}
		\item consider all input at each step \\
		$\Rightarrow$ reduce maximal num of ops before 2 positions interact to $\mathcal O(1)$
		\item v.s. in rnn/conv/dialated conv, ops between positions grow $\mathcal O(n) / \mathcal O(\frac nk) / \mathcal O(\log_k^n)$, \\
		where $n$ the distance between positions, $k$ the conv kernel size
		\end{itemize}
	\item Interpretable Model
		\begin{itemize}
		\item can visualize \& interpret how the network models various relations \\
		(via attention distributions)
		\end{itemize}
	\end{itemize}
	
\item Encoder-Decoder Attention
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item model relationship between input-output \& overcome long-term dependency \\ 
		(e.g. $1$st input position - last output position)
		\item focus on different attributes/features based on various requirement
		\end{itemize}
	\item Input
		\begin{itemize}
		\item sequence of input vector 
		\item current output location \& previous context (attention output)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item a function over all spatial location: $[f_i, \mathbf r]\rightarrow \text{logits}_i$, \\ 
		where $f_i$ the feature at location $i$, $\mathbf r$ task specific requirement
		\item construct a distribution over all location: $\text{logits} \xrightarrow[]{\text{softmax}} \alpha$, \\
		where $\alpha$ the attention over all location $i$
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a probability distribution over all location of the input
		\item $\Rightarrow$ weighted-sum on all input vector to obtain a context vector \\
		(which is specifically designed for current output location)
		\end{itemize}
	\item Practice in Seq-to-Seq RNN
		\begin{itemize}
		\item a bi-RNN encode each input as hidden state: $h^1,...,h^{T_x}$
		\item context from attention at time $t$: $\displaystyle c^t=\sum_{i=1}^{T_x}\alpha^t_i h^i = [h^1,...,h^{T_x}]\alpha^t$, \\
		where attention $\alpha^t$ being a column vector \\
		$\Rightarrow$ a weighted sum over all hidden states as context
		\item a small (1-layer) net mapping $[c^{t-1}, h^{i}] \xrightarrow[]{\text{dense}} \text{logits}_i \xrightarrow[]{\text{softmax}} \alpha^t_{i}$ \\
		$\Rightarrow$ attention on $h^i$ depends on previous global context $c^{t-1}$ \& $h^i$ itself \\
		(softmax to ensure $\sum_{i=1}^{T_x}\alpha^t_i=1$)
		\item decoder RNN takes $c^{t}$ as input, until stop sign generated \\
		(as $c^t$ can be calculated infinite times)
		\item or, decoder RNN process as usual, but concat $[c^t, h^t]$ before output $y^t$ \\
		(still takes its previous output $y^{t-1}$ as input)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item \textbf{essence}: re-weight certain features in the network
		\item at each step, amplify relevant input hidden state (state with more scores) \\
		$\Rightarrow$ \underline{capture the token-level relationship between input-output sequence}
		\item use the whole sequence of hidden states as sequence encoding \\
		(instead of only last hidden state as context, which can be bias \& have info lost) \\
		$\Rightarrow$ at each step, provide both sequence- \& word-level info \\
		(aggregate all location of input seq for decoder rnn at each step) \\
		$\Rightarrow$ every output location can directly interact with input seq
		$\Rightarrow$ overcome RNN encoder-decoder failure in longterm dependency \\
		\item quadratic cost: attention $\alpha$ a $T_x\times T_y$ matrix to be calculated
		\end{itemize}
	\end{itemize}

\item Self Attention
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item model not only sentence-sentence relation, but also relation within the sentence \\
		$\Rightarrow$ each word recognizes all other words in its sentence
		\end{itemize}
	\item Input
		\begin{itemize}
		\item sequence of a vector \& current location
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item with matrix for Query $W^Q$, Key $W^K$ and Value $W^V$ \& a word embedding $e$ \\
		$\Rightarrow$ generate query $q=W^Q e$, key $k=W^K e$ and value $v=W^V e$, where \\
		$W^Q, W^K, W^V$ to be optimized \& $e$ a col vector
		\item given current word $t$, for each word $i$, obtain score (scalar) $s_i=(q^t)^T\cdot {k^i}$ \\
		i.e. use its "query" to multiply the "key" of other word in the sequence
		\item normalize (standardize) by the square root of the length of the "key" vector \\
		$\Rightarrow s'_i=\frac {1} {\sqrt{d_k}} s_i$, where $d_k$ the dimension of "key" \\
		(to obtain a more stable gradient)
		\item softmax over $\{s_i\}$ to obtain attention $\alpha^t$ as a col vector
		\item "self" context $c^t = \sum_{i=1}^{T}\alpha^t_iv_i = [v^1,...,v^{T}]\alpha^t$ \\
		i.e. weighted-sum over "value" vectors
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a distribution over "value" vector of each word in the current sequence \\
		$\Rightarrow$ to obtain an aggregation (weighted-sum) of "self" sequence
		\end{itemize}
	\item Speed-up
		\begin{itemize}
		\item use matrix calculation: $E = [{e^1}, ..., {e^T}]$, with each $e$ a col vector \\
		$\Rightarrow Q=W^QE, K=W^KE, V=W^VE$, with each $q, k, v$ as col vector \\
		\begin{minipage}[r]{0.7\linewidth}
		\item $\Rightarrow A = \text{softmax}\left( \frac 1 {\sqrt{d_k}} Q^T K \right)$, \\
		where $A_i$ the attention for $i^\text{th}$ word (as a row vector)
		\item $\Rightarrow C = V A^T$, where each context vector as a col vector \\
		\end{minipage}
		\begin{minipage}[r]{\linewidth}
		\includegraphics[height=0.15\linewidth, left]{"./Deep Learning/plot/layer-attention self attention".png}
		\end{minipage}
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item in seq2seq: capture the relationship inside input/output seq \\
		e.g. let model associate "it" with "animal" in sentence: \\
		"The animal didn't cross the street because it was too tired"
		\item normalization to obtain more stable gradient: \\
		e.g. with $q,k$ vector of random variables with mean $0$ \& variance $1$ \\
		$\Rightarrow q\cdot k = \sum_{i=0}^{d_k} q_ik_i$ has mean $0$ variance $d_k$ \\
		$\Rightarrow$ actually, standardization within attention layer
		\item contextualized word-embedding / sequence representation \\
		$\Rightarrow$ determine word embedding / representation with sequence context considered
		\item could be dense/conv layers, instead of matrices \\ 
		$\Rightarrow$ a layer / network branch shared across all location \\
		(for 2D input: map $m\times n$ input feature map to $(m\times n)^2$ attention map...) \\
		(if directly apply self attention)
		\end{itemize}
	\end{itemize}

\item Multi-headed Self Attention
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item provide multi-modal attention to focus on various location
		\end{itemize}
	\item Input
		\begin{itemize}
		\item same as self attention
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item the "batch-mode" of self attention to get $N$ separate results (context matrices)
		\item concat \& further project into a single context matrix $C = [C_0, ...C_{N-1}] \cdot W^O$
		\end{itemize}
	\item Output
		\begin{itemize}
		\item same as self attention \\
		$\Rightarrow$ transient to layers before \& after
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item avoid attention to focus on the current location itself \\
		$\Rightarrow$ able to focus on multiple position
		\item multiple sets of $W^Q, W^K, W^V$ to optimize \\
		$\Rightarrow$ multiple representation subspace to project the input into
		\end{itemize}
	\end{itemize}

\item Visual/Spatial Attention
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item focus on correct spatial context when generating corresponding caption/sequence
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given feature map $\mathbf a = {\mathbf a_1,...\mathbf a_L}$ \\
		$\Rightarrow$ attention $\alpha = \text{softmax} [\phi(\mathbf a, h)]$, where $h$ other info to guide the attention \\
		\item e.g. use high-level semantic feature to generate attention \\
		$\Rightarrow$ ensure enough receptive fields to accounts for global context \\
		(may need to upsample attended low-level feature to fit spatial dimension)
		\item e.g. use previous RNN hidden state to generate attention \\
		$\Rightarrow$ ensure sequential conditioning \\
		(as in "\hyperref[DL_CVNLP_Imgcap_show_attend_tell]{show attend and tell}")
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item enable model to better capture low-level info \\
		(from different location \& on demand)
		\end{itemize}
	\end{itemize}

\item Spatial Transformer
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item enable end-to-end learning of transformation invariance
		\end{itemize}
	\item Input
		\begin{itemize}
		\item feature map
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item regress parameters $\theta$, which parameterizes transformation $\mathcal T_\theta$
		\item apply transformation on output/target location $x_i^t,y_i^t$ to get source index $x_i^s, y_i^s$
		\item sample \& fill the output feature map \\
		(bilinear interpolation to avoid quantization \& smooth back prop)
		\item same transformation for every channel (to preserve spatial consistency across channels)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature map of same size \& channel
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item end-to-end optimized transformation \\
		$\Rightarrow$ dynamic mechanism \& conditioned on input data \\
		(v.s. fixed local receptive fields in pooling)
		\item achieve invariant representations \\ 
		$\Rightarrow$ able to learn \& discover pose-normalised representation (empirically) 
		\item embedded transformation model e.g. affine transformation \\
		(introduce hyper-parameters)
		\item can be interpreted as spatial attention via sampling
		\item more informative representations to base the predicted transformation parameters on (if used in deep net)
		\end{itemize}
	\end{itemize}

\item Channel-Wise Self Attention in Conv
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item better examine inter-dependency between channels \\
		(with the help of current global context info)
		\end{itemize}
	\item Input
		\begin{itemize}
		\item feature maps
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item squeeze: spatially aggregate global feature to summarize each channel \\
		e.g. channel-wise average pooling \\
		$[H, W, C]\rightarrow [1, 1, C]$
		\item excite: perform inter-channel interaction to get attention $\alpha$ \\
		e.g. dense (1x1 conv) layer(s) \\
		$[1,1,C]\rightarrow [1,1,C]$
		\item scale: channel-wisely multiply $\alpha$ back to input feature map $X$
		\end{itemize}
	\item Output
		\begin{itemize}
		\item re-weighted feature maps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item break the limited receptive fields of conv ops \\
		$\Rightarrow$ include global information to augment conv net
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item lower layers tend to have more uniform attention \\
		$\Rightarrow$ correlates with "lower layers learn more general feature"
		\item high layers tend to have more attention at some specific channels \\
		$\Rightarrow$ correlates with "higher layers are more specific \& discriminative"
		\item attention becomes almost "all 1s" at last stage \\
		$\Rightarrow$ already contains global feature at last/latter stage
		\end{itemize}
	\end{itemize}

\item Stand-Along Self Attention as Conv
	\begin{itemize}
	\item Operation
		\begin{itemize}
		\item stand-along attention
		\includegraphics[width=0.5\linewidth, left]{"./Deep Learning/plot/layer-attention local self attention as conv".png}
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item provide alternatives to conv ops \\
		(Stand-Alone Self-Attention in Vision Models - https://arxiv.org/abs/1906.05909)
		\item less FLOPS \& less params than standard dense conv
		\end{itemize}
	\end{itemize}

\item Attention in Attention

\item General Spatial Attention
	\begin{itemize}
	\item Setting
		\begin{itemize}
		\item $q$ index a query element with content (original feature) $z_q$
		\item $k$ index a key element with content (original feature) $x_k$
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item $\displaystyle y_q=\sum_{m=1}^M W_m [ \sum_{k\in\Omega_q} A_m(q, k, z_q, x_k) \circ W'_m x_k ]$, \\
		$\begin{alignedat}{1}
		\text{where } & m \text{ index the attention head i.e. a } M\text{-head attention (sum-up as final projection)} \\
		& \Omega_q \text{ specify the supporting key region for the query} \\
		& W_m, W_m' \text{ are usually learnable weights} \\
		& \circ \text{ the element-wise product}
		\end{alignedat}$
		\end{itemize}
	\item Operation - General Transformer
		\begin{itemize}
		\item key \& query from the same sentence: $z_i=x_i$ (using the same embedding)
		\item normalized attention weight: $\sum_{k\in\Omega}A_m(q, k, z_q, x_k)=1$
		\item $A_m^\text{Trans}(q, k, z_q, x_k) \propto \exp(\sum_{j=1}^4 \varepsilon_j)$
		\item $\varepsilon_1 = z_q^T U_m^T V_m^C x_k$, with $U_m,V_m^C$ learnable embedding matrix for $z_q, x_k$ \\
		(query \& key content only)
		\item $\varepsilon_2 = z_q^TU_m^TV_m^R R_{k-q}$, \\ 
		$\begin{alignedat}{1} \text{where } & V_m^R \text{ also learnable embedding matrix} \\
		& R_{k-q} \text{ encoding relative position between query and key (positional embedding)}
		\end{alignedat}$ \\
		(query content \& relative position)
		\item $\varepsilon_3 = u_m^TV_m^Cx_k$, with $u_m$ a learnable vector \\
		(key content only $\Rightarrow$ capture salience key content)
		\item $\varepsilon_4 = v_m^TV_m^RR_{k-q}$, with $v_m$ also a learnable vector \\
		(relative position only $\Rightarrow$ capture spatial bias between key and query)
		\end{itemize}
	\item Operation - Conv as (Not Normalized) Attention
		\begin{itemize}
		\item regular conv: $A_m^\text{regular}(q, k) = \begin{cases} 1 & k=q+p_m \\ 0 & \text{otherwise} \end{cases}$ \& $W_m'$ fixed as identity, \\
		where $p_m$ the offset of $m$-th sampling location, $\Omega_q$ the window for current kernel \\
		(relative position only)
		\item deformable conv: $A_m^\text{deform}(q, k, z_q)=G(k, q+p_m+w_m^Tz_q)$, where
			\begin{itemize}
			\item $G(a, b)$ the bilinear interpolation to obtain feature at $b$ via surrounding $a$
			\item $w_m$ a learnable vector (the conv to produce offset field)
			\item $\Omega_q$ is thus entire input, as $k$ determined by the required interpolation
			\end{itemize}
		$\Rightarrow$ spatial attention = dynamically predicted sampling position \\
		(query content and relative position)
		\item dynamic conv: $y_q=\sum_{m=1}^{C_\text{in}}W_m[\sum_{k\in\Omega_q}A_m^\text{dynamic}(q, k, z_q)\cdot x_{k,m}]$, where \\
		(originally designed for NMT task in NLP)
			\begin{itemize}
			\item $C_{\text{in}}$ the input chanel: each channel an attention head
			\item $x_{k, m}$ the $m^\text{th}$ channel of key $x_k$
			\item $A_m^\text{dynamic}(q, k, z_q) = \begin{cases} K_{j,m} & k=q+p_j \\ 0 & \text{otherwise} \end{cases}$, \\
			$\begin{alignedat}{1}
			& \text{with } K_{j, m} = K_{j, g}^\text{share} \propto\exp(d_{j,g}^T x_q), \text{ with } g=\ceil{\frac{c}{C_\text{in} / N_g}} \\
			& \phantom{\text{with }} K_{j, g}^\text{share} \text{ the dyanmic kernel weight shared in } g^\text{th} \text{ channel group}\\ 
			& \phantom{\text{with }} d_{j,g} \text{ the weight to predict dynamic kernel weight } K_{j, g}^\text{share} \\ 
			& \phantom{\text{with }} N_g \text{ the num of channel groups to divide input (channel-wise)} \\
			& \Rightarrow \text{ spatial attention = dynamically predicted kernel weight for depth-wise conv} \\
			\end{alignedat}$
			\item $W_m$ a learnable weight i.e. the 1x1 conv after depth-wise conv
			\end{itemize}
		(query content and relative position)
		\end{itemize}
	\item Understanding - Self Attention
		\begin{itemize}
		\item similar performance (minor drop) between $\varepsilon_{2,3,4}$ \&. $\varepsilon_{1,2,3,4}$ (full) \\
		$\Rightarrow$ query-key content is NOT that important
		\item yet, performance of using $\varepsilon_{1,2} > $ that of $\varepsilon_{3,4}$
		\item gain from using $\varepsilon_{1,2} \rightarrow \varepsilon_{1,2,3,4}$ and $\varepsilon_{3,4}\rightarrow\varepsilon_{1,2,3,4}$ are both small \\
		(compared to the gain from not using attention to $\varepsilon_{1,2}/\varepsilon_{3,4}$)
		\item also, similar performance between $\varepsilon_{2,3}$ \&. $\varepsilon_{1,2,3,4}$ \\
		($\varepsilon_{2,3}$ contain all key, query and relative position info)
		\item $\Rightarrow$ potentially duplicate/overlapped info between $\{\varepsilon\}$
		\item solely using $\epsilon_3$ results in noticeable gain: salient key is important in self-attention \\
		(as a summary of input)
		\end{itemize}
	\item Understanding - Encoder-Decoder Attention
		\begin{itemize}
		\item similar performance between $\varepsilon_{1}$ \& $\varepsilon_{1,2,3,4}$ \\
		(also, not using $\varepsilon_1$ cause noticeable drop)
		\item $\Rightarrow$ important to align element between input-output (a traversal of query-key pair is viral)
		\end{itemize}
	\item Understanding - Conv as (Self) Attention
		\begin{itemize}
		\item deformable conv {\small(query \& relative position)} + $\varepsilon_{3}$ {\small(salient key)} = the best trade-off \\
		(as can be viewed as using $\varepsilon_{2,3}$) \\
		(though, query \& key are the same in self attention)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Blocks}
\subsubsection{Inception Block}
\begin{itemize}
\item Basic Inception
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item op1 = $1\times1$ conv
		\item op2 = $1\times1$ conv, $3\times3$ conv with same padding
		\item op3 = $1\times1$ conv, $5\times5$ conv with same padding
		\item op4 = max pooling with same padding and stride $1$, $1\times1$ conv
		\item channel concate: concatenate the output channel from each op \\
		(as output size ensured to be the same in each op)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature maps with the same spatial size as input
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item enable network to learn the desired combination of info \\
		(instead of predefined hyperparamter) \\
		$\Rightarrow$ more representability
		\item $1\times1$ conv to shrink feature map before ops
			\begin{itemize}
			\item less computation for afterwards larger kernel (e.g. $3\times3, 5\times5$)
			\item prevent output of other ops from being overwhelmed by pooling
			\item expensive block due to the number of ops
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item Xception
	\begin{itemize}
	\item 
	\end{itemize}	
\end{itemize}

\subsubsection{Residual Block} \label{DL_Block_Res}
\begin{itemize}
\item Basic Structure
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for the $l+2$ layer, $a^{l+2} = g(z^{l+2} + a^l)$, \\
		where $g(\cdot)$ the activation, $z^{l+2} = W^{l+2}a^{l+1}+b^{l+2}$ from layer $l+1$ 
		\end{itemize}
	\item Main Path
		\begin{itemize}
		\item the usual passing of $a^l$ to layer $l+1$, then the $z^{l+2}$ at layer $l+2$
		$\Rightarrow$ two 3x3 conv (with batch-norm, relu)
		\end{itemize}
	\item Skip/Passthrough Connection (Shourtcut) \label{DL_Block_Res_Passthrough}
		\begin{itemize}
		\item $s(x^l)$: the passing of $x^{l}$, from layer $l$ directly to the layer $l+2$
		\item different spatial size /channel in $l, {l+2}$: adjust $x^l$ by
			\begin{itemize}
			\item padding: pad to the size in layer ${l+2}$ (pad a volume)
			\item spatial stack: merge local channels into single channel by stacking \\
			(e.g. $26\times26\times512\rightarrow13\times13\times2048$)
			\item stride-1 1x1 conv to adjust channel, stride-2 1x1 conv to also downsample \\
			(used in original paper)
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item Bottleneck Structure
	\begin{itemize}
	\item Main Path
		\begin{itemize}
		\item 1x1 conv to shrink the channel
		\item 3x3 conv to process
		\item 1x1 conv to expand the channel
		\end{itemize}
	\end{itemize}
\item Understanding
	\begin{itemize}
	\item Bottleneck
		\begin{itemize}
		\item to reduce parameters in conv kernel (used in deep net, e.g. Resnet-50, -101)
		\end{itemize}
	\item Regularization
		\begin{itemize}
		\item align with $L2$ regularization: smaller weights $\rightarrow$ 0 \\
		(as encourage to learn residual and discard useless layer)
		\end{itemize}
	\item Guaranteed Baseline
		\begin{itemize}
		\item with ReLu \& residual mapping, easy to learn identity function \\ 
		$\Rightarrow$ layer $l+2$ only need to make $z^{l+2}=0$ (as weights \& bias initialized near $0$)
		\item $\Rightarrow$ deeper net can easily guarantee to be at least as good as its shallow version \\
		(then search for luck to surpass baseline)
		\end{itemize}
	\item Ensemble
		\begin{itemize}
		\item contains much more shallow paths than real deep paths \\
		$\Rightarrow$ ensemble those shallow paths at various level
		\end{itemize}
	\item \underline{Easier Optimization}
		\begin{itemize}
		\item provide a prior function $y=x$, which is closer to the target function to learn \\
		(compared with $y=0$, as params rand init to be closed to $0$) \\
		$\Rightarrow$ learning by reference is easier \\
		(shown by smaller response \& params closer to $0$ \& faster convergence)
		\item progressive learning by design: each block learn a modification \\
		$\Rightarrow$ stacked blocks progressively fit the target function \\ 
		$\Rightarrow$ guaranteed baseline
		\item makes optimization for deep ($\ge 20$) net possible \& unleash deep power
		\end{itemize}
	\end{itemize}
\item Variants
	\begin{itemize}
	\item Wide ResNet
		\begin{itemize}
		\item increase the num of filter each conv layer \\
		$\Rightarrow$ achieve same performance as 156-layer ResNet with 50-layer wide ResNet
		\end{itemize}
	\item Stochastic Depth
		\begin{itemize}
		\item random drop out blocks in the training (replaced with identity mapping)
		\item use the full deep network at test time
		\item $\Rightarrow$ dropout on the layer to have shorter network in training \\
		(better gradient flow)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Training}
\subsubsection{Forward-Backward Propagation}
\begin{itemize}
\item Representation
	\begin{itemize}
	\item Layers
		\begin{itemize}
		\item input layer
		\item hidden layer(s): layer with NO ground truth (for the associated weights) available \\
		note: input \& hidden layers have associated biases as well (usually)
		\item output layer
		\end{itemize}
	\item Neuron (Unit)
		\begin{itemize}
		\item $s_l$: num of units in layer $l$
		\item $w^l$: weight matrix of mapping from layer $l$ to $l+1$, with shape of $\left( s_{l+1}, s_l + 1 \right)$
		\item $h(\cdot)$: activation function (usually shared)
		\item $a_j^l$: activation output of unit $j$ at layer $l$
		\item $z_j^l$: output of unit $j$ at layer $l$ \\ 
		(represent parameterized basis, also the input for layer $l+1$)
		\end{itemize}
	\item Intuition
		\begin{itemize}
		\item all stacked vertically (vertical vector) \\
		$\Rightarrow$ horizontally for different examples; vertically for different units
		\end{itemize}
	\end{itemize}
	
\item Forward Propagation (Inference)
	\begin{itemize}
	\item Activation $a^{j+1} = w^j \cdot [z_0^j, ..., z_{s_j}^j]^T, \text{ with } z_0=1$
	\item Unit Output $z^{j+1} = h(a^{j+1}) = [z_1^{1}, ..., z^{j+1}_{s_{j+1}}]^T$
	\end{itemize}

\item Backward Propagation
	\begin{itemize}
	\item Loss $\mathcal L(W) = $
	\end{itemize}

\item Practice of Back Prop
	\begin{itemize}
	\item Caching Intermediate Result
		\begin{itemize}
		\item naturally cached: input $a^0=x$, weights matrix $w$ and bias $b$
		\item activation input/output $a/z$ \\
		(since will be used in back-prop)
		\end{itemize}
	\item Auto Difference
		\begin{itemize}
		\item achievement: calculate the derivatives along the forward prop \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Practice}
\begin{itemize}
\item Check \& Debug
	\begin{itemize}
	\item Overfit Check
		\begin{itemize}
		\item sample a small dataset
		\item check network is able to overfit (0 loss without regularization)
		\end{itemize}
	\item Loss Function Check \\
	\includegraphics[width=0.4\linewidth, left]{"./Deep Learning/plot/train-learningrates".jpeg}
	\item Converge Check by First-Layer Visualization
	\includegraphics[width=0.4\linewidth, left]{"./Deep Learning/plot/train-feature visualization".png}
		\begin{itemize}
		\item noisy feature (left) indicates unconverged network
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Strategy}
\begin{itemize}
\item Layer-Wise Fine-Tuning
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item freeze the low-level layers of pretrained net \& only train the output part
		\item gradually (e.g. layer-by-layer, block-by-block) unfreeze lower layers
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item low-level layer capture generic \& task-independent info
		\item high-level layer capture task-specific \& semantic info \\
		(e.g. conv net)
		\item high-level can settle down only after its input are more consistent
		\item low-level can get effective update only if its gradient are more consistent
		\item $\Rightarrow$ train high-level first (more directly beneficial) \\
		(then jointly optimization)
		\end{itemize}
	\end{itemize}
	
\item Multi-Tasking Training
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item optimization goal composes of losses from multiple tasks \\
		(usually a sum over all losses)
		\item require weighting across different types of loss \\
		$\Rightarrow$ avoid dominant loss from one task
		\end{itemize}
	\item Understanding
		\item joint optimization find the shared representation suitable for all tasks \\
		(e.g. classification representation NOT suitable for localization)
		\item could prevent net from using false evidence by constraints from other tasks \\
		$\Rightarrow$ thus may improve per-class performance
		\item able to design loss more aligned with final evaluation matrices \\
		(though, really depends on loss design)
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architectures}

\subsection{Convolutional Networks}
\subsubsection{Classic Backbone}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Convolution Part
		\begin{itemize}
		\item one or multiple "same" conv layer(s) with stride $s=1$
		\item followed by a max (rarely average) pooling
		\item apply on input \& repeat on feature maps afterwards \\
		$\Rightarrow$ usually decreasing spacial size (width, height), increasing channel
		\end{itemize}
	\item Fully Connected (Dense) Part
		\begin{itemize}
		\item apply flattening / average pooling on last feature maps \\
		$\Rightarrow$ generate a single vector as input
		\item apply fully connected layers \& repeat for $1~2$ times \\
		$\Rightarrow$ usually with decreasing size
		\item finally output prediction probability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item trainable weights are mostly from dense layer \\
		$\Rightarrow$ conv layers, though large in number, contains far less weights
		\item conv stride $s=1$ (compare to $s>1$) \\
		$\Rightarrow$ decouple downsampling into the pooling \\
		$\Rightarrow$ account for more info/possibility before downsampling \\
		(also trade-off between required computability)
		\end{itemize}
	\end{itemize}

\item Receptive Fields
	\begin{itemize}
	\item 
	\end{itemize}

\item VGG-16
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item fixed conv: $3\times3$ kernel, stride $s=1$, same padding, ReLU activation
		\item fixed pooling: $2\times2$ kernel, stride $s=2$, max pooling
	\item Notation
		\begin{itemize}
		\item{} [conv64] to denote a conv layer with 64 output channels
		\item pool to denote max pooling
		\item{} [fc4096] to denote a fully connected layer with output vector of length 4096
		\end{itemize}
	\item Structure
		\item input image: $224\times224\times3$
		\item{} ([conv64]$\times$2, pool) $\rightarrow$ ([conv128]$\times$2, pool) $\rightarrow$ ([conv256]$\times$3, pool) $\rightarrow$ ([conv512]$\times$3+pool)$\times$2
		\item last feature maps ($7\time7\times512$) flatten into a input vector of length 4096
		\item{} [fc4096] $\rightarrow$ [fc4096] $\rightarrow$ logits $\rightarrow$ softmax prediction for $1000$ categories
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fixed conv \& pooling operation \\
		$\Rightarrow$ few hyperparameters, yet large in parameters ($\sim$138 million)
		\item introduce clean \& neat philosophy \\
		$\Rightarrow$ if not downsampling: keep to have same channel num \\
		$\Rightarrow$ at each downsample stage: halve the spatial size \& double the channel number \\
		(the complexity of each layer is preserved) \\
		(till small or deep enough e.g. 7$\times$7 size, 512 channels)
		\end{itemize}
	\end{itemize}

\item Inception Net
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item inception block
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stack of inception blocks
		\item auxiliary loss from hidden layers $\Rightarrow$ to avoid gradient vanishing
		\end{itemize}
	\end{itemize}

\item \textbf{ResNets}
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item residual block (basic/bottleneck)
		\item batchnorm applied in the conv(s) inside residual block \\
		(enormous batchnorm to address gradient exploding\&vanishing)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stack of residual block, for hundreds, even thousands of layers
		\item average pooling (channel-wise) at the last layer
			\begin{itemize}
			\item classification: softmax classifier
			\item detection: regressor \\
			... for other tasks
			\end{itemize}
		\end{itemize}
	\item Training
		\begin{itemize}
		\item warm-up the training with small learning rate till net starts to converge \\
		(avoid randomness at the beginning) \\
		$\Rightarrow$ explore the right optimization direction first
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item notice directly building deep-net cause degradation:
			\begin{itemize}
			\item high train error \& high test error $\Rightarrow$ no over-fitting
			\item batch-norm $\Rightarrow$ no gradient vanishing/exploding
			\end{itemize}
		$\Rightarrow$ current optimizer (SGD) fails to optimize deep net
		\item provide a \underline{prior} for optimizer: start with a identity mapping \\
		$\Rightarrow$ $y=x$ are closer to target function than $y=x$ \\
		(easier to push params to $0$, than fit a identity mapping from scratch) \\
		(if no modification needed) \\
		$\Rightarrow$ \underline{learning with reference}
		\item $\Rightarrow$ progressively fitting target function by design \\
		$\Rightarrow$ deep net can at least be optimized based on the result of shallow net \\
		(each residual block gradually modify the signal to the desired one)
		\item ensemble nets with shallow \& medium depth $\Rightarrow$ essentially a wide net \\
		(as the real deep net takes only few paths) \\
		$\Rightarrow$ avoid the training of real deep net (instead of solving it)
		\item compared with inception net: less choice for network (easier for optimizer) \\
		$\Rightarrow$ less params and complexity, but deeper \& better performance (depth wins!)
		\item res-net makes SGD optimization of deep net ($\ge 20$) \underline{possible} \\
		(plain net with depth $\ge20$ NOT converges even if more time/epochs given)
		\item resnet has lower response \& params closer to $0$ \& converges faster \\
		(compared to non-residual plain net) \\
		$\Rightarrow$ empirically prove resnet is \underline{easier for optimization}
		\end{itemize}
	\end{itemize}
	
\end{itemize}

\subsubsection{Fully Convolutional Networks (FCN)} \label{DL_Arch_FCN}
\begin{itemize}
\item FCN for Classification \& Regression
	\begin{itemize}
	\item Convolution as Flattening
		\begin{itemize}
		\item given input feature maps $i\times i\times c$, with $c$ channels
		\item perform valid conv with kernel $i\times i \times o$ \\ 
		$\Rightarrow$ output size $1\times1 \times o$
		\end{itemize}
	\item $1\times1$ Convolution as Fully Connected Layer
		\begin{itemize}
		\item for flatten vector $1\times1\times i$, perform $1\times1$ conv with kernel size $1\times1\times 0$ \\
		$\Rightarrow$ output size $1\times1\times o$, with same computation as fc layer
		\end{itemize}
	\item Use Case
		\begin{itemize}
		\item \hyperref[DL_CV_Objdet]{object detection}
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Recurrent Neural Network}
\subsubsection{Classic RNN}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Stacked RNN Layers
		\begin{itemize}
		\item the output of layer $l$ becomes the input for layer $l+1$ \\
		$\Rightarrow$ RNN scanning the output of previous RNN layer
		\item $3\sim5$ layer considered deep: as RNN can be unrolled into a deep plain net
		\end{itemize}
	\item Encoder-Decoder RNN
		\begin{itemize}
		\item encoder RNN scans the input sequence, last hidden layer as sequence encoding
		\item decoder RNN takes encoding as initial hidden state, unrolled the output sequence
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Attention}

\subsection{Encoder-Decoder Architecture}
\subsubsection{Basic}
\begin{itemize}
\item Encoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item downsample/encode input into rich feature maps/vectors
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual input: CNN backbone
		\item natural expression input: RNN backbone
		\end{itemize}
	\end{itemize}
\item Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item upsample/decode rich feature maps back to the original size
		\item actually, impose requirement onto the encoder
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual output: CNN backbone
		\item natural expression output: RNN backbone
		\end{itemize}
	\end{itemize}
\item Connection
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item combine high level information with low level information
		\item image $\rightarrow$ image: outline refinement ...
		\item language $\rightarrow$ language: sentence style capturing
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item concatenation
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Extension}
\begin{itemize}
\item Siamese Network
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image pair $(p_1,p_2)$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item CNN as feature encoder \\
		$\Rightarrow$ same CNN process each image, yielding $(f_1, f_2)$ as extracted feature
		\item jointly process concat $[f_1,f_2]$
		\end{itemize}
	\item Variants
		\begin{itemize}
		\item joint process from the beginning \\
		$\Rightarrow$ concat $[p_1,p_2]$, then CNN till the end \\
		$\Rightarrow$ generally \textbf{better performance}, especially when detailed structure are vital
		\end{itemize}
	\end{itemize}

\item Multiple Encoder (Pseudo Siamese Network)
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item project different information into the same space \\
		(jointly process those information after feature projection/extraction)
		\end{itemize}		
	\end{itemize}

\item Multiple Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item impose multiple requirements to the encoder (via auxiliary loss) \\
		$\Rightarrow$ multi-task training (multiple output branch)
		\end{itemize}
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Vision}
\subsection{Visual Sensor}
\subsubsection{Camera}
\begin{itemize}
\item Basic Image-forming Theory
	\begin{itemize}
	\item Camera Coordinate
		\begin{itemize}
		\item image axises: same as axises for matrix index
		\item $x,y$ align with the image axises, $z$ align with focal line
		\end{itemize}
	\item Resolution
		\begin{itemize}
		\item the minimum feature size of the object under inspection \\
		$\Rightarrow$ finally, represented as the pixel in the image \\
		(hence, jointly controlled by pixel size, lens, object distance and sensor size)
		\item pixel count: total number of pixels in the row/col of image
		\item pixel density: dots/pixels oer inch (dpi)
		\item sensor resolution: pixel count on sensor and their physical size and spreading
		\item lens resolution: the impact of optics (diffraction, etc) on light info towards sensor
		\end{itemize}
	\item ISO: Light Sensitivity
		\begin{itemize}
		\item used to be the light sensitivity of the film
		\item $\Rightarrow$ in digital camera: the amplification/attenuation for raw sensor value \\
		$\Rightarrow$ can add noise if amplified too much, due to dark current
		\item base ISO: 
		\end{itemize}
	\item Exposure Time
		\begin{itemize}
		\item control incoming light (denoted by shutter speed)
		\end{itemize}
	\item Focal Length $F$
		\begin{itemize}
		\item the distance from last lens central point to focal point, determined by lens physics
		\item given a fixed object distance, larger $F$, larger formed object \\
		(object distance need to be larger than focal length) \\
		\includegraphics[width=0.5\linewidth, left]{"./Deep Learning/plot/topic-vision sensor camera focal len".jpg}
		\end{itemize}
	\item Aperture
		\begin{itemize}
		\item diameter of entrance pupil $D$: control the size of aperture \\
		$\Rightarrow$ aperture stop: aperture setting to restrict input pupil size and hence brightness
		\item larger size ($D$), more incoming light
		\end{itemize}
	\item F-Number $N = \frac F D$
		\begin{itemize}
		\item reveal the signal to noise ratio, contrast, image brightness
		\item larger $N$, smaller $D$, less incoming light, larger DOF, larger depth of focus
		\end{itemize}
	\item Depth of Field (DOF) \& Depth of Focus
		\begin{itemize}
		\item definition: the range on $z$ axis where resolution can be maintained \\
		before the lens: depth of field (in the field) \\
		after the lens: depth of focus (inside camera) \\
		\includegraphics[width=0.5\linewidth, left]{"./Deep Learning/plot/topic-vision sensor  camera dof".jpg}		
		\end{itemize}
	\item Depth of Field
		\begin{itemize}
		\item DOF as the range where $1$ pixel can hold all info from $1$ position \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor  camera n and dof".png} \\
		where, dashed line the incoming light; \\ 
		red line the system resolution (the pixel size, changing with object dist) \\
		$\Rightarrow$ DOF defined by intersection of two type of lines \\
		$\Rightarrow$ with large enough DOF, resolution can be bad at the end of DOF \\ 
		(pixel size too large)
		\item resolution with DOF \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor  camera dof and resolution".png} \\
		where black boxes a series of pixels, beams in colors as light info from an object \\
		$\Rightarrow$ the placement of box on beams shows the information contained in a pixel \\
		$\Rightarrow$ larger DOF, large depth range for acceptable resolution \\ 
		(as pixel contain more separable info) \\
		$\Rightarrow$ out of DOF: pixel can not distinguish beams from diff location
		\end{itemize}
	\item Depth of Focus
		\begin{itemize}
		\item depth of focus as the range where $1$ pixel can hold all info from $1$ position
		\item resulted from tilted sensor $\Rightarrow$ may not in the ideal position to receive light info \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor camera do focus".png} \\
		where right-most line as ideal focal plane, left lines as sensor tilted by $12.5/25\mu m$ \\
		$\Rightarrow$ tilt not influence pixel in the center, but the pixel in the corner \\
		$\Rightarrow$ out of depth of focus: pixel can not distinguish beams from diff location
		\end{itemize}
	\end{itemize}
\item Camera System
	\begin{itemize}
	\item Optics
		\begin{itemize}
		\item lens: determine focal length, control f-number, thus DOF
		\item filter: filter out the influence of infer-red
		\end{itemize}
	\item Image Sensor
		\begin{itemize}
		\item tilted sensor for optic-electric conversion
		\item determine the pixel-size, thus influence resolution
		\item dynamic range: range of full electric capacity - smallest signal-to-noise ratio
		\end{itemize}
	\item Color Filter Array
		\begin{itemize}
		\item a filter for each sensor to allow only light of specified color to reach sensor
		\item interpolation to recover color info for each pixel \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor camera color filter array".png}
		\end{itemize} 
	\item ISP
		\begin{itemize}
		\item optimize the raw sensor data to account for:, e.g. \\
		white balance, motion compensation, tone mapping, denoising, etc.
		\end{itemize}
	\item Serdes
		\begin{itemize}
		\item serialized output the displayed image
		\end{itemize}
	\end{itemize}
\item Noise
	\begin{itemize}
	\item Rolling Shutter
		\begin{itemize}
		\item due to image sensor scanning through across the scene \\
		(instead of snapshot the whole scene) \\
		$\Rightarrow$ speed of shutter $+$ object movement speed: skewed scene
		\item scanning scene takes time \& each time step only scanning a roll of sensor
		\item trade-off: able to continue gather photons, thus more sensitive \\
		(v.s. global shutter)
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Lidar}
\begin{itemize}
\item Time of Flight (ToF) System
	\begin{itemize}
	\item Light Source
		\begin{itemize}
		\item wavelength: 905nm vs. 1550nm
		\item laser peak power: need to avoid eye damage
		\item repetition rate: positively related to fps and points intensity
		\item pulse width
		\item beam quality
		\item background noise: more noise for 905nm than 1550nm
		\end{itemize}
	\item Scanning
		\begin{itemize}
		\item spinning: emit single beam \& scan emit angle across 3D space \\
		$\Rightarrow$ \underline{do need motion compensation if sensing when moving}
		\item optical phase array: use electricity to control beam angle (solid state streamer)
		\item flash light: emit laser like flashlight (consume more power)
		\end{itemize}
	\item Receiver
		\begin{itemize}
		\item sensitivity: determine effective detect distance
		\item saturability: determine noise and dynamic range
		\end{itemize}
	\item A/D Converter
		\begin{itemize}
		\item convert received laser to electricity signal
		\item response curve: usually non-linear
		\end{itemize}
	\end{itemize}
\item 
\end{itemize}

\subsubsection{Radar}
\begin{itemize}
\item Radar System
	\begin{itemize}
	\item Waves
		\begin{itemize}
		\item range resolution inversely $\propto$ bandwidth
		\item frequency $\in (76, 81)$ Ghz
		\end{itemize}
	\item Pulse Radar
		\begin{itemize}
		\item transmit antenna $G_t$ send pulse signal
		\item receiver antenna $A_t$ receive signal
		\item measure range by ToF
		\end{itemize}
	\item FMCW Radar
		\begin{itemize}
		\item send frequency modulated continuous-wave signal, instead of pulse \\
		$\Rightarrow$ measure speed (vector) via observing Doppler frequency shift
		\end{itemize}
	\end{itemize}
\item Understanding
	\begin{itemize}
	\item Ability
		\begin{itemize}
		\item work under extreme condition (-40 to 80$^{\circ}$, rain, dust, etc.)
		\end{itemize}
	\item Features
		\begin{itemize}
		\item sensitive to metal
		\item multiple radar interference: receive signal sent by others
		\item multiple reflectance: waves bounce between parallel metal objects
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Objects Classification} \label{DL_CV_Objcls}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Challenges
	\begin{itemize}
	\item Semantic Gap
		\begin{itemize}
		\item perceive giant matrix of number (img)
		\item while the goal is to determine the semantic meaning (class)
		\end{itemize}
	\item Viewpoint Variation, Illumination and Deformation
		\begin{itemize}
		\item mapping completely different matrix of number to the same class
		\end{itemize}
	\item Occlusion and Background Clutter
		\begin{itemize}
		\item need to distinguish information from noise
		\end{itemize}
	\item Inter- and Intra- class Variance
		\begin{itemize}
		\item need to distinguish the variance within \& between classes
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Objects Detection} \label{DL_CV_Objdet}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Spatial Information
		\begin{itemize}
		\item image maps as $3$-D tensor: width, height, channel (rgb, etc.)
		\item multi/stereo -images for $3$-D detection
		\end{itemize}
	\end{itemize}

\item Goal
	\begin{itemize}
	\item Bounding Box Prediction
		\begin{itemize}
		\item localization as regression task on box attributes ($x,y,h,w$)
		\item classification on object classes \& object existence
		\end{itemize}
	\item Landmark (Key Point) Prediction
		\begin{itemize}
		\item landmarks: the coordinates of key points (of  each type) in image
		\item label of landmark should be consistent across image
		\item output real number as regression task \\
		$\Rightarrow$ used in pose detection, bounding box detection, etc.
		\end{itemize}
	\end{itemize}

\item Evaluation Metrics
	\begin{itemize}
	\item Recall-to-IoU
		\begin{itemize}
		\item the plot of recall at each IoU threshold, similar to average precision curve
		\item should use only for diagnosis, instead of performance evaluation \\
		(as only loosely related to accuracy, etc.)
		\end{itemize}
	\end{itemize}

\item Challenge and Development
	\begin{itemize}
	\item Varying Targets
		\begin{itemize}
		\item variable number of targets $\Rightarrow$ varying number of pred
		\item proposals: dynamically generated candidate box \& refine the proper one (discard others)
		\item anchors: fixed list of proposals
		\end{itemize}
	\end{itemize}

\item Understanding
	\begin{itemize}
	\item multi-object localization \& classification
	\item Two-Stage Detection
		\begin{itemize}
		\item $1^\text{st}$ stage: objectness classification (filter out simple neg) \\
		(trained with imbalanced data)
		\item $2^\text{nd}$ stage: fine $k$-class classification \& location refinement \\
		(trained with more balanced data)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Common Processing}
\begin{itemize}
\item Non-max Suppression \label{DL_CV_Objdet_nonmax}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item multiple predicted bounding boxes, with probability of existence ($p_e$) relatively large
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item clean up \& account for duplicate bounding boxes on the same object \\
		$\Rightarrow$ for each (predicted) object, finalize prediction to be a single bounding box
		\end{itemize}
	\item Naive Operation
		\begin{itemize}
		\item choose the bounding boxes with highest (maximal) $p_e$ (for each object classes)
		\item remove those bounding boxes whose IoU with it are large \\
		$\Rightarrow$ remove (suppress) bounding boxes with large overlap
		\item repeat until all bounding boxes have a low IoU with each other \\
		$\Rightarrow$ remaining boxes are the final predictions
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item usually not harm performance (as preserving high-score pred)
		\item help reduce false positive
		\end{itemize}
	\end{itemize}
\item Selective Search
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item class independent regions to distinguish regions of different objects \\
		$\Rightarrow$ object segmentation: potential region containing object (with classification waived)
		\end{itemize}
	\item Hierarchical Grouping
		\begin{itemize}
		\item initialize the image with a region proposals method
		\item compute similarities for each adjacent region pairs
		\item group the most similar region pair into one region \\
		(similarity include: size, color in color space, sift, etc.)
		\item iterate through the process, until whole image grouped into a single region
		\item diverse settings: color space, similarity measurement, region initialization method
		\end{itemize}
	\item Region Scoring
		\begin{itemize}
		\item the earliest merged (grouped) images get the highest score in current run \\
		e.g. last merge (whole image) scored $1$, $2^\text{nd}$ merge scored $2$
		\item propose region with diverse settings \& accumulate score for the same region
		\item select regions in first $n^\text{th}$ high score
		\end{itemize}
	\item Understand
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\item Localization Loss Normalization
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item proposal box $b=\{b_x,b_y,b_w,b_h\}$
		\item pred box $p=\{p_x,p_y,p_w,p_h\}$ \\
		$\Rightarrow$ network output $\{\frac{p_x-b_x}{b_w}, \frac{p_x-b_y}{b_h}, \log\frac{p_x}{b_w}, \log\frac{p_x}{b_h}\}$ \\
		\item ground truth box $g=\{g_x,g_y,g_w,g_h\}$ \\
		$\Rightarrow$ label $l=\{\frac{g_x-b_x}{b_w}, \frac{g_y-b_y}{b_h}, \log\frac{g_w}{b_w}, \log\frac{g_h}{b_h}\}$ \\
		(under classic encoding: predict the adjustment)
		\Item \begin{align*} \Rightarrow & \textstyle {\text{ diff } \Delta=\{ \delta_x, \delta_y, \delta_w, \delta_h \},} \\
		& \textstyle {\text{ where } \delta_x = \frac{g_x-p_x}{b_w}, \delta_y = \frac{g_y-p_y}{b_h}, \delta_w=\log\frac{g_w}{b_w}, \delta_h=\log\frac{g_h}{b_h}} \end{align*}
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item avoid too small $\Delta$ localization loss (compared to classification loss) \\
		$\Rightarrow$ normalization to avoid small numerics
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item normalize as $\forall \delta \in \Delta, \delta := \frac{\delta-\mu}{\sigma}$, where $\mu$ the mean; $\sigma$ the standard deviation
		\end{itemize}
	\item Understand
		\begin{itemize}
		\item ensure balanced loss  $\Rightarrow$ more effective multi-tasking
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Region-based Detection}
\begin{itemize}
\item Sliding Window Detection
	\begin{itemize}
	\item Bounding Box Proposal
		\begin{itemize}
		\item slide the window with various size, across the image \\
		$\Rightarrow$ propose bounding boxes with predefined size \& location
		\item feed the window into CNN for classification \\
		$\Rightarrow$ CNN as classifier, window as bounding box
		\end{itemize}
	\item Fast Implementation: Sliding Window as Convolution
		\begin{itemize}
		\item implement CNN classification as \hyperref[DL_Arch_FCN]{FCN} \\
		$\Rightarrow$ as conv independent from input size
		\item run directly the CNN on the input image (instead of on each sliding window) \\
		$\Rightarrow$ output size $m\times n \times k$, \\ 
		where $m\times n$ the total number of sliding windows on the image, $k$ the class number
		\item $\Rightarrow$ one times running for all sliding windows \\ 
		(as sliding window essentially is a crop from image)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item though with fast implementation, still not promising regarding result \\
		$\Rightarrow$ sliding window propose a fixed set of window \\
		$\Rightarrow$ fail if not matching window size; or missing location/rotation
		\end{itemize}
	\end{itemize}

\item R-CNN: Regions with CNN Features
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item tightly crop/wrap over regions from region proposal methods
		\item regressor output refinement of $x, y, w, h$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item selective search $\Rightarrow \sim 2000$ regions potentially with object
		\item each region edged with a tight bounding box, resized according to CNN
		\item CNN to extract feature for each bounding box, into $4096$-length vector
		\item class-specific SVMs (one for each class) to classify each box (feature vec)
		\item per-class linear regressor to refine the proposed box \\ 
		(better localization mAP $4\%\uparrow$)
		\end{itemize}
	\item Training CNN
		\begin{itemize}
		\item pre-train on classification
		\item fine-tine on detection: classify resized proposed box into $N+1$ classes \\
		($+1$ for non-objectness)
		\item box cropping: the tightest box + some space for context
		\item $32$ box with object \& $96$ box without object $\Rightarrow$ bias to positive example
		\item neg example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} < 0.5$
		\item pos example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} > 0.5$ \\
		(assigned to the label box with max IoU if multiple matched)
		\item rely on val set, with train set as auxiliary source for pos example \\
		$\Rightarrow$ regenerate more class-balanced data split \\
		(avoid annotation noise in train set)
		\end{itemize}
	\item Training SVM
		\begin{itemize}
		\item input $4096$-length vector, output binary classification for a class
		\item neg example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} < 0.3$
		\item pos example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} > 0.5$
		\item box in-between are ignored
		\item all label box as pos example, hard neg mining on 5k images \\
		$\Rightarrow$ control data volume, speed up SVM inference
		\end{itemize}
	\item Training Linear Regressor
		\begin{itemize}
		\item input the feature at CNN logits layer, regress location offset
		\item regress $x,y$ directly, $w,h$ in the log space
		\item only regress box nearby:  $\text{IoU}^\text{region proposal}_\text{label box} > 0.6$ \\
		(assigned to label box with max IoU if multiple matched)
		\item heavily regularized (as only performing nearby regression)
		\end{itemize}
	\item Extension in Segmentation
		\begin{itemize}
		\item region proposal switched to that for segmentation
		\item wrapped bounding box on proposed region
		\item get conv features with \& without background in box removed (set to $0$)
		\item output classification for the region
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item first to bridge the gap between classification \& detection
		\item first to show supervised pre-training on auxiliary dataset is effective \\
		$\Rightarrow$ multi-tasking is good
		\item important to distinguish pos-neg examples \\ 
		$\Rightarrow mAP 5\%\uparrow$ when thr$=0.3$, (v.s. thr$=0.5$) \\
		$\Rightarrow$ pos-neg balance important
		\item context in box matters
		\item by ablation study reveals: CNN ability comes from conv layer \\
		$\Rightarrow$ \underline{CNN as feature map extractor} for other domain-specific algorithm
		\item by visualization reveals: CNN is able to learn \underline{distributed representation} \\
		$\Rightarrow$ recognize abstract attributes e.g. shape, texture, color, material etc.
		\item speed legged by region proposal
		\item performance legged by localization (instead of classification, thanks to CNN)
		\item jittered example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box}\in(0.5, 1)$ but is not ground truth \\
		$\Rightarrow$ need more careful fine-tunning to avoid SVM (which is used for hard negative mining) \\
		$\Rightarrow$ problems arise from NOT being end-to-end trained
		\end{itemize}
	\end{itemize}

\item Fast R-CNN
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item similar to SPP \& R-CNN: predict adjustment/refinement for each proposal \\
		(focusing on the 2nd stage)
		\item classifier for all $k+1$ class \& each $k$ class has its own location regressor \\
		($1$ for the background class)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item input: an image \& a list of region of interest (RoI) in the image \\
		(RoI pre-calculated from proposals)
		\item one-pass inference: shared computation for proposals from backbone CNN \\
		(similar to SPP-net \& improved over R-CNN)
		\item \hyperref[DL_Layers_Pooling_roi]{RoI pooling} for each projected proposal \\
		(a special SPP with only one pyramid level) \\
		$\Rightarrow$ extract fix-size feature from each proposal
		\item further 2 dense layers generate RoI feature vector
		\item fed into classification \& localization branches (each with 1 dense layer)
		\item perform \underline{per-class NMS} on refined proposals to generate final bbox output \\
		$\Rightarrow$ all bboxes are included in NMS (as independent NMS for each class)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item VGG to extract feature of image
		\item RoI pooling to extract proposal (RoI) region into fixed-len feature
		\item discard SVM: no need to cache tons of feature (support vector) on disk
		\item post-RoI dense layers could be implemented as 1-D conv over RoI feature vectors
		\end{itemize}
	\item Training
		\begin{itemize}
		\item in R-CNN/SPP: uniformly sample RoI, each RoI has an associated image \\
		$\Rightarrow$ NOT able to share computation if each RoI is from a different image
		\item discard the concern of \\ 
		"RoI from one image are correlated and can cause slow convergence" \\
		(no harm ... as each example treated independently anyway)
		\item \underline{enforced computation sharing}: \\ 
		sample $N$ images, then sample $R/N$ RoIs for each image \\
		($R$ the size of batch: each RoI considered as an example) \\
		$\Rightarrow$ much faster training as computation shared over $R/N$ gradient updates \\
		(in paper,  $N=2,R=128$)
		\item \underline{multi-task loss}: $L(p,u,t^u,v) = L_\text{cls}(p, t^u) + \lambda [u\ge 1] L_\text{loc}(t^u, v)$,
		\begin{align*}
		\text{where } & p / u \text{ the pred / label for classification (0 being background class)} \\
		& t^u / v \text{ the pred / label bbox for true class } u
		\end{align*}
		\item classification loss $L_\text{cls}(p,u) = -\log p_u$ the cross-entropy
		\item localization loss: $L_\text{loc}(t^u, v) = \sum_{i\in\{x,y,\log w,\log h\}} \text{smooth}_{L_1}(t^u_i - v_i)$, \\
		where: $\text{smooth}_{L_1}(x) = \begin{cases} 0.5x^2 & \abs {x} < 1, \\ \abs x - 0.5 & \text{otherwise}. \end{cases}$
		\item normalize location label $v$ to be $0$-mean \& unit variance \\
		$\Rightarrow$ help setting $\lambda=1$
		\item $IoU^\text{proposal}_\text{labe} > 0.5$ to be positive example, $0.1 < IoU^\text{proposal}_\text{label} < 0.5$ to be negative \\
		(some data discarded due to GPU limit; pos example ensured to be $< 25\%$)
		\item joint end-to-end training
		\end{itemize}
	\item Analysis: v.s. Stage-Wise Training
		\begin{itemize}
		\item first train classification \& then train location branch with other params frozen \\ 
		$\Rightarrow$ worse than joint multi-task training
		\item joint optimization find the shared representation suitable for both tasks \\
		(classification representation NOT suitable for localization)
		\item could prevent net from using false evidence by constraints from other tasks \\
		$\Rightarrow$ thus may improve per-class performance
		\end{itemize}
	\item Analysis: Scale Invariance
		\begin{itemize}
		\item conv net able to achieve 
		\end{itemize}
	\item Analysis: Proposal Density
		\begin{itemize}
		\item Fast R-CNN cannot handle dense proposals
		\item average precision fall \& average recall not able to review true ability of net \\
		(as recall due to more input, rather than ability of net)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item \underline{better performance from multi-tasking \& end-to-end joint optimization} \\
		(by exploring a more robust shared representation \& no SVM)
		\item faster training: sharing computation in batch \& no SVM
		\item faster inference: sharing convolution across proposals \\
		(together with, network compression by SVD on weight matrix of dense layer)
		\item \underline{smooth $L_1$} is less sensitive to outliers than $L_2$ \\
		$\Rightarrow$ thus less sensitive to learning rate tunning \\
		(as $L_2$ needs to avoid gradients exploding under unbounded location pred)
		\item per-class regressor: big output volume \& require enough data for all classes \\
		$\Rightarrow$ hard to scale to detection with thousands of classes
		\end{itemize}
	\end{itemize}

\item RPN: Faster R-CNN
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item anchor box $[x_a, y_a, w_a, h_a]$: predefined boxes of various $\{\text{scale}\}\times\{\text{ratio}\}$ \\
		(a grid of reference boxes - $k$ anchors at each location on output feature map)
		\item proposal: similar to Fast R-CNN, but adjustment predicted w.r.t anchors \\
		$\Rightarrow t_x = \frac {x-x_a} {w_a}, t_y = \frac{y-y_a}{h_a}; t_w = \log(\frac{w} {w_a}), t_h = \log(\frac{h}{h_a})$ \\
		$\Rightarrow$ used as reference box for $2$nd-stage detector
		\item Fast R-CNN as detector \& its box encoding used at final pred
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item RPN predicts proposals based on anchor box, with only objectness classification \\
		(binary fore- vs. back-ground classification) \\
		$\Rightarrow$ channel-size = $2k$ ($k$ if using sigmoid) for cls; $4k$ for reg
		\item non-maximal suppression (NMS) to reduce proposals
		\item project \& quantize floating-point $[x,y,w,h]$ box onto the feature map \\
		($[x,y,w,h]$ box is under original image coord, after translated by anchor box) \\
		$\Rightarrow x'=\floor{\frac{x}{s}}$, where $s$ the network stride
		\item detector (Fast R-CNN) predicts final boxes \& multi-class classification \\ 
		(with class-specific NMS)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item backbone CNN (VGG, Resnet-50, etc.) extract feature map
		\item RPN branch $=$ 3x3 conv $+$ 2 sibling 1x1 conv (for cls \& reg)
		\item RoI pooling on the extracted feature map, according to each adopted proposal
		(followed by rest of the Fast R-CNN) \\
		$\Rightarrow$ proposal generator \& detector shares the same backbone
		\end{itemize}
	\item Training RPN
		\begin{itemize}
		\item RPN loss $L(\{p_i\} \{t_i\}) = \frac 1{N_\text{cls}} \sum_i L_\text{cls}(p_i, p_i^*) + \lambda \frac 1 {N_\text{reg}} p_i^* L_\text{reg}(t_i,t_i^*)$, \\
		where $*$ denotes label, $p^*_i / t^*_i$ the cls/reg label for $i$-th anchor
		\item $L_\text{cls}$ the log loss on objectness of proposal (binary classification) \\
		$\Rightarrow p_i^* = \begin{cases} 1 & \text{IoU}^\text{anchor}_\text{label} > 0.7 \text{ or anchor overlap with a label with highest IoU}, \\ 0 & \text{IoU}^\text{anchor}_\text{label} < 0.3 \end{cases}$
			\begin{itemize}
			\item ignore confusing anchors with $\text{IoU}^\text{anchor}_\text{label} \in [0.3, 0.7]$ (not included in loss)
			\item ensure at least 1 (can have multiple) anchor for each label
			\end{itemize}
		\item $L_\text{reg}$ the smooth $L_1$ loss on location regression \\ 
		(only include positive anchor as guarded by $p^*_i$)
		\item $N_\text{cls/reg}$ normalizing the cls/reg loss by num of involved example \\
		$\Rightarrow N_\text{cls}$ = batch size; $N_\text{reg}$ = num of positive anchor
		\item $\lambda$ the balancing term, default to $10$ \& robust in a wide range ($1-100$)
		\item each batch sample $256$ anchors from the same image \& ensure pos $:$ neg $\le 1:1$ \\
		(share computation across anchors as much as possible)
		\item cross-border anchors can match to label (thus positive) \\
		$\Rightarrow$ predict boxes from $0$-padding influenced feature (noisy feature) \\
		$\Rightarrow$ NOT converge \\ 
		$\Rightarrow$ \underline{cross-border anchors are discarded}
		\end{itemize}
	\item Joint Training
		\begin{itemize}
		\item final loss = loss of 1st stage (RPN) + loss of 2nd stage (Fast R-CNN) \\
		$\Rightarrow$ shared backbone updated by gradient from both branch
		\item YET, stop the gradient from 2nd stage to RPN \\
		(as gradient from RPN loss \& from 2nd stage can diverge) \\
		(though still converge after enough training)
		\end{itemize}
	\item Alternating (4-step) Training
		\begin{itemize}
		\item train RPN: pretrain on ImageNet \& finetune on detection dataset
		\item train Fast R-CNN: pretrain on ImageNet \& finetune on detection dataset
		\item finetune 1st stage: using Fast R-CNN backbone as fixed \& update RPN branch
		\item finetune 2nd stage: backbone and RPN fixed \& update Fast R-CNN branch \\
		(could alternatively fine-tune for more times...)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item RPN: high-quality region proposal by learning:
			\begin{itemize}
			\item fully-conv net as sliding window proposal generator \\
			(use same spatial window at each reg location) \\
			(use diff regressor for each of the $k$ anchor)
			\item benefit from deep model \& increasing data \\
			(e.g. resnet-101 \& coco leads to better result)
			\item more stable proposal due to fully-conv net setting
			(translation invariant, upto the network stride)
			\end{itemize}
		$\Rightarrow$ less proposal ($\le300$) required \& less burden on 2nd stage \\
		(top-ranked RPN proposals are accurate)
		\item $\Rightarrow$ faster \& unified end-to-end detection framework \\
			\begin{itemize}
			\item enable shared conv between proposal generation \& refinement
			\item shared backbone optimized with multi-tasking \& more end-to-end framework
			\end{itemize}
		\item \underline{anchors} as multi-scale prior $\Rightarrow$ detection reference pyramid \\
		(as predict the adjustment of referenced/anchor box)
			\begin{itemize}
			\item avoid resource-consuming input image / kernel size pyramid
			\item address scale-ratio variance with dedicated params ($k$ regressor for $k$ anchors)
			\end{itemize}
		$\Rightarrow$ multi-scale reference is efficient \& effective		
		\item faster inference \& training:
			\begin{itemize}
			\item no image/filter pyramid, but anchors, for multi scale
			\item sharing conv for proposals generator \& detector
			\end{itemize}
		\item \underline{2-stage framework}:
			\begin{itemize}
			\item RPN generates proposals by \underline{only objectness}, with highly imbalanced pos-neg \\
			$\Rightarrow$ filters out easy negative \& bad prior by objectness \\
			$\Rightarrow$ more balanced training data \& good prior for 2nd stage
			\item detector further regress \& classify (class-specific detection)
			\end{itemize}
			$\Rightarrow$ 2-stage hierarchical refinement on both regression \& classification \\
			(RPN has its own loss: thus 2-stage, not 1-stage network with a sub-net)
		\item improve on hard case, indicated by recall/precision @ high IoU threshold
		\item still a $16$-stride (= downsample rate) detection network \\
		(could improve by smaller stride)
		\item indeed, 4-branch network \& sequential inference \\
		$\Rightarrow$ still need to design well-aligned loss for the 1st-stage proposal \\
		(as no gradient from 2nd stage)
		\end{itemize}
	\end{itemize}

\item R-FCN: Region-based Fully Conv Net
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item similar to Faster R-CNN (anchor-proposal-pred)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item RPN head generate proposals
		\item R-FCN head generate $k^2$ group of feature maps, each group $n$ feature maps \\
		(in total $k^2n$: $n=C+1$ for cls \& $n=4$ for reg), \\
		where $k\times k$ the bins num of RoI pooling
		\item \hyperref[DL_Layers_Pooling_psroi]{positional-sensitive RoI pooling} \\
		$\Rightarrow$ for $i$-th bin, slice \& average the feature maps of $i$-th group \\
		$\Rightarrow$ producing $n$ feature maps of size $k\times k$
		\item voting (another average pooling) on RoI features to obtain final cls/reg prediction
		\item NMS with IoU threshold = 0.3
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item RPN \& detector (conv + ps-roi pooling) sharing backbone
		\item atrous ResNet-101 used as backbone (stride reduced $32\rightarrow16$) \\
		(1x1 conv to reduce last feature maps channel num to 1024)
		\item conv to produce feature maps for $k\times k=7\times7$ bins ps-roi pooling
		\end{itemize}
	\item Training
		\begin{itemize}
		\item loss similar to Faster R-CNN: $L_\text{cls}=$ cross entropy; $L_\text{reg}=$ smooth L1
		\item alternating training RPN \& R-FCN branch
		\item $\text{IoU}^\text{RoI}_\text{label} \ge 0.5$ to be positive \& $\le 0.5$ to be negative
		\item online hard examples mining (OHEM) \\
		$\Rightarrow$ evaluate $N$=300 proposal \& select $B$=128 RoIs with highest loss for back-prop
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item location info demanded by detection v.s. translation invariance FCN backbone \\
		(motivation: solving dilemma \& maintain simplicity of FCN) \\
		$\Rightarrow$ encode \underline{spatial info into different semantic feature maps} \\
		$\Rightarrow$ demands dedicated conv kernel to infer cls\&reg from each bin location \\
		(e.g. $k=3$: can have positions of top-left, ..., bottom-right) \\
		$\Rightarrow$ dedicated feture maps strongly activated at specific relative position of an obj
		\item position-sensitivity are necessary for detection \\
		(if $k=1$, i.e. remove spatial info $\Rightarrow$ NOT even converge)
		\item thus, \underline{an FCN detection net}: \\
		$\Rightarrow$ sharing all computation (including cls/reg prediction) across proposals \\
		$\Rightarrow$ moving params forward \& more params for less computation by FCN fashion \\
		$\Rightarrow$ produce dense prediction from each bin before RoI:
			\begin{itemize}
			\item $\mathcal O(1)$ inference time w.r.t. proposals $\Rightarrow$ enable costless OHEM
			\item parallel proposal generation \& prediction
			\end{itemize}
		\item $\Rightarrow$ more robust prediction \\
		(as $k\times k$ prediction made for each proposal)
		\item transfer deep classifier into deep detector by adding conv for ps-roi pooling \\
		$\Rightarrow$ without introducing per-RoI process (thus no efficiency burden)
		\item cls branch: $\{\text{bin location}\}\times\{\text{class}\}$ \\
		$\Rightarrow$ infeasible for large-category classification (e.g. 1000 classes)
		\end{itemize}
	\end{itemize}
	
\item Feature Pyramid Network (FPN)
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item same as Faster R-CNN \& R-FCN
		\item anchors assigned to different pred head according to anchor scale $P_k$, \\
		where $P_k$ denote a feature map of stride $2^k$ (i.e. downsampled by $2^k$ times)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item infer as 2-stage detection, with backbone network replaced by / extended to FPN 
		\item shared network head (e.g. RPN head) predict at each pyramid level
		\item assign RoIs to scale/pyramid level $k, P_k$ (similar to anchor assignment), \\
		with $k = \floor{k_0 + \log_2(\frac {\sqrt{w_rh_r}} {244})}$,
		\begin{align*}
		\text{where } & 244 \text{ the canonical imagenet pretraining size} \\
		& k_0=4 \text{ the level to which RoI of } w\times h=244^2 \text{ should be assigned} \\
		& \frac {\sqrt{wh}}{244} \text{ the downsample rate } (\log_2\text{ as stride-2 downsample})
		\end{align*}
		$\Rightarrow$ assign large RoI to low-res (e.g. $P_5$), small RoI to high-res (e.g. $P_2$)
		\item 7x7 RoI pooling for each RoI at its scale level
		\item prediction made by a pred head shared across all levels
		\item convert pred at each scale/pyramid level, using anchor assigned to that level
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item downsample (bottom-up): downsample at each ResNet stage \\
		$\Rightarrow$ collect last layer of each stage $\{C_2,C_3,C_4,C_5\}$
		\item upsample (top-down): interpolation (e.g. nearest neighbor, bilinear) \\
		$\Rightarrow$ results in corresponding $\{P_2,P_3,P_4,P_5\}$ (generate $P_5$ first)
		\item skip connection: 1x1 conv to match channel to $d=256$ \& element-wise adding \\
		$\Rightarrow$ residual connection to associate across resolution \& semantic level
		\end{itemize}
	\item Training
		\begin{itemize}
		\item 15 anchors $=\{32^2, 64^2, 128^2, 256^2, 512^2\}\times\{$1:2, 1:1, 2:1$\}$ \\
		(pos-neg assignment as in Faster R-CNN, i.e. RPN) \\
		$\Rightarrow$ assigned to $\{P_2,P_3,P_4,P_5\}$ according to scale \\
		(e.g. $P_2$ accounts for $32^2$ anchor, $P_5$ accounts for $512^2$ anchor)
		\item associate label box to anchor as usual (thus to each pyramid/scale level)
		\item train as Fast/Faster R-CNN
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item provide both high-resolution \& high-level semantic info from a single scale input \\
		$\Rightarrow$ provide feature pyramid with rich semantic info \\
		(dedicated feature map for each scale) \\
		(instead of a single high- res, high-level feature map)
		\item prediction independently made at each scale \\
		(analogs to using image pyramid: easily generate to more/less-level pyramid) \\
		$\Rightarrow$ \underline{alleviate the mismatch between: RPN receptive field \& actual object size}
		\item $\Rightarrow$ explicitly \underline{address multi-scale problem by design} \\
		(effective pyramid from learning) \\
		$\Rightarrow$ much better in detection small object
		\item remove the need of image pyramid at test time \\
		(still needed as deep feature / anchors are not enough for multi-scale problem) \\
		$\Rightarrow$ much faster inference \& consistent train-test
		\item yet, \underline{still slow inference in industrial deploymen}t: slow post-processing  \\
		(too many pred due to large final feature map \& per-box post-processing)
		\item skip connection
			\begin{itemize}
			\item residual adding (v.s. concat in U-net) to ensure effective depth increase
			\item mitigate the large semantic gap between resolutions (in downsample path)
			\item fix the location of upsampled feature
			\end{itemize}
		\end{itemize}
	\end{itemize}

\item Cascade R-CNN
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item same as Faster / Fast R-CNN: encode as the adjustment on proposals
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item extract feature map from backbone (e.g. ResNet)
		\item stage-0 (RPN head) generates pred boxes
		\item sample pred boxes into pos-neg (class-specific) examples according to IoU threshold \\
		$\Rightarrow$ construct proposals from pred of stage $t-1$
		\item stage-$t$ head (classifier \& regressor) predict (with RoI pooling) based on previous proposals
		\item ensemble pred for all stages with $t\ge1$, i.e. except RPN \\
		(as NOT all proposals can pass to the last stage: can get filtered out by classifiers)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item shared backbone (e.g. ResNet, FCN)
		\item RPN head to generate initial proposal
		\item cascaded multiple R-FCN head for class-specific classification \& regression
		\item comparison: \\ 
		\includegraphics[width=0.8\linewidth, center]{"./Deep Learning/plot/topic-objdet cascade r-cnn".png}
		where "pool" the roi pooling, "B0" the proposals, "H" the network head
		\end{itemize}
	\item Training
		\begin{itemize}
		\item loss at stage $t$: \\
		$\displaystyle L(\mathbf b^t, \mathbf g) = L_\text{cls}(h_t(x^t), y^t) + \lambda [y^t\ge 1] L_\text{loc}(f_t(x^t, \mathbf b^t), \mathbf g)$,\\
		\abovedisplayskip=2pt\abovedisplayshortskip=2pt~\vspace*{-\baselineskip}
		\begin{align*}
		\text{where } & h_t, f_t \text{ the stage } t \text{ head for classifier, regressor} \\
		& \mathbf b^t=f_{t-1}(x^{t-1}, \mathbf b^{t-1}), \text{ with } x^t \text{ the image (feature map) patch cropped by } \mathbf b^t \\
		& \mathbf g \text{ the corresponding label box of current examples} \\
		& y^t \text{ the class label for } \mathbf b^t \text{, considering IoU}^{\mathbf b^t}_{\mathbf g} \text{ under the threshold }u^t \\
		& \text{(} y^t= \text{class of } \mathbf g \text{ if IoU}>u^t \text{; else 0)}
		\end{align*}
		\item $\displaystyle \Rightarrow L=\sum_{t} L(\mathbf b^t, \mathbf g)$, with $\{u^t\}=\{0.5, 0.6, 0.7\}$ for $t\ge1$ \\
		(stage-0 is RPN: use its default sampling strategy)
		\item sequential, stage-by-stage training: improve based on a fixed input distribution
		\end{itemize}
	\item Analysis: Ineffectiveness of Single Detector - Performance Perspective
		\begin{itemize}
		\item optimization of different points of ROC requires different loss function \\
		$\Rightarrow$ detector performs best
			\begin{itemize}
			\item with the proposals IoU close to its training IoU threshold (left)
			\item when evaluated under IoU threshold close to its training IoU threshold (right)
			\end{itemize}
		\includegraphics[width=0.8\linewidth, center]{"./Deep Learning/plot/topic-objdet cascade r-cnn performance-iou plot".png}
		\item $\Rightarrow$ \underline{detector performs best when NO train-test mismatch} \\
		i.e. can NOT be best
			\begin{itemize}
			\item for proposal of all input IoU
			\item under all evaluation IoU threshold
			\end{itemize}
		\end{itemize}
	\item Analysis: Ineffectiveness of Single Detector - Data Distribution Perspective
		\begin{itemize}
		\item quality: the threshold $u$ for IoU$^\text{proposal}_\text{label}$ to consider a proposal to be positive \\
		\abovedisplayskip=2pt\abovedisplayshortskip=2pt~\vspace*{-\baselineskip}
		\begin{align*} \Rightarrow \text{ quality of detector } &= \text{ the IoU threshold } u \text{ under which it is trained} \\
		&= \text{ the quality of its input proposals} \end{align*}
		\item increase quality leads to exponential decrease in positive proposal num \\
		$\Rightarrow$ delimma: 
			\begin{itemize}
			\item low quality allows close false positive $\Rightarrow$ noisy positive
			\item high quality allows little positive $\Rightarrow$ overfit the positive \& extreme imbalance \\
			(also need to operate on low-quality proposal at test time)
			\end{itemize}
		\includegraphics[width=0.8\linewidth, center]{"./Deep Learning/plot/topic-objdet cascade r-cnn distribution at iou thr".png} \\
		where red number the positive percentage of examples higher than that IoU \\
		$\Rightarrow$ \underline{distribution at each IoU quality are different}
		\end{itemize}
	\item Analysis: Effectiveness of Cascading
		\begin{itemize}
		\item observe that output IoU are better than input IoU (if detector operates on its quality) \\
		$\Rightarrow$ enable boostrapping
		\item each stage improves the average IoU (quality) of all proposals \\
		$\Rightarrow$ more balanced dataset under higher quality (for the next stage) \\
		(as proposals are improved by previous stages)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item v.s. iterative bbox regression
			\begin{itemize}
			\item not a post-processing process, but involved in training
			\item use specialized regressor, instead of the same one applied multiple times
			\end{itemize}
		$\Rightarrow$ each regressor operates on its training quality \& distribution: no mismatch
		\item v.s. integral loss
			\begin{itemize}
			\item not sharing regressor: each head a complete detector
			\item avoid operating on the same input distribution \\
			(resample by producing new proposal for next stage)
			\end{itemize}
		$\Rightarrow$ more balanced pos-neg for classifier at each IoU threshold
		\item sequentially resample \& improve proposals to higher quality \\
			\begin{itemize}
			\item each stage optimizes for a specified IoU quality: \underline{specialized pred head}
			\item each stage improves the distribution of higher quality to be \underline{more balanced}
			\end{itemize}
		$\Rightarrow$ sequentially more selective against close false negative \\
		$\Rightarrow$ enable \underline{precise detection} at high quality (i.e. high IoU threshold)
		\item input distribution of $t$ stage is ensured by $t-1$ stage \\
		$\Rightarrow$ NO train-test mismatch
		\item multi-stage \& multi-task: stage-1 already improves over original Faster R-CNN \\
		(multi-tasking at various IoU threshold) \\
		$\Rightarrow$ end-to-end model ensemble
		\item more stages lead to better AP at higher IoU threshold \\
		(yet, 3-stage achieves a good trade-off)
		\end{itemize}
	\end{itemize}

\item Deformable Conv Net (v1)
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item same as Fast / Faster R-CNN
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item backbone (with deformable conv) extracts feature \& RPN predicts proposals
		\item RoI pooling: sampling location is also augmented by learnt offset \\ 
		(similar to \hyperref[DL_Layers_DeformableConv]{deformable conv})
		\item 2nd-stage produce final pred
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item ResNet-101 with last 3 layers (in the $5^\text{th}$/last stage) replaced by deformable conv \\
		$\Rightarrow$ performance gain saturate if using deformable conv in $\ge3$ layer
		\item deformable RoI pooling (could also be deformable ps-roi pooling) \\
		$\Rightarrow$ another roi produce normalized offset $\Delta \hat{\mathbf p}_{ij}=\frac 1 \gamma (\frac{\mathbf p_{ij}[x]}{w}, \frac{\mathbf p_{ij}[y]}{h})$, \\
		where $\gamma=0.1$ a scalar, $w$-$h$ the size of roi, $\mathbf p_{ij}$ the offset for $(i,j)$-th bin
		\end{itemize}
	\item Training
		\begin{itemize}
		\item similar to Faster R-CNN
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item more in \hyperref[DL_Layers_DeformableConv]{deformable conv}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Singe-Pass Detection}
\begin{itemize}
\item Dense Box
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item for $i^\text{th}$ output with location $(x_i, y_i)$ on output volume, output vector $t = \{s, dx_t, dy_t, dx_b, dy_b\}$, \\
		$\Rightarrow \begin{cases} (x_t, y_t) = (x_i+dx_t, y_i+dy_t), \\ (x_b, y_b) = (x_i+dx_b, y_i+dy_b) \end{cases}$
		\item $\Rightarrow$ output bbox with top-left point $(x_t, y_t)$ \& bottom-right point $x_b, y_b$ \\ 
		(under output feature map coord \& with $s$ the confidence score)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item VGG $\rightarrow$ upsample 
		\item image pyramid at test time
		\item convert output volume into bbox \& NMS at 0.5 for face detect / 0.75 for car
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item VGG19 based FCN: downsample \& bi-linear upsample (with concat to fuse feature before\&after)
		\item each head (cls, loc, landmark) consists of two 1x1 conv \\
		(localization head may use the concat of cls \& landmark output volume as input map)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item localization (regression) label: project each output vector back to input image
		$\Rightarrow$ each output vector uses its nearest label bbox center to generate $dx_{t/b}, dy_{t/b}$
		\item classification label: project each label bbox to output map \\ 
		$\Rightarrow$ for each box, label its center as positive, with a radius of $r_c$ \\
		where $r_c = 0.3\times$ box size, the pixel distance on output map \\
		(with objectness label map initialized to be 0)
		\item to avoid compute on background: crop patch with label box roughly at the center \\
		$\Rightarrow$ bbox within 1.25 scale range w.r.t. central box is positive; ignore otherwise \\
		(leave as 0 on classification label map) \\
		$\Rightarrow$ ensure each location reponsible for its own central area (with enough receptive fields)
		\item landmark label: only consider the landmarks of central target
		\item ignore other samples/predictions on the margin ($\le 2$ pixel distance) of positive label
		\item pos-neg balance: select all positive labeled location \& keep pos-neg ratio = 1:1 \\
		$\Rightarrow$ select from not non-ignored \& negative samples (random + hard neg mining)
		\item $\Rightarrow$ optimization goal $L=\sum_{i}M_i \left( L_\text{cls}^i + \lambda_\text{loc}[y_i>0]L_\text{loc}^i +  \right) + \lambda_\text{landmark} L_\text{landmark}^i$, \\
		where $M_i$ indicating if the sample ignored, $y_i>0$ indicating if being positive \\
		with $L_\text{cls}, L_\text{loc}, L_\text{landmark}$ all being L2 loss
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item small network stride (x4) \& select samples only with enough receptive fields \\
		$\Rightarrow$ able to detect small \& occluded object
		\item boost from landmark localization (as further constraint on bbox reg)
		\item anchor free in segmentation fashion
		\item L2 loss: optimize as 4 independent variables (yet could use IoU score to improve)
		\end{itemize}
	\end{itemize}

\item You Only Look Once (YOLO)
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item grid the input image into disjoint $S \times S$ cells \\
		(apply usually fine grid, e.g. $S=19$)
		\item assign each object to a cell by its central point \\ 
		$\Rightarrow$ try ensuring maximal $\mathbf 1$ object in a grid
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $conf=p(\text{obj}) \times \text{IoU}^\text{label}_\text{pred}$ confidence of current box containing an object \\
		$\Rightarrow 0$ desired if none, IoU between pred and label desired if exist \\
		where, $p(\text{ob}j)$ the probability of object existence in the CELL
		\item $x, y \in [0,1]$ the box center in the cell, normalized by cell size \\
		$\Rightarrow$ easier to learn, as dense layer not confused by varying locations
		\item $h, w \in [0, 1]$ the box size normalized by image size \\
		$\Rightarrow$ easier to learn for dense layer
		\end{itemize}
	\item Classification Encoding
		\begin{itemize}
		\item $p_1,...,p_C$ the conditional probability $p(\text{class}_c|\text{obj})$ for object inside the CELL \\
		i.e. the class probability of object, given object existing \& assigned to the cell
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item for each cell, predict both $[p_1,...,p_C]$ and $B$ bounding boxes $[conf, x, y, w, h]$ \\
		$\Rightarrow$ final prediction as a $S\times S \times (C+5B)$ tensor
		\item $p_c\cdot conf = p(\text{class}_c|\text{obj})\cdot p(\text{obj})\cdot \text{IoU}^\text{label}_\text{pred} = p(\text{class}_c, \text{obj}) \cdot \text{IoU}^\text{label}_\text{pred}$ \\
		where $p_c$ from the cell, $conf$ from each box in the cell \\
		$\Rightarrow$ combine the decoupled pred for various class-specific boxes prediction
		\item non-max suppression to fix multiple detection box (at test time)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item $24$ conv layer downsampling from $448\times448\times3$ to $S\times S\times 1024$
		\item followed by: $\rightarrow$dense with dropout$\rightarrow4096\rightarrow$dense$\rightarrow S\times S\times (C+5B)$ \\
		(dropout to prevent co-adaption, i.e. dependent units between 2 dense layers)
		\item leaky ReLu $a=\max(z,0.1z)$ used, except for linear activation in final layer
		\end{itemize}
	\item Optimization Goal
		\begin{itemize}
		\Item \begin{align*} \displaystyle \text{loss} = & \lambda_\text{obj}\sum_{i=0}^{S^2}\sum_{j=0}^{B} \mathbf{1}^{obj}_{ij} [(x_i-\hat x_i)^2 + (y_i-\hat y_i)^2] +\\
		& \lambda_\text{obj}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbf 1^{obj}_{ij} [(\sqrt{w_i}-\sqrt{\hat w_i})^2 + (\sqrt{h_i}-\sqrt{\hat h_i})^2] + \\
		& \sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbf 1^{obj}_{ij} (conf_{ij} - \hat {conf}_{ij})^2 + \lambda_\text{none}\cdot\mathbf 1^{none}_{ij} (conf_{ij} - \hat {conf}_{ij})^2 +\\
		& \sum_{i=0}^{S^2}\mathbf 1^{obj}_{i}\sum_{c=0}^C (p_{ci} - \hat p_{ci})^2 \end{align*}
		\item $\lambda_\text{obj} = 5$ to up-weight box with obj, $\lambda_\text{none}=0.5$ for empty box
			\begin{itemize}
			\item balance the dataset: avoid large num of empty box pushing $conf$ pred to $0$
			\item emphasize on predicting box responsible for object \\
			(adjust its pred $x,y,w,h$)
			\end{itemize}
		\item $\mathbf{1}^{obj}_{ij}$ the indicator: $1$ if obj exists in $j$ box of $i$ cell, similar for $\mathbf 1^{none}_{ij}$
		\item minimize $\sqrt{w_i}, \sqrt{h_i}$: mitigate various box size into similar scale \\ 
		(as same error occurred in small box matter more than that in big box)
		\item $p_{ci}$ the classification prob for $i$ cell (only trained if it contains obj)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item optimize the sum-squared error (due to its simplicity)
		\item for each cell: correct its classification if it contains obj
		\item for each box:
			\begin{itemize}
			\item increase $conf$ \& adjust box pred if it overlaps the most with label of its cell
			\item decrease $conf$ for other box (not responsible for having obj)
			\end{itemize}
		\item pre-training: pretrained first 20 conv layers on classification dataset
		\item data augmentation: random scaling size, adjusting exposure and saturation
		\end{itemize}
	\item Limitation
		\begin{itemize}
		\item use only coarse feature to predict box \\ 
		(due to multiple downsampling \& produced by dense layer)
		\item NOT able to handle: multiple objects of the same type in a cell \\
		(NOR when same-type objects more than total pred boxes) \\
		$\Rightarrow$ suffer from small objects in group (e.g. birds)
		\item struggle in box localization
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item bounding box localization \& classification as regression problem \\
		(predicted totally by a dense layer)
		\item end-to-end optimizing directly the detection score \\
		(vs. separate modules: region proposal, bounding box localization, classification)
		\item global information for each bounding box localization \& classification \\
		(instead of taking only local info to classify boxes) \\
		$\Rightarrow$ less false alarm (mistake background as object)
		\item generalizable representation for detection \\
		(as feature extraction embedded in the network) \\
		$\Rightarrow$ able to generalized to art work
		\item much less predicted box (compared with ~2000 boxes in R-CNN framework)
		\item fast $\Rightarrow$ ensemble with (fast) R-CNN framework with no overhead \\
		$\Rightarrow$ mitigate the false alarm in R-CNN framework
		\end{itemize}
	\end{itemize}

\item YOLOv2, YOLO9000 \label{DL_CV_Objdet_YOLOv2}
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item no more explicit grid, implicitly set by downsampling factor \\
		(similar to spatial pyramid pooling)
		\item select $N_A$ anchor boxes: covering most of the interested objects \\
		(e.g. tall-thin box for pedestrian, low-wide box for car)
		\item assign the objects to a tuple (cell, anchor/prior box)
			\begin{itemize}
			\item assign to cell by its central point $\Rightarrow$ try ensuring maximal $N_A$ object in a cell
			\item assign to one from $N_A$ anchor boxes depending on its IoU with all $N_A$ boxes \\
			(NOT assigned, if all $\text{IoU}^\text{\tiny label}_\text{\tiny anchor} < 0.5$)
			\end{itemize}
		\end{itemize}
	\item Anchor Box Selection (Prior)
		\begin{itemize}
		\item run K-means on all label box in train set \\
		with distance $\text{dist}(\text{box}, \text{centroid}) = 1 - \text{IoU}(\text{box}, \text{centroid})$
		\item use cluster centroid as the anchor box \\
		$\Rightarrow$ for anchor box $a\in A, a = [w_a,h_a]$
		\item num of anchor box (centroid) $N_A$ reflect recall $\leftrightarrow$ model complexity trade-off
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $t_x,t_y \in (0,1)$ the box center in the cell, normalized by cell size \\
		$\Rightarrow$ true pred center $x=c_w\sigma(t_x) + c_x, y=c_h\sigma(t_y) + c_y$ where $c_x,c_y,c_w,c_h$ the cell \\
		$\Rightarrow$ easier to learn, as YOLO
		\item $t_w,t_h$ coefficient to adjust anchor box size \\
		$\Rightarrow$ true pred size $w=w_a{\rm e}^{t_w}, h=h_a{\rm e}^{t_h}$ where $w_a,h_w$ the anchor box size \\
		$\Rightarrow$ more robust, avoid influence from anchor prior
		\item $p_1,...,p_C$ classification pred $\Rightarrow$ decouple from cell, each box a classification
		\end{itemize}
	\item Classification with Hierarchical Encoding
		\begin{itemize}
		\item tree-structure for multi-label: \hyperref[DL_Layers_Hisoftmax]{a hierarchical softmax} \\ 
		(hence, NOT assuming mutual exclusion between class)
		\item lookup WordNet for class label \& its path to the selected root "physical object" \\
		(if multi-path, choose the one with least new word involved) \\
		$\Rightarrow$ build a WordTree
		\item multiple softmax: one for each set of direct children of a node (if not leaf) \\
		e.g. at node "dog": predict $p(\text{malamute dog}|\text{dog}), p(\text{terrier dog}|\text{dog}), ...$ \\
		$\Rightarrow$ pred a Bayesian network, according to WordTree
		\item $\Rightarrow t_o$ objectiveness by summing conditional prob accordingly \\
		$\Rightarrow$ true objectiveness: $conf = p(\text{obj})\cdot \text{IoU}^\text{label}_\text{pred} = \sigma (t_o)$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item for each cell, $N_A$ bounding boxes: $[\text{spatial info}[t_x, t_y, t_w, t_h], \text{ a flatten WordTree}[p_1,...,p_C]]$ \\
		$\Rightarrow$ final prediction as a $\frac I{32} \times \frac I{32} \times N_A(4+C)$ tensor (each box a WordTree!)
		\item objectiveness of each box: from WordTree of each box
		\item box classification: choose highest confidence path at every node \\ 
		(till a specified threshold or a leaf node reached)
		\item non-max suppression to fix multiple detection box (at test time)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item fully conv net: downsample by $32$, directly predict $\frac I{32}\times \frac I{32}\times (C+4N_A)$ tensor
		\item \hyperref[DL_Block_Res_Passthrough]{passthrough layer} with concat ($1\%\uparrow$) \\
		e.g. spatially stack $26\times26\times512$ into $13\times13\times1024$ before concat
		\item batch norm in ALL conv layer ($>2\%$ mAP$\uparrow$)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item pre-training on classification with data augmentation \\
		(e.g. random crop, rotation, hue, saturation and shifting shift)
		\item more time for model fine-tuning on detection dataset ($\sim4\%$ mAP$\uparrow$) \\ 
		as model takes time to adjust to resolution change
		\item multi-scale training s.t. model able to cope with varying resolution
		\item joint training: mix detection \& classification dataset ($\sim5\%\uparrow$) \\
		$\Rightarrow$ for detection label, full loss available \\
		$\Rightarrow$ for classification: only classification \& objectiveness loss considered
			\begin{itemize}
			\item select responsible pred box by the largest objectiveness
			\item objectiveness loss: considered if $t_o$ of responsible box $<$ threshold ($0.3$)
			\item classification loss: only nodes in its WordTree above current label considered \\
			with $p(\text{physical object})=1$ for label
			\end{itemize}
		\item balancing joint dataset: oversampling detection dataset \\
		$\Rightarrow$ s.t. detection data : classification data = 1:4 
		\end{itemize}
	\item Limitation
		\begin{itemize}
		\item each box a classification: too much output channel
		\item still, detectable objects number restricted by anchor box num in a cell \\
		(e.g. group of small objects)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item still, fast \& global context for box detection
		\item auto-select prior, better than handpicked \\
		(examined by using anchor box directly as prediction)
		\item adjustable speed$\leftrightarrow$accuracy trade-off via input resolution \\
		(enabled by being fully conv \& multi-scale trained) \\
		$\Rightarrow$ larger resolution, slower, more accurate
		\item joint training with WordTree: multi-task (detection + classification) learning \\
		$\Rightarrow$ enrich classes to detect \& more robust in class-detection \\
		$\Rightarrow$ graceful degrade on detecting new/unknown data
		\item decoupled localization \& classification \\
		$\Rightarrow$ able to handle some multiple same-type objects in same cell
		\end{itemize}
	\end{itemize}

\item YOLOv3
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item same as YOLOv2
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $t_x,t_y,t_w,t_h$: same as YOLOv2
		\item $t_o$ the objectiveness, modeling directly $p(\text{obj})$, instead of $p(\text{obj})\cdot\text{IoU}$
		\item $p_1,...,p_C$ the classification of obj: conditional prob $p(\text{class}_c|\text{obj})$, as YOLO
		\end{itemize}
	\item Classification Encoding
		\begin{itemize}
		\item independent logistic regression for each class (instead of hierarchical softmax) \\ 
		$\Rightarrow$ multiple binary classification (NOT assuming mutual exclusive classes) \\
		$\Rightarrow$ multi-label approach (box may contain more than one class)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item for each cell, $N_A$ bounding boxes $[t_x, t_y, t_w, t_h, t_o, p_1,...,p_C]$ \\
		($N_A$ the num of anchor box)
		\item $\Rightarrow$ final prediction as a $\frac I{N^l} \times \frac I{N^l} \times N_A(C+5)$ tensor \\
		where $N^l$ the size of downsampled feature map (at pred layer $l$)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item residual blocks \& batch norm as YOLOv2
		\item 2D encoder-decoder structure with concat connection, with $3$ pred branches \\
		$\Rightarrow$ final num of pred boxes $\displaystyle\sum_{l\in\text{pred layer}}(N^l\times N^l\times N_A)$
		\begin{figure}[ht]
		\includegraphics[width=0.9\linewidth, right]{"./Deep Learning/plot/topic-objdet onestage yolov3 architecture".png}
		\end{figure}
		\end{itemize}
	\item Optimization
		\begin{itemize}
		\item sum of the binary cross-entropy for classification loss
		\end{itemize}
	\item Training
		\begin{itemize}
		\item pretrain backbone (feature extractor) on classification
		\item multi-scale training \& data augmentation as YOLOv2
		\end{itemize}
	\item Failed Approach
		\begin{itemize}
		\item box $x,y$ as an offset to anchor box position $x_a,y_a$ (stability decreased)
		\item linear regression for $x,y$ (worse than bounded by logistic)
		\item focal loss (already solved by decoupled $p(\text{obj})$ \& $p(\text{class}_c|\text{obj})$)
		\item dual IoU threshold in assigning object (need more tunning for stabilized model)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item real-time applicable detector
		\item detection at multiple scale, with encoder-decoder structure
		\item decent localization + great box classification \\
		$\Rightarrow$ emphasize on box classification, since human insensible to IoU change
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Object Tracking}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Raw Data
		\begin{itemize}
		\item time-series observation from sensor $\Rightarrow$ need to embed detection
		\end{itemize}
	\item Object Representation
		\begin{itemize}
		\item bounding box: 2/3-D size, with pose, motion, velocity, etc.
		\item point model: centroid point with attributes denoting an object (including bbox)
		\item silhouette: for non-rigid object
		\item articulate: for articulated model, e.g. human skeletal model
		\end{itemize}
	\item Object Existence
		\begin{itemize}
		\item probability density for existence
		\item probability of existence as a feature in representation info \\
		(objectiveness in detection)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Trajectory
		\begin{itemize}
		\item recover the true trajectory of the object \\ 
		(including current location, potentially future prediction)
		\end{itemize}
	\end{itemize}
\item Overview
	\begin{itemize}
	\item Online Learning (Tracking)
		\begin{itemize}
		\item trade off speed - model complexity, as train on arriving frames
		\item adaptive, as accounting the history info of a track \\
		$\Rightarrow$ may provide more info (e.g. covariance matrix from Kalman filter)
		\end{itemize}
	\item Offline Tracking
		\begin{itemize}
		\item learn similarity functions between frames offline
		\item fast, as no online training needed $\Rightarrow$ but NO explicit adaptive
		\item batch method: generate the track after all frames examined
		\end{itemize}
	\item Tracking-by-Detection
		\begin{itemize}
		\item detect target(s) in each frame; link target into track \\
		$\Rightarrow$ as a two-stage problem
		\item $\Rightarrow$ viewed as meta learning / one-shot learning \\
		(train on first frame, detect on subsequent frames)
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Appearance Model
		\begin{itemize}
		\item robust target-specific model
		\item shape\&color can be trap: changing clothes etc.
		$\Rightarrow$ learning directly transformation by image pair (e.g. siamRPN)
		\end{itemize}
	\item Long Term Dependency
		\begin{itemize}
		\item re-identification: people leaving and re-entering the scene
		\item hot standby surveillance
		\end{itemize}
	\item Initialization \& Termination Criteria
		\begin{itemize}
		\item bad initialization: overlapped box due to overlapped object \\
		(e.g. two closely standing people)
		\item termination vs. occlusion \\
		$\Rightarrow$ need to tell leaving the view or temporally occluded
		\end{itemize}
	\item Fast Motion
		\begin{itemize}
		\item large search area, hence more background noise, hence similarity can fail
		\item motion blur (which can be modeled as appearance change)
		\end{itemize}
	\end{itemize}
\item Evaluation Metrics
	\begin{itemize}
	\item Success
		\begin{itemize}
		\item overlap success rate: average pred-label box IoU
		\item overlap success plot: the rate of pred-label box IoU $\ge$ a given threshold \\
		$\Rightarrow$ can have AUC
		\item orientation success: diff of pred-label yaw angle $\le$ threshold $=\ang{10}$
		\item other success... \\
		$\Rightarrow$ review the overall estimated quality
		\end{itemize}
	\item Normalized Cumulative Sum of Success vs Normalized Time
		\begin{itemize}
		\item a plot $\Rightarrow$ review tracking quality over time
		\end{itemize}
	\item Trajectory Difference
		\begin{itemize}
		\item compare the similarity of pred-label trajectory \\
		(may consider abrupt change, slowly drift, and etc.)
		\end{itemize}
	\item VOT Metrics
		\begin{itemize}
		\item robustness: failure times (frame cnt of lost track)
		\item accuracy: average overlap while tracking successfully
		\item expected average overlap (EAO): accounts for both accuracy \& robustness
		\end{itemize}
	\end{itemize}
\item Dataset
	\begin{itemize}
	\item Video (Single) Object Tracking
		\begin{itemize}
		\item Youtube-BB
			\begin{itemize}
			\item $>100,000$ videos, annotated once every $30$ frames
			\end{itemize}
		\item ILSVRC (ImageNet) VID
			\begin{itemize}
			\item $\sim4000$ videos annotated frame-by-frame
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item State-of-the-Art
	\begin{itemize}
	\item Data Balance
		\begin{itemize}
		\item avoid simple negative (foreground-background in siamese tracker) \\
		$\Rightarrow$ use negative box with semantic instance (discussed in DaSiam) \\
		$\Rightarrow$ cascaded RPN to filter out hard negative
		\item triplet loss: mine the relation of (template, positive instance, negative instance) \\
		$\Rightarrow$ focus on making correction decision on hard neg
		\item scale balance: having enough detail for semantic similar distractor \\
		cascaded RPN: feature-transform-block for multi-scale feature \\
		siamgrpn++: multi-level feature (enough diversity from deepnet) \\
		(yet, in siamRPN, model overfits to object scale)
		\end{itemize}
	\item Deep Power
		\begin{itemize}
		\item revealed to be central bias in deepnet, due to padding \\
		(network can realize input patch location by if feature is padding influenced) \\
		(padding on template prevent xcorr from effectively measuring similarity) \\
		$\Rightarrow$ siamRPN++: cropping for template branch \& data augmentation for search branch \\
		$\Rightarrow$ siamDW: crop-inside unit: as early as where the padding is introduced \\
		\item worse localization due to large receptive field \& accumulated stride
		\end{itemize}
	\item $\Rightarrow$ Discriminative Ability
		\begin{itemize}
		\item deep feature via deepnet
		\item mining pos-neg for better discriminative ability
		\item use background info in tracking stage, instead of only in training phase \\
		(introducing optimization approach?)
		\end{itemize}
	\item Finer Detail
		\begin{itemize}
		\item instance tracking via multi-tasking with video segmentation
		\item $\Rightarrow$ attention based cropping?
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Detection in Tracking}
\begin{itemize}
\item Single-frame Detection
	\begin{itemize}
	\item \hyperref[DL_CV_Objdet]{Bounding Box Detection}
		\begin{itemize}
		\item YOLO, R-CNN, etc.
		\end{itemize}
	\item Point Detection
		\begin{itemize}
		\item detect landmarks
		\end{itemize}
	\item Background Modeling
		\begin{itemize}
		\item segmentation, ...
		\end{itemize}
	\end{itemize}
\item Temporal Detection
	\begin{itemize}
	\item Optic Flow
		\begin{itemize}
		\item able to represent non-rigid, deformable object
		\item yet, may failed in moving foreground (e.g. birds, fog, smoke...)
		\end{itemize}
	\item Motion Detection
	\item Orientation
		\begin{itemize}
		\item frame differencing of location, ...
		\end{itemize}
	\item Background Modeling
		\begin{itemize}
		\item adaptive background, ...
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Single-Object Tracking}
\begin{itemize}
\item Kalman Filter
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item predict covariance $\Rightarrow$ better gating in association
		\item cooperate with noise \\
		$\Rightarrow$ able to modified to account association uncertainty
		\end{itemize}
	\end{itemize}

\item Particle Filter

\item Correlation Filter
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item online-training to learn the object appearance at previous frame \\
		$\Rightarrow$ conv over test (current) frame for similarity response
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item no explicitly modeled, though can augment online-training data with past images
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item $\mathbf w$ the weight to learn $\mathbf x$ input image of current frame, $y$ the desire response
		\item $\mathbf x[\tau]$ the circular shift of $\mathbf x$ by $\tau=[\tau_x, \tau_y]$ pixel
		\item formulated as regression problem \\ 
		$\Rightarrow \displaystyle \arg_{\mathbf w}\min\frac 1 2\sum_{n\in N}\norm{\mathbf x[\tau_n]^T\star \mathbf w - y_{\tau_n}}^2 + \frac \lambda 2 \norm{ \mathbf w}^2$ \\
		(mosse: minimum output sum of squared error, with regularization)
		\item original solution for regression $(X^T X + \lambda I)^{-1}$, where $X=\{\mathbf x[\tau_n]\mid n = 1,...,N\}$ \\
		(corresponding $\mathbf y = \{y_{\tau_n}\mid n =1,...,N\}$) \\
		$\Rightarrow \mathcal O(N^3)$, where $N$ the num of training example in $X$
		\item since $\mathbf x[\tau_n]$ a circulant toeplitz matrix $\Rightarrow$ fourier transform into diagonal matrix
		\end{itemize}
	\item 
	\end{itemize}

\item GOTURN: 
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item CNN
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item no explicit tracker (not modeling temporal info)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop $t^\text{th}$ image: centered at current bounding box with context
		\item crop $t+1^\text{th}$ image: centered at bbox in $t$, doubled size (gating)
		\item encode both image via CNN detector; concat encoding \& feed to dense layer
		\item regress the bbox in $t+1^\text{th}$ crop
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item arbitrary target object selected for tracking (treated as ground truth)
		\item $1$-step prediction of location, then predict further on...
		\end{itemize}
	\item Training
		\begin{itemize}
		\item train with consecutive image pair froxm sequences
		\item $L1$ loss for exact match, as it does NOT smooth out in $(0^-, 0^+)$ as $L2$ loss \\ 
		$\Rightarrow$ mitigate the introduced motion noise
		\item data augment: motion noise applied on the $2^\text{th}$ image; random crop on the $1^\text{st}$\\ 
		(motion noise $\sim$ Laplace distribution and prefer smooth \& small motion)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $2$-frame model as a simple local generic detector, no explicit temporal tracking \\
		$\Rightarrow$ learn to locate the nearest similar object by comparing $t-1,t$ frame \\
		$\Rightarrow$ learn to discover relationship between object appearance \\ 
		(by the Siamese setting) \\
		$\Rightarrow$ refine bbox for nearest similar object with one proposal ($t-1$ bbox)
		\item $\Rightarrow$ able to specialize tracker for specific object tracking \\
		e.g. fine-tune on car video for car tracker
		\item can NOT possess intrinsic invariance of object movement at various direction \\
		$\Rightarrow$ detector needs to be trained by data augmentation for object at all positions \\
		(due to dense layer)
		\item can NOT handle large drastic target change: preferring local \& smooth change 
		\item Not good at exact frozen scene: training augmented with noisy motion \\
		(could be worse if NOT using $L1$ loss)
		\item real-time timing ($100$fps), included in opencv $\Rightarrow$ stable performance
		\end{itemize}
	\end{itemize}

\item SiamFC: Fully-Conv Siamese Network for Object Tracking
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item siamese CNN to extract feature from exemplar \& search image
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item no explicit tracker, yet cross-correlation to measure spatial similarity
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item exemplar and search image cropped with target in the center \& feature extracted into $z, x$ \\
		(at most $T$ frames apart)
		\item conv exemplar $z$ on search image $x$ for response map of similarity
		\item maximum score in response map as object in cur frame \& fit a box accordingly
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item AlexNet as CNN
		\end{itemize}
	\item Training
		\begin{itemize}
		\item logistic loss for final response map from final cross-correlation \\
		(cosine window applied to penalize large displacements)
		\item weighted for pos-neg balance
		\item bbox encoded as a mask with a square area of $1s$
		\item image cropped with padding \& scaled to $127\times127$ \& $255\times255$ for convenience
		\item $50$ epochs ($50000$ examples per epoch), batch size $8$
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item crop with padding on box of $1^\text{st}$ frame, crop with $t-1^\text{th}$ box padded to $4\times$ size on frame $t$
		\item tracking with scale: processing multi-scaled version of search image ($5$ scale $1.025^{\{-2,-1,0,1,2\}}$)
		\item produce response map of similarity \& upsampled to $272\times272$ for better localization
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $2$-frame model: measuring spatial similarity by correlation for localization \\
		$\Rightarrow$ an unrolled RNN trained with length $2$ \\ 
		$\Rightarrow$ strong initialization for RNN-based tracker
		\item learning strong offline embedding for exemplar-search similarity measurement \\
		$\Rightarrow$ complementary to online tracker
		\item updating exemplar $z$ in time series NOT gaining much (as initial box assumed to be a great one)
		\end{itemize}
	\end{itemize}

\item Re$^3$: Real-time Recurrent Regression Network
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item CNN to extracts multi-scale representation from image \\
		(more descriptive info e.g. human in red/blue shirt) \\
		$\Rightarrow$ a siamese net as encoding
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item two-layer, factored LSTM, taking tracker input at both layer \\
		$\Rightarrow$ longer dependency \& more complex object transformation with 2 layers
		\item hidden state as tracker state \\
		$\Rightarrow$ forward-prop to update (no training)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop $t^\text{th}$ image: centered at current bounding box, extended to twice of box size
		\item crop $t+1^\text{th}$ image: centered at box of $t$, doubled size (gating)
		\item late fusion: concat CNN output from $t, t+1$ \& fed into dense layer for fusion
		\item LSTM tracker updates on fused info
		\item dense layer regress the tracker (LSTM) output for bounding box at $t+1$
		\end{itemize}
	\item Training
		\begin{itemize}
		\item bounding box defined by up-left, right-bottom coordinates in the crop \\
		(hence as a ratio of bounding box size)
		\item $L1$ loss to encourage exact match
		\item short sequence ($2$ unrolls) \& multi-batch ($64$) to overcome plateaus \\
		$\Rightarrow$ slowly increase to $32$ unrolls \& batch size $4$
		\item use ground-truth crop when training with short sequence \\ 
		$\Rightarrow$ slowly increase probability to use predicted crop, when increasing unrolls \\
		$\Rightarrow$ to prevent from accumulating drifts
		\end{itemize}
	\item Data Augmentation
		\begin{itemize}
		\item utilize detection dataset: crop the object with bounding box
		\item random crop a patch from the same image as background
		\item occluders randomly taken from the same image
		\item object with box initialized with velocity with Gaussian noise
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item an initial bounding box over arbitrary object given at the start
		\item crops fed into net at each time step
		\item LSTM state reset after each $32$ frames (as maximally trained with $32$ unrolls) \\
		$\Rightarrow$ hot-start by using the state in $1^\text{st}$ forward pass (instead of $\mathbf 0$) \\
		$\Rightarrow$ preserve the initial encoding \& recover from drift
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item end-to-end training for both detector and tracker
		\item model regress changes to the box ratio $\Rightarrow$ easier as bbox refinement
		\item LSTM tracker maintain track state $\Rightarrow$ temporal fusion \\
		$\Rightarrow$ overcome occlusion, update for variance/shape change \\
		(observation usefulness modeled by LSTM tracker)
		\item LSTM tracker needs specialized training, and single-layer LSTM NOT enough \\
		(or, can hurt performance due to instability)
		\item LSTM state reset: prevent drift, yet can fail if initial box overlaps other object
		\item may still, drifted to similar nearby object, known object (e.g. face), large motion \\
		(comparison \& failure: \url{https://youtu.be/RByCiOLlxug?t=214})
		\end{itemize}
	\end{itemize}

\item CFNet: Correlation Filter based Tracking (SiamFC v2)
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item siamese CNN 
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item correlation filter embedded in forward prop $\Rightarrow$ online-learning $1$-step tracker
		\end{itemize}
	\item Correlation Filter Block
		\begin{itemize}
		\item input training img $x\in \mathbb R^{m\times m}$, test img $z$
		\item $w\star x$ construct 
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop $t$ frame at box center of $t-1$ frame, with $4\times$ larger \& siamese CNN extracts feature from \\
		$\Rightarrow$ extracted last frame feature (train) \& cur frame feature (test) into $x, z$
		\item solving correlation filter optimization: get circular shift of $x$ by circular linear matrix $k$ \\
		$\Rightarrow$ via Lagrangian dual, turn optimization into linear equations system \\
		$\Rightarrow$ solve for equations system \& construct back prop map \\
		(can even learn the desired response $y$ instead of assuming it to be Gaussian)
		\item crop CF output \& convolve with test image $z$ to regress box
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item 
		\end{itemize}
	\item Training
	\item Understanding
		\begin{itemize}
		\item correlation filter embedded in forward prop \\ 
		$\Rightarrow$ utilize online-learning to acquire prior for target (while tracking generic object)
		\item enable ultra-lightweight network comparative with deep net \\
		(though correlation filter layer does NOT improve the ceiling performance)
		\item updating template on each frame (v.s. using always $1^\text{st}$ frame)
		\end{itemize}
	\end{itemize}

\item \textbf{SiamRPN: Tracking with Siamese Region Proposal Network}
	\begin{itemize}
	\item Tracking Initialization
		\begin{itemize}
		\item take initial frame \& given bbox as template $z$
		\item CNN (modified AlexNet) obtains a feature map $\varphi(z)$ \\
		(crop $(w+p)\times (h+p)$ with padding $p=\frac {w+h}2$ \& resized to $227\times227$)
		\end{itemize}
	\item Detector
		\begin{itemize}
		\item in $t>1$ frames, each frame as detection frame $x$
		\item same CNN (modified AlexNet) extracts feature maps into $\varphi(x)$\\
		(crop $2(w+p)\times2(h+p)$ \& resized to $255\times255$)
		\item $2$ independent \& identical branch in RPN: classification/regression branch \\
		$\Rightarrow$ conv to get $\zeta[\varphi(z)], \zeta[\varphi(x)]$ \& conv with $\zeta[\varphi(z)]$ as kernel over $\zeta[\varphi(x)]$ \\
		(with different output branch)
		\item use kernel from template branch to convolute on feature map of detection branch \\
		(same channel num) \\
		$\Rightarrow$ xcorr directly regress bbox based on response map \\
		(template branch has a following 1x1 conv to adjust channel)
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item tracking by (one-shot) detection
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item anchor box $x^\text{an},y^\text{an},w^\text{an},h^\text{an}$, ground truth box $x^\text{gt},y^\text{gt},w^\text{gt},h^\text{gt}$
		\item perfect prediction as $\delta x = \frac {x^\text{gt}-x^\text{an}}{w^\text{an}}, \delta y = \frac{y^\text{gt}-y^\text{an}}{y^\text{an}}, \delta w = \ln \frac {w^\text{gt}}{w^\text{an}}, \delta h = \frac{h^\text{gt}}{h^\text{an}}$ \\
		$\Rightarrow$ predict box as $x^\text{p}=\delta xw^\text{an} + x^\text{an}, y^\text{p}=\delta yh^\text{an} +y^\text{an}, w^\text{p}=e^{\delta w}w^\text{an}, h^\text{p}=e^{\delta h}h^\text{an}$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop on template \& search img \& extract feature volume
		\item take template volume as kernel, convolute on feature map of current frame \\
		$\Rightarrow$ RPN takes resulted response map as proposal \& regress bboxes
		\item select top $K$ response in classification branch as set of $\{(i_k,j_k,c_k)\mid k\in(0,K)\}$ \\
		$\Rightarrow$ use the location $\{(i_k,j_k,c_k)\}$ to obtain corresponding anchor \& pred \\
		$\Rightarrow$ generate $K$ proposal as $\{x_k^\text{pro}, y_k^\text{pro}, w_k^\text{pro}, h_k^\text{pro}\}$
		\item box refinement: only central $g\times g$ anchors remain considered \\ 
		(use the center size of search region)
		\item size change penalty $\displaystyle e^{k*\max(\frac {r_t}{r_{t-1}}, \frac {r_{t-1}}{r_{t}})*\max(\frac {s_t}{s_{t-1}}, \frac {s_t}{s_{t-1}})}$, \\
		where $r_t$ the bbox $\frac w h$ ratio of frame $t$, $s^2=(w+p)(h+p)$ with padding $p=\frac {w+h} 2$ \\
		$\Rightarrow$ bbox score (objectness/classification response) multiplied by penalty
		\item non-max suppression on re-ranked boxes for final bbox \\ 
		(target size updated by linear interpolation for smooth translation)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item arbitrary target object selected for tracking (treated as ground truth) \\
		$\Rightarrow$ extract the feature template $\varphi(z) \& \zeta[\varphi(z)]$
		\item each $t>1$ frame, perform feature extraction, detection with kernel $\zeta[\varphi()z]$ \& bbox regression \\
		$\Rightarrow$ formulate as task of a one-shot learning for detection
		\item gating on frame $t$ based on $t-1$ result
		\item detect target within gated region of frame $t$ \\
		(the best remained \& adjusted anchor as the target to track)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item crop input by $\sqrt{(w+p)*(h+p)}$, with padding $p=\frac{w+h}2$ \& resize to fixed size \\
		$\Rightarrow$ treat box as square \& has always the same percentage  target in the input \\
		(crop based on each label box: NOT using previous label box)
		\item cross-entropy loss for classification \& smooth $L1$ loss for regression
		\item pos-neg example: dual IoU threshold $(0,0.3,0.6)$
		\item batch: size $64$, with at most $16$ positive examples
		\item fine-tune only last $2$ conv layers in AlexNet
		\item dataset: image pair from imagenet VID \& youtube-BB
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $2$-frame model: tracking as one-shot detection (with response map of similarity) \\
		$\Rightarrow 1^\text{st}$ bbox as only examplar to learn the conv kernel in RPN \\
		(which is used in following frames for bbox proposal) \\
		$\Rightarrow$ Siamese net learns to efficiently map $z$ to weights $[\varphi (z)]$ (learning to learn) \\
		$\Rightarrow$ learning the transformation of target \\
		(hence update template has mere improvement)
		\item template branch to extract feature discriminate fore-/back-ground \\
		$\Rightarrow$ predict detection kernel by input $z$ and its weight in $[\varphi(\cdot)]$ as a meta learner
		\item detection branch (RPN) to propose \& refine a compact bbox \\ 
		(instead of online fine-tuning) \\ 
		$\Rightarrow$ no need for online learning, nor multi-scale test
		\item meta-learning / oneshot detection: predict the last kernel for regression/classification \\
		(separately)
		\item crop the kernel-branch: crop out edge case to avoid noise \\
		$\Rightarrow$ NOT able to handle large search region \\ 
		(use only limited center size in refinement)
		\item yet, using both VID \& youtube-bb dataset \\
		$\Rightarrow$ youtube-bb contain larger transform between frames \\
		(due to one labeled frame in every 30 frames) \\
		$\Rightarrow$ needs larger center-size \& context to overcome large transformation
		\item tracking based on \underline{similarity \& transformation \& semantic}
			\begin{itemize}
			\item similarity: from correlation
			\item transformation: from using $z$-$x$ pair from a large frame range ($\sim100$) \\
			(both template-search img are cropped by corresponding label box) \\
			(instead of box from previous frame as in Re$^3$) \\
			$\Rightarrow$ learning appearance transformation, instead of object motion
			\item semantic: from using detection net (RPN) \\
			$\Rightarrow$ anchor for obejct different $w$-$h$ ratio
			\end{itemize}
		\item fail in large \& fast moving object, potentail reason: \\
		large search area contains too much noise \& similarity get more response
		\item fail with different image crop: training with only a specific crop strategy \\
		(trained with a fix 255-255 crop, with a fixed ratio of region being target object) \\
		$\Rightarrow$ overfit the chosen scale \& NOT able to handle different scale \\
		$\Rightarrow$ not fully explore the discriminative ability from the semantic
		\item may fall back to simple detector if backbone can not extract meaningful template \\
		(potential reason: not fully trained, training set do not have hard negative, ...) \\ 
		by meaningful: discriminative enough for two similar object \\
		(need to able to recognize tiny feature: similar to requirement in re-ID)
		\item offline transformation + semantic learning to take the place of online learning \\
		$\Rightarrow$ NOT using background info of each video \\
		(use only foreground info via similarity)
		\end{itemize}
	\end{itemize}

\item DaSiamRPN: Distractor-aware siam network
	\begin{itemize}
	\item Analysis
		\begin{itemize}
		\item data imbalance in tracker training
		\item lack of diversity in positive: too few category in tracking dataset \\
		$\Rightarrow$ model can not generalize to real-generic object \& bias to known object
		\item most negative are sampled from background $\Rightarrow$ no semantic \\
		$\Rightarrow$ learn a simple fore-/back-ground classifier for objectness \\
		$\Rightarrow$ easily drift to arbitrary object \& high score even after target lost
		\end{itemize}
	\item Training
		\begin{itemize}
		\item introduce detection dataset \& use augmentation \\
		$\Rightarrow$ more diverse classes in positive examples
		\item use inter-class negative: semantic target from different class as negative \\
		$\Rightarrow$ avoid drifting to other object
		\item use intra-class: semantic target from same class as negative (diff track) \\
		$\Rightarrow$ force tracker to be able to realize fine-grained feature
		\item $\Rightarrow$ improve intra-/inter-class discriminative ability \& generalization ability \\
		$\Rightarrow$ tracker able to produce meaningful objectness score
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item inference as siamRPN: $17*17*5$ proposal \& apply non-max suppression
		\item select the best-scored proposal $z$
		\item collect distractors set $D = \{ \forall d_i,  f(d_i,z_t) > \text{thr} \wedge d_i \neq z_t \}$, \\
		where thr a pre-defined threshold, $f=\varphi(d_i)\star\varphi(z_t) +\mathbf b$
		\item collect top-$k$ scored proposal into $P$
		\item select $q$ from $P$ s.t. \begin{align*} \displaystyle q &= \arg\max_{p_k\in P} f(z,p_k)-\hat \alpha \frac{\sum_{i=1}^n\alpha_i f(d_i,p_k)}{\sum_{i=1}^n \alpha_i} \\ &= \arg\max_{p_k\in P} (\varphi(z) - \hat\alpha \frac{\sum_{i=1}^n\alpha_i \varphi(d_i)}{\sum_{i=1}^n\alpha_i}) \star \varphi(p_k) \text{by xcorr being linear operation} \end{align*}
		\item to online model distractors: incrementally learn the target templates \\ (inspired by associative law) \\
		$\Rightarrow \displaystyle q_{T+1} = \arg\max_{p_k\in P_T} (\frac{\sum_{t=1}^T\beta_t\varphi(z_t)}{\sum_{t=1}^T\beta_t} - \hat\alpha \frac{\sum_{t=1}^T\beta_t \sum_{i=1}^n\alpha_i \varphi(d_{i,t})}{\sum_{t=1}^T\beta_t\sum_{i=1}^n\alpha_i}) \star \varphi(p_k)$, where \\
		$beta_t$ the learning rate at time $t$, set to be $\sum_{i=0}^{t-1}\frac{\eta}{1-\eta}$ with $\eta=0.01$ \\
		$\Rightarrow$ model a online-updating classifier (by re-ranking proposals) \\
		(yet, also introduce a lot of hyper-params...)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item siamnet tracker
		\item long-term tracking by re-detecting: use model objectness to denote target lost/recovery \\
		$\Rightarrow$ larger search region if target lost (increase from $255$ to $767$)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item using negative pair with semantic object $\Rightarrow$ more discriminative \\
		$\Rightarrow$ model realize difference between similar object (both inter- \& intra- class) \\
		(instead of only foreground-background) \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-obj track DaSiamRPN".png} \\
		(bird: siamFC vs siamRPN vs DaSiamRPN)
		\item $\Rightarrow$ hence more robust \& meaningful score \\ 
		$\Rightarrow$ able to use score to denote lost-recovery in noisy \& big search region \\
		(otherwise, may pick up random object to track with high score) \\
		($\ge 10\%$ absolute improvement on most datasets...)
		\end{itemize}
	\end{itemize}

\item C-RPN: Siamese Cascaded Region Proposal Network
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item modified AlexNet for encoding (multi-scale info utilized) \\
		$\Rightarrow$ extracts info from search region $x$ with gating \\ 
		(target template $z$ already prepared) \\
		$\Rightarrow \phi_n(\cdot)$ for features at $n^\text{th}$ layer, backwards
		\item feature fusion $\Phi_l(\cdot) = f \left( \Phi_{l-1}, \phi_l \right)$, with $\Phi_1 = \phi_1$ \\
		$\Rightarrow$ recursively fuse semantic info with lower-level spatial infoz
		\item $f(\Phi_{l-1}, \phi_l)$: feature transfer block for fusion \\
		$\Rightarrow$ upsample $\Phi_{l-1}$ by deconv, further 2 convs on $\phi_l$ for channel matching \\
		$\Rightarrow$ element-wise sum, then interpolation as downsampling
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item 2-frame siamese net as tracker
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item same CNN extracts features from search region
		\item $l^\text{th}$ RPN convolute $\Phi_{l}(z)$ on $\Phi_{l}(x)$ \\
		$\Rightarrow$ regresses boxes based on response map \& anchor boxes $A_{l}$
		\item discard any boxes $\in A_l$ with confidence/objectness lower than a \underline{preset} threshold \\
		$\Rightarrow$ produce anchor boxes $A_{l+1}$
		\item fuse semantic info with lower-level info, for both branch for $x, z$
		\item $l+1^\text{th}$ RPN further refine anchors $A_{l+1}$
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item loss for $l^\text{th}$ RPN $\displaystyle L_l = \sum_{a\in A_{l}}L_\text{cls}(c^l_a, \hat c^l_a) + \lambda \sum_{a\in A_l} \hat c^l_a \cdot L_\text{loc}(r_a^l, \hat r_a^l)$, \\
		where $c^l_a$/$\hat c^l_a$ the predict/label objectness for anchor $a$; \\
		with  $r_a^l$/$\hat r_a^l$ the predict/label location for anchor $a$ (encoded as YOLOv2) \\
		note: label based on current anchor $a\in A_l$ and ground truth $\hat r_a$
		\item $\Rightarrow$ total loss $\displaystyle L = \sum_{l} L_l$
		\end{itemize}	
	\item Training
		\begin{itemize}
		\item random sample image from a video \\
		$\Rightarrow$ forming image pair (target template image, image with/without target)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multi-stage tracking: each RPN sequentially refine bbox (size \& location)
		\item hard negative mining by filtering out box proposal at each RPN stage \\
		$\Rightarrow$ training samples sequentially more balanced \\
		$\Rightarrow$ RPNs sequentially more discriminative
		\item $\Rightarrow$ hence, more discriminative between similar nearby object \\
		(compare with Re$^3$ \& SiamRPN)
		\item fusion of multi-level feature (spatial + semantic info) for RPN \\
		$\Rightarrow$ provide detail for semantic similar distractor
		\end{itemize}
	\end{itemize}

\item Triplet Loss in Siamese Tracker
	\begin{itemize}
	\item Tracker
		\begin{itemize}
		\item two-frame siamese net as tracker
		\item inference as siamese tracker: init \& track
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item triplet: exemplar (template) $z$, positive instance $x_i$, negative instance $x_j$,
		where positive/negative instance the search image with/without target
		\item matching prob: $\displaystyle prob(vp_i, vn_j)=\frac{e^{vp_i}}{e^{vp_i} + e^{vn_j}}$, \\
		where $vp_i / vn_j$ the predicted prob of $x_i/x_j$ to be pos-/neg-ative \\
		$\Rightarrow$ a softmax over prediction on pos-neg instance (with different search img)
		\item joint prob: $\displaystyle \frac {1}{MN} \prod_{i}^M\prod_j^N prob(vp_i, vn_j)$, where \\
		$M/N$ the number of instance in the positive/negative set
		\begin{align*} \Rightarrow \text{triplet loss } \displaystyle L_t &= - \frac 1 {MN} \sum_{i}^M\sum_{j}^N\log prob(vp_i, vn_j) \\
		&= \frac 1 {MN} \sum_{i}^M\sum_{j}^N \log(1+e^{vn_j-vp_i}) 
		\end{align*}
		\end{itemize}
	\item Training
		\begin{itemize}
		\item train another $10$ epoch with the triplet loss after the original training
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item scale the normal logistic loss (used due to binary classification) accordingly \\
		\begin{align*}
		\Rightarrow \displaystyle L_l &= \sum_i^M \frac 1 {2M}\log(1+e^{-vp_i}) + \sum_{j}^N \frac 1 {2N}\log(1+vn_j) \\
		&= \frac 1 {MN} \sum_i^M\sum_j^N \frac 1 2 (\log(1+e^{-vp_i}) + \log(1+e^{vn_j}))
		\end{align*}
		i.e. repeat $N$ times for $M$ positive instance; $M$ times for $N$ negative instance
		\item thus, logistic v.s. triplet lies in their term inside sum \\ 
		i.e. $\underbrace{\frac 1 2 (\log(1+e^{-vp_i}) + \log(1+e^{vn_j}))}_{T_l} \text{ v.s. } \underbrace{\log(1+e^{vn_j-vp_i})}_{T_t}$
		\item for logistic: $\frac {\partial T_l}{\partial vp} = \frac {-1}{2(1+e^{vp})}, \frac {\partial T_l}{\partial vn} = \frac {1}{2(1+e^{-vn})}$
		\item for triplet: $\frac {\partial T_t}{\partial vp} = \frac {-1}{1+e^{vp-vn}}, \frac {\partial T_t}{\partial vn} = \frac {1}{1+e^{vp-vn}}$
		\item $\Rightarrow$ gradient value: \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-obj track triplet loss vs logistic loss".png} \\
		$\Rightarrow$ triplet loss offers larger absolute gradient when $vp < vn$ (up-left triangular) \\
		$\Rightarrow$ logistic loss: focus on making $vp$ larger \& $vn$ smaller, separately \\
		($\frac {\partial T_l}{\partial vp} \rightarrow 0$ when $vp \rightarrow +\infty$, similar for $vn$) \\
		$\Rightarrow$ while, triplet loss: focus on having $vp > vn$ (to ensure the correct decision)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item mine the relationship between exemplar-positive-negative \\
		$\Rightarrow$ larger absolute gradient under wrong classification (predict $vp \le vn$) \\
		$\Rightarrow$ focus on correct prediction, instead of only high score
		\item form more training examples: num of examples increases from $M+N$ to $MN$ \\
		$\Rightarrow$ more importantly, form a more diverse dataset
		\item all model trained with triplet loss outperform its original logistic loss version \\
		(compared with training extra $10$ epoch with logistic loss) \\
		(though not outperforming in every scenario, e.g. low resolution)
		\end{itemize}
	\end{itemize}

\item Learning Discriminative Model Prediction for Tracking
	\begin{itemize}
	\item 
	\end{itemize}

\item SiamRPN++
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item deep backbone $\Rightarrow$ multi-level feature with enough detail-semantic diversity
		\item depth-wise separable correlation: xcorr on each corresponding channel-channel \\
		(similar to depthwise separable conv: 1x1 conv to replace summation) \\
		$\Rightarrow$ each semantic info measure similarity separately (more semantic) \\
		$\Rightarrow$ meanwhile, reduce parameters and thus faster convergence
		\item neck: conv-bn block to adjust \& align channel before xcorr \\ 
		(separate neck for classification - regression)		
		\item multi-RPN: corresponding to multi-level feature \\
		$\Rightarrow$ depthwise xcorr applied on each level feature \& each followed by an RPN \\
		$\Rightarrow$ (trainable weight) weighted average directly on RPN result
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item siamese RPN net tracker, with deeper backbone (e.g. resnet, rather than alexnet)
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item siamese tracker: correlation to measure similarity \\
		$\Rightarrow f(\mathbf z, \mathbf x) = \phi(\mathbf z) * \phi(\mathbf x) + b$, where $\phi(\cdot)$ map into embedded space, $b$ the bias
		\item $\Rightarrow$ need to have strict translation INvariance: $f(\mathbf z, \mathbf x[\Delta\tau_j]) = f(\mathbf z, \mathbf x)[\Delta\tau_j]$, where \\ $[\Delta\tau_j]$ the translation, i.e. shifting sub-window \\
		(i.e. conv kernel canNOT infer the input location by inspecting its feature)
		\item $\Rightarrow$ need to have structure symmetry: $f(\mathbf z, \mathbf x) = f(\mathbf x, \mathbf z)$
		\item YET, deep net involves too much $0$-padding, which creates unique response \\
		$\Rightarrow$ conv kernel can recognize if its input is from center-border-corner \\
		$\Rightarrow$ $0$-padding introduce border signal \& destroy the translation invariance \\
		(as network can now realize the input location from its content) \\
		(e.g. if/how the content is influenced by $0$-padding) \\
		$\Rightarrow$ introduce central bias (\underline{from in-model padding}) \\
		(as most targets labeled in center - a wrong cue learned for objectness) \\
		$\Rightarrow$ trained with shift augmentation \\ 
		(random shifting the crop region s.t. target not right in the center) \\
		$\Rightarrow$ ensure network realize: border signal is UNreliable cue for objectness \\
		$\Rightarrow$ destroy the central bias
		\item $0$-padding in template \& only border of search img get $0$ padded \\
		$\Rightarrow$ template with $0$-padding hard to match the center of search img \\
		(as only the border get padded) \\
		$\Rightarrow$ strong, yet wrong, signal for network to realize location \\
		$\Rightarrow$ central bias again! (\underline{introduced by xcorr}) \\
		$\Rightarrow$ crop out padding-influenced part from template \\
		(e.g. use only the central 7x7 of template branch as template)
		\item $0$-padding in preparing network input (input image patch) \\
		$\Rightarrow$ may get network realize location again (\underline{from outside-model op}) \\
		$\Rightarrow$ use the image average (channel-wise) instead of $0$ to pad
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item depthwise xcorr with deepnet: tracking with semantic - similarity \\
		$\Rightarrow$ extract useful semantic for similarity \\
		$\Rightarrow$ each channel captures some specific semantic (enforced by depthwise xcorr) \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-obj track rpn++ semantic in depthwise xcorr".png} \\
		(response of $xcorr(\phi(\mathbf z), \phi(\mathbf x))$ at different channel, only search image shown) \\
		$\Rightarrow$ better tracking result in \underline{hard case} (e.g. under high IoU threshold)
		\item depthwise xcorr: less params $\Rightarrow$ more balanced params number between
			\begin{itemize}
			\item template-search branch: avoid overfit/underfit template, as search branch are easier to be trained
			\item backbone-rpn: more stable
			\end{itemize}
		$\Rightarrow$ more stable training procedure
		\item \underline{deep tracker can learn central bias, due to padding \& target-centered cropping} \\
		$\Rightarrow$ network learn to locate target by wrong evidence (input patch location) \\ 
		(as padding introduces unique response on the border \& corner) \\
		$\Rightarrow$ shift augmentation \& template central crop \& average padding \\
		$\Rightarrow$ force network to be location invariance \& ignore response from padding
		\item RPN use supervision more than similarity \\ 
		$\Rightarrow$ need asymmetric part to map similarity to RPN regression-classification task \\
		$\Rightarrow$ separate xcorr for different class
		\item multi-level feature is meaningful only given enough diversity \\
		(i.e. feature from different level has different resolution \& semantic level) \\
		$\Rightarrow$ only meaningful with deep-enough net
		\end{itemize}
	\end{itemize}

\item SiamDW: Deeper and Wider Siam
	\begin{itemize}
	\item Analysis
		\begin{itemize}
		\item padding destroy the translation invariance \\
		(can know if a feature comes from corner-border-center by analyzing its content) \\
		$\Rightarrow$ as all target labeled in center, thus central bias
		\item padding results in INconsistent template-search feature map
			\begin{itemize}
			\item template feature extracted with padding
			\item search feature map has no such padding in non-border area
			\end{itemize}
		$\Rightarrow$ xcorr canNOT effectively measure similarity
		\item $\Rightarrow$ remove padding by "cropping-inside residual" (CIR) unit \\
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW central bias with-without crop".png}		
		\end{itemize}
	\item $\Rightarrow$ CIR Unit (to remove padding influence)
		\begin{itemize}
		\item add cropping after addition $\Rightarrow$ remove padding influenced feature \\
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW cir unit".png}
		\item change the position of downsampling ops \\
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW cir downsample".png} \\
		$\Rightarrow$ use conv with stride $1$ (instead of $2$) \& add 2x2 max pool after addition-crop \\
		$\Rightarrow$ remove padding influence \& considering info at the border (by max pool) \\
		(directly crop after stride-2 conv: miss out potential strong response at the edge) \\
		(has been also empirically proven direct-cropping has worse result)
		\item increase the width of each unit (as inception, resXnet) \\
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW cir wider".png}
		\end{itemize}
	\item Backbone
		\begin{itemize}
		\item modified Resnet with stacked CIR unit \& downsampled by CIR-D unit \\
		$\Rightarrow$ with CIResNet-22 being the best-performance net \\
		(CIResNet-43 perform almost the worst, due to in less weight CIResNet-43) \\ 
		really? less wright, yes, but being deeper not help?
		\end{itemize}
	\item Training
		\begin{itemize}
		\item gradually finetune the network from back to front \\ 
		(e.g. unfreeze a unit every $5$ epoch)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item siam net as tracker \& tracking by detection
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item padding leads to \underline{central bias \& degraded similarity measurement} \\
		$\Rightarrow$ cropping to remove padding influence \\
		(why not, crop only before xcorr \& logit feature map to speedup?)
		\item ensure receptive field to be $60\% \sim 80\%$ size of the template image \\ 
		(empirical setting to have best performance for siam tracker) \\
		$\Rightarrow$ as larger RF includes more context \& thus insensitive to spatial location
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Multi-Object Tracking \& Data Association}
\begin{itemize}
\item Gated Association
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item given prediction, its variance and detection noise, filter out an interested area
		\item consider only detection inside the interested area (satisfying requirements)
		\item score each detection \& associate detection (detection result) with tracker \\
		$\Rightarrow$ which detection belongs to which trajectory
		\end{itemize}
	\item Global Nearest Neighbor
		\begin{itemize}
		\item choose the best / most probable / nearest \\
		(under the constraint that an detection can associate with at most one track)
		$\Rightarrow$ assume one detection is produced by single object
		\item require accurate and sparse detection, with few false alarm
		$\Rightarrow$ sensible to noise (easily fail in crowded scene)
		\end{itemize}
	\item Nearest Neighbor
		\begin{itemize}
		\item choose the best / most probable / nearest
		(thought one detection may be used by multiple tracks)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\item Joint Probability Data Association (JPDA)
	\begin{itemize}
	\item Procedure
	\item Understanding
		\begin{itemize}
		\item probabilistic perspective for prediction-detection relation \\ 
		$\Rightarrow$ cooperate with uncertain association: weight all detections by probability
		\item hence, crowded detections tends to pull multiple tracks together \\ 
		$\Rightarrow$ coalescence problem
		\end{itemize}
	\end{itemize}
\item Multiple Hypothesis Tracking
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item Gaussian models for target dynamics and noise in detection
		\item uniform distribution for false alarm (false-positive detection)
		\end{itemize}
	\item Track Hypothesis
		\begin{itemize}
		\item association result given track initiation, prediction and detection \\
		$\Rightarrow$ a sequence of selected detection
		\item compatibility: tracks are compatible, if they do NOT share any detection \\
		$\Rightarrow$ any track update using the same detection are INcompatible
		\end{itemize}
	\item Track Tree (Clustering)
		\begin{itemize}
		\item use incompatibility as edge, track as vertice, sort tracks by time \\
		$\Rightarrow$ each connected tree becomes a cluster (track family) \\
		(the tree level denotes time sequence)
		\item each tree shares a common root node (the initial detection)
		\item growing: whenever a new detection can be accounted for a track hypothesis (node)  \\ 
		$\Rightarrow$ the node generates 2 children nodes (tracks): update / not update
		\end{itemize}
	\item Global Hypothesis
		\begin{itemize}
		\item a global hypothesis contains only compatible track(s) \\ 
		$\Rightarrow$ the collection of track, with $\le1$ track from each tree/family
		\end{itemize}
	\item Track Score
		\begin{itemize}
		\item posterior ratio $r=\frac{p(D|T)p(T)}{p(D|F)p(F)}\triangleq \frac{p_T}{p_F}$, \\
		where $p(D|T), p(D|F)$ the likelihood given detection is true, false alarm \\
		with $D$ the detections in current track \\
		$\Rightarrow$ log ratio $lr = \ln \frac{p_T}{p_F}$
		\item use log ratio as score, at time $t, L(t) = L(t-1) + \Delta L(t)$, \\
		where $\Delta L(t) = \begin{cases} \ln(1-\hat{P}(D)) & \text{no update} \\ \Delta L_u(t) & \text{update} \end{cases}$, \\
		with $\hat{P}(D)$ the expected probability of detection; \\ 
		and $\Delta L_u(t)$ the residual error between prediction and detection \\
		($\Delta L_u(t)$ may include covariance, density, $\hat P_D$, etc.)
		\end{itemize}
	\item Global Hypothesis Score
		\begin{itemize}
		\item $s_H = \sum_{k\in K} L_k(t)$, \\ 
		where $K_H$ all (compatible) tracks in hypothesis $H$, $L_k$ the score for track $k$
		\end{itemize}
	\item Global Hypothesis Probability
		\begin{itemize}
		\item computed from hypothesis score (a maximum weighted independent set problem)
		\end{itemize}
	\item Track Probability
		\begin{itemize}
		\item the sum of probability of all hypothesis that contains the track
		\end{itemize}
	\item $N$-scan Pruning
		\begin{itemize}
		\item given detection at time $t$, eliminate IMplausible tracks originated at $t-N$ \\
		$\Rightarrow$ suppress tree from exponentially growing;
		\item $N$ the time step buffer before decision (scan = time), usually $N\ge 5$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item probabilistic perspective towards the result of decisions for data association \\
		(a larger scope than JPDA)
		\item defer critical decision into the future \\
		$\Rightarrow$ make decisions for the past after their observation available
		\item model track alternatives, each with a probability, by track tree and hypothesis \\ 
		(for all possible tracks, model joint prob over all detections in a track)
		\item model global joint probability of all tracks, by global hypothesis
		\item similar to DP-longest substring: maintain a set of candidates
		\item essentially, a bread-first search $\Rightarrow$ real-time ability constrained by tree size 
		\end{itemize}
	\end{itemize}

\item Maximum Net Flow
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item detection
		\end{itemize}
	\item Output
		\begin{itemize}
		\item association result
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item detection as node, possible association as edge \\
		$\Rightarrow$ construct a graph, with detection time as layer
		\item $\Rightarrow$ solve as maximum flow / minimal cost
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item another global optimization, vs. probabilistic perspective in MHT
		\end{itemize}
	\end{itemize}

\item Inversed Reinforced Learning for Data Association with Markov Decision Process
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item current detected bounding boxes, with objectness
		\item previously predicted bounding boxes
		\end{itemize}
	\item Output
		\begin{itemize}
		\item decision of data association between track \& detection
		\end{itemize}
	\item Markov Decision Process (MDP) for Track Management
		\begin{itemize}
		\item states: active, tracked, lost, inactive \\
		$\Rightarrow$ model the state of a track
		\item probability at each state given by trained model for each state
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item binary classification for track start (given a detection)
		\item optic flow to track \& association \\
		(rule model to decide if lost)
		\item regression model to associate lost track \& current detection \\
		(measure similarity)
		\item rule for track death: lost for consecutive $6$ frames
		\end{itemize}
	\item Training: Inversed Reinforcement Learning for Lost Recovery
		\begin{itemize}
		\item classifier for track start: trained offline
		\item regressor for lost track association: trained only when MDP make wrong decision \\
		(similar to hard-example mining?)
		\end{itemize}
	\item Tracking
	`	\begin{itemize}
		\item one MDP for a track in multi-object tracking \\
		$\Rightarrow$ multiple tracks may update with same detection (need to tune optic flow)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item explicit expression for track state \\
		$\Rightarrow$ can design state for hard scenario \\
		$\Rightarrow$ enable explicit control over optimization
		\item ugly crashed model for each state $\Rightarrow$ can be all unified to NN(s)
		\end{itemize}
	\end{itemize}

\item Siamese CNN for Association
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image pair $(I_1, I_2)$, with optic flow $I_1\rightarrow I_2$ as $(O_1,O_2)$ \\
		$\Rightarrow D = [I_1,I_2,O_1,O_2]$ (resized \& channel concat)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item probability of data association between two detections
		\end{itemize}
	\item Inference \& Structure
		\begin{itemize}
		\item 3 conv layers, max pooling, 4 dense layers
		\item examine spatio-temporal info: position change \& relative velocity by difference
		\item NN feature vector concat with handcraft feature
		\item feed into gradient boosting classifier (with $400$ trees) \\ 
		$\Rightarrow$ output as binary classification of (match, no match)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item true positive: associated ground truth detection in $2$ frames \\
		(time gap $\le15$ frames)
		\item negative: wrong association to true detection of other track / false detection
		\item data augmentation: false alarm, distortion on image
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item given current data association probability for all track-detection pair \\
		$\Rightarrow$ construct a linear program problem (with constraints)
		$\Rightarrow$ a global optimization for association given probability
		\item online tracker (e.g. kalman filter)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item NN approach for association probability \\
		(with fusion of NN \& rule-model via GB classifier) \\
		$\Rightarrow$ fusion much better than pure NN $\Rightarrow$ spatio-temporal info important
		\end{itemize}
	\end{itemize}

\item Online Multi-Target Tracking Using RNN
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item $D$ the dimension of bbox encoding (e.g. x, y, w, h, objectiveness, etc.)
		\item $x_t\in \mathbb{R}^{ND}$ all the $N$ track state (bbox) at time $t$
		\item $x^\star_{t} \in\mathbb{R}^{ND}$ all the $N$ predicted bboxes for time $t$, from time $t-1$
		\item $z_t\in \mathbb{R}^{MD}$ all the $M-1$ detected bbox at time $t$, with an empty detection
		\item $\varepsilon_t\in(0,1)^N  \in\mathbb R^N$ the existence probability (liveness) for all tracks
		\item $A_t \in \mathbb{R}_{N\times M}$ the probability matrix for data association between track-detection
		\item $h_t$ hidden state of track RNN at time $t$
		\item $C_t \in \mathbb{R}_{N\times M}$ the distance matrix between $x^\star_{t}$ and $z_t$ \\ 
		i.e. $C_t[i,j] = dist(x^\star_{t}[i] - z_t[j])$
		\item available ground truth: $\widetilde x_t, \widetilde A_t, \widetilde \varepsilon_t$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item given $x_t,h_t$, track RNN outputs state prediction $x^\star_{t+1}$, compute $h_{t+1}$
		\item based on pred $x^\star_{t+1}$ and detection $z_{t+1}$, compute $C_{t+1}$
		\item given $C_{t+1}$ and hidden state for $i^\text{th}$ track $h_{t+1}[i]$ \\ 
		$\Rightarrow$ association LSTM scans over all detection $z_{t+1}$ \\
		$\Rightarrow$ regress $A_{t+1}[i, :]$, the association prob for $i^\text{th}$ track and each bbox in $z_{t+1}$ \\
		(as part of track RNN process)
		\item given detection $z_{t+1}$, association prob $A_{t+1}$, latest liveness $\varepsilon_t$, with $h_{t+1}$ \\ 
		$\Rightarrow$ update state to be $x_{t+1}$, estimate liveness $\varepsilon_{t+1}$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item track RNN consists of a $2$-layer association LSTM
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item prediction $L_\text{pred} = \frac \lambda {ND} \sum (x^\star_{t+1} - \widetilde x_{t+1})^2$
		\item updated state $L_\text{update} = \frac \kappa {ND} \sum (x_{t} - \widetilde x_{t+1})^2$
		\item liveness $L_\varepsilon = \widetilde \varepsilon_t\log\varepsilon_t + (1-\widetilde \varepsilon_t)\log(1-\varepsilon_t) + \abs{\varepsilon_t - \varepsilon_{t-1}}$ \\
		$\Rightarrow$ minimize the diff between consecutive liveness estimation $\Rightarrow$ smoothness \\
		(prevent track from termination for only a single detection lost)
		\item association $L_a = -\log(A_{t+1}[i,\widetilde{j}])$, where $\widetilde{j}$ the true association for $i^\text{th}$ track
		\end{itemize}
	\item Training
		\begin{itemize}
		\item data augmentation: sample synthetic trajectories from each labeled video \\
		(Gaussian distribution)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item forward unroll track RNN, if liveness $\le0.6$, corresponding track ignored
		\item liveness $\ge0.6$ again, a new track initiated
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item specialized RNN cell accounting for prediction, update, birth-death of all tracks
		\item another LSTM cell designed for data association \\
		$\Rightarrow$ able to learn $1-1$ association by scanning \\
		(yet, unnecessary, since $N,M$ fixed i.e. a fixed size mapping)
		\item utilize given detector $\Rightarrow$ no appearance model (but only location \& size)
		\item able to maintain at most $N$ track with maximally $M$ detection at a time
		\end{itemize}
	\end{itemize}

\item Collaborative Deep Reinforcement Learning for MOT
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $I_t$ the $t^\text{th}$ image frame
		\item $b^\star_{i,t}$ a bbox for $i^\text{th}$ ground truth object $p_i$ at frame $t$
		\item $B^\star_{i,t}$ a set of bboxes sampled around $b^\star_{t,i}$
		\item $g(a,b)$ cal the IoU between bbox $a,b$
		\item $p_{i,t} = \{ b,f \}$, the $i^\text{th}$ detected object at $t$, \\
		where $b=\{x,y,w,h\}$ the bbox; $f$ the appearance model \\
		$\Rightarrow$ distance $d(p_1,p_2) = \alpha (1-g(b_1,b_2)) + (1- \underbrace{\frac{f_1^Tf_2}{\norm{f_1}\norm{f_2}}}_\text{(cos dist)})$
		\item $H=\{ b_1, ..., b_t \}$ the history trajectory of an object \\
		$\Rightarrow H^K = \{b_{t-K+1}, ..., b_t\}$ the history of past $K$ frames
		\item an agent for each object $g = \{ H, p \}$
		\item detections as environment $\hat P_t = \{\hat p_1,...,\hat p_{n_t}\}$
		\item state at frame $t, s_t=\{G_t, \hat P_t \}$, where $A_t = \{ g_1,...,g_m \}$
		\item set of actions $\mathcal A=\{\text{update}, \text{ignore}, \text{block}, \text{delete} \}$
		\end{itemize}
	\item Prediction Net Inference
		\begin{itemize}
		\item crop the frame $t+1$ at the location of estimated bbox $b_{i,t}$ (of frame $t$)
		\item 3 conv layers, then dense, then concat with $H^{K=10}$ (fuse with temporal info)
		\item 2 dense layers to regress $b_{i,t+1}$, the bbox for object $i$ of frame $t+1$
		\end{itemize}
	\item Prediction Net Training
		\begin{itemize}
		\item regression loss $\displaystyle L = \sum_{i,t}\sum_{b\in B_{i,t}} g ( b^\star_{i,\mathbf{t+1}}, \phi(I_t, b, H^{K=10}_i))$, \\
		where $I_t$ the image frame at time $t$, $\phi$ the mapping of pred net
		\end{itemize}
	\item Action
		\begin{itemize}
		\item update: $f_{t+1}= (1-\rho_f) \cdot f_t + \rho_f \cdot \hat f_{t+1}$, where $\hat f_{t+1} \in$ selected detection $\hat p_{t+1}$; \\
		$b_{t+1} = (1-\rho_b) \cdot b_{t} + \rho_b \cdot b'_{t+1}$, where $b'_{t+1}$ predicted position; \\
		($\rho_f, \rho_b$ pre-selected)
		\item ignore: no detection suitable, use only prediction for update ($\rho_f=0, \rho_b=1$)
		\item block: same as ignore, no detection due to occlusion
		\item delete: remove the agent
		\end{itemize}
	\item Reward
		\begin{itemize}
		\item for agent $g$ at time $t$, with pred and ground-truth box $b'_{t+1}, b^\star_{t+1}$
		\item reward for agent $g$ at time $t: r^\star_{t} = r_t + \beta r_{j,t}$, where \\ 
		$r_t$ for itself; $r_{j,t}$ for its nearest neighbor, $\beta$ a balance factor \\ 
		($r_t,r_{j,t}$ calculated in the same manner) \\
		$\Rightarrow$ agents need to collaborate for better reward
		\item for action $a\in\{\text{update}, \text{ignore}, \text{block}\}$ \\
		$\Rightarrow r_t = \begin{cases}1 & \text{if }IoU \ge 0.7 \\ 0 & \text{if } 0.5 \le IoU \le 0.7 \\ -1 & \text{otherwise} \end{cases}$ ($IoU$ calculated between $b'_{t+1}$ and $b^\star_{t+1}$) \\
		for action $a=$ delete $\Rightarrow r_t = 1 \text{ if object disappear; else } -1$
		\item $\Rightarrow Q(s_t, a_t) = r^\star_{t} + \gamma r^\star_{t+1} + \gamma^2 r^\star_{t+2} + \dotsb$, where $\gamma$ decaying param
		\end{itemize}
	\item Decision Net Inference
		\begin{itemize}
		\item for each $g\in G_t$ with its current \& predicted location $b_t, b'_{t+1}$
		\item select a neighbor agent $g_{j} \in G_t-\{g\}$, that is nearest to $b_{t}$
		\item select the detection $\hat p \in \hat P_{t+1}$ that is nearest to $b'_{t+1}$
		\item 3 feature maps $(p\in g, p_j\in g_j, \hat p)$, flatten as $1$-D vector input \\
		$\Rightarrow 3\times$ dense layer to output prob over actions $\pi(a|s,\theta)$, where $\theta$ the weights
		\end{itemize}
	\item Decision Net Training
		\begin{itemize}
		\item goal $\displaystyle \arg \max_{\theta} L(\theta) = \mathbb E_{s,a} \log (\pi (a|s,\theta)) \cdot Q(s,a)$ \\
		\begin{align*} \displaystyle \Rightarrow \frac \partial {\partial \theta} L  &= \mathbb E_{s,a} \frac \partial {\partial \theta}[ \log (\pi (a|s,\theta)) \cdot Q(s,a) ] \\ &= \mathbb E_{s,a} [\frac{Q(s,a)}{\pi(a|s,\theta)} \cdot \frac {\partial }{\partial \theta} \pi(a|s,\theta)] \end{align*} \\
		$\Rightarrow$ increase probability for actions with $Q>0$; decrease for those with $Q<0$
		\item to speed up converge: value for state $s, \displaystyle V(s) = \frac{\sum_{a}p(a|s)Q(s,a)} {\sum_{a} p(a|s)}$ \\
		$\Rightarrow$ advantage value $A(s,a) = Q(s,a) - V(s)$ \\
		(in case all $Q>0$, or all $Q<0$ at the beginning: use expectation as zero-line)
		\item policy gradient as $L(\theta) = \mathbb E_{s,a} \log(\pi(a|s.\theta)) A(s,a)$
		\item pre-training: set $\gamma, \beta = 0$ before searching hyper-parameter $\beta,\gamma$
		\item training set: detection bbox $=\{ \hat b \in \text{detection } \hat P | \text{ IoU}^{\hat b}_\text{label} > 0.5 \} + \{\text{label } b^\star \}$
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item initiate a track for each initial detection
		\item for each agent, predict its location at $t+1$, by pred net
		\item for each agent $g$, select its closest detection $\hat p \in \hat P_{t+1}$, $p_j\in$ closest agent $g_j$\\
		$\Rightarrow$ decision net: $(p\in g, \hat p, p_j) \rightarrow$ action $\mathcal A$ 
		\item track terminates when decision net decides to "delete"
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item prediction net as a detector to give more precise location \\
		(an implicit self-trained detector)
		\item decision net to eliminate false alarm \& combine prediction \\ 
		$\Rightarrow$ robust to different detector/predictor
		\item decision net trained to collaboratively maximize utility \\ 
		$\Rightarrow$ mitigate the false negative \\
		$\Rightarrow$ introduce distractor in training for better discriminative ability
		\item may trapped in false appearance feature $\Rightarrow$ ID switch \\ 
		(e.g. blue box handed from one person to another person)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Instance Tracking}
\begin{itemize}
\item SiamMask: Fast Online Object Tracking and Segmentation : A Unifying Approach
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item initial bbox $z$ as target template at first frame (exemplar)
		\item cropped search region $x_t$, centered at target location of $t-1$
		\end{itemize}
	\item Output
		\begin{itemize}
		\item response map for object localization
		\item bbox regression for target (with resizing \& \underline{rotation})
		\item binary segmentation mask i.e. pixel $\in$ (target, not target)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item given $z, x_t$ backbone CNN extract feature as $f(z), f(x_t)$
		\item cross-correlation response map $g(z,x_t) = f(z)\star f(x_t)$, \\
		where $f(z)$ used as kernel \\
		$\Rightarrow g$ as response of candidate window (RoW) \\
		$\Rightarrow$ encode similarity between $z$ and each candidate window/bbox in $f(x_t)$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item backbone CNN ResNet-50 as feature extractor, with adjustment \\
		(dilated conv for better resolution)
		\item cross-relation between two extracted feature maps \\ 
		$\Rightarrow$ response map, each location an RoW
		\item bbox regression: 2$\times$(1-by-1 conv) for $k$ anchors at each RoW
		\item bbox classification: 2$\times$(1-by-1 conv) for score of $k$ anchors at each RoW
		\item segmentation: 2$\times$(1-by-1 conv) + upsampling with skip + per-pixel sigmoid \\
		$\Rightarrow$ upsample into a mask for each RoW location
		\end{itemize}
	\item Training
		\begin{itemize}
		\item loss $L = \lambda_1 L_\text{mask} +\lambda_2 L_\text{score} +\lambda_3 L_\text{box}$ \\
		($L_\text{mask}, L_{box}$ considered only at location for ground truth)
		\item trained different branch using corresponding dataset
		\item data augmentation: random jitter, translation, rescaling
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item one-step update, may use mask to produce 
		\item multi-object: multiple initialization, each with a net as tracker
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multi-task training improves all branches \\
		(semi-supervised video object segmentation, bbox tracking)
		\item mask branch: output the mask in the context of $x$ for that obj in $z$
		\item simple starting point \& fast speed for video object segmentation (compared to $0.1$FPS)
		\item more descriptive representation for tracked object $\Rightarrow$ more detail in mask
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Instance Segmentation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item images
	\end{itemize}
\item Goal
	\begin{itemize}
	\item obtain pixel mask for each instance
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Within-Category Overlap
		\begin{itemize}
		\item need to realize the boundary of multiple same-class instances
		\end{itemize}
	\item Semantic Segmentation + Detection
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\item Evaluation Metrics
	\begin{itemize}
	\item AP \@ IoU
		\begin{itemize}
		\item use mask IoU to classify an pred mask to be true/false positive
		\item calc the average precision accordingly
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Instance-first}
\begin{itemize}
\item Mask R-CNN
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item backbone extract feature map
		\item existing cls-reg head produce bbox detection \& NMS \\
		$\Rightarrow$ selecting out 100 highest-score boxes \\
		(apply mask head after NMS for less overhead)
		\item for each corresponding RoI, mask head produce $K$ mask, each of size $m\times m$, \\
		where $K$ the num of classes
		\item select the $k^\text{th}$ mask, with $k$ the predicted class of the RoI
		\item resize  mask into predicted bbox size
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item ResNet-FPN backbone
		\item based on two-stage (Faster R-CNN) framework
		\item RoI align (instead of RoI pooling) to avoid quantization
		\item mask head: a small FCN to produce $K$ binary masks via per-pixel sigmoid
		\end{itemize}
	\item Training
		\begin{itemize}
		\item $L=L_\text{cls}+L_\text{reg}+L_\text{mask}$, 
		where $L_\text{cls}, L_\text{reg}$ same as detection
		\item $L_\text{mask}$: an average binary cross-entropy on one of the $K$ mask \\
		$\Rightarrow$ define only on $k^\text{th}$ mask of a positive RoI \\
		(only consider the part of label mask inside the pred box)
		\item ensure pos-net ratio = 1:3
		\item joint training: still, stop gradient from RoI Align to RPN proposals
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item \underline{RoI Align} to remove quantization \\
		$\Rightarrow$ remove mis-alignment between RoI location \& extracted feature \\
		\abovedisplayskip=2pt\abovedisplayshortskip=2pt~\vspace*{-\baselineskip}
		\begin{align*}
		\Rightarrow & \text{ preserve exact localization} \\
		& \text{i.e. per-pixel spatial correspondence}
		\end{align*}
%		$\Rightarrow$ preserve exact localization \\
%		i.e. per-pixel spatial correspondence
		\item $\Rightarrow$ able to use large-stride network (e.g. 32-stride) \\
		(as quantization leads to larger misalignment with larger network stride)
		\item $\Rightarrow$ significant improvement in AP \& bigger gain in AP at higher IoU
		\item \underline{decouple} mask \& class pred
			\begin{itemize}
			\item per-pixel sigmoid to produce each mask for each class v.s. per-pixel softmax
			\item binary cross-entropy loss v.s. multinomial cross-entropy loss
			\end{itemize}
		(could be further a class-agnostic mask pred of $m^2$ size) \\
		$\Rightarrow$ avoid class competition when generating mask \\
		$\Rightarrow$ much easier to learn
		\item FCN as mask head: utilize pixel-to-pixel correspondence of conv ops \\
		(instead of dense layers) $\Rightarrow$ more accurate
		\item generalization ability: extend to keypoint detection \\
		$\Rightarrow$ each mask produce a key point (spatial softmax over each mask)
		\item able to multi-tasking all three tasks: detection, instance seg \& key-point \\
		(improve on all tasks)
		\item still, introduce $\sim20\%$ overhead, even after box selection \\
		(due to heavy mask head)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Segmentation-first}

\subsection{Face Recognition}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item image from camera
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Identity Recognition
		\begin{itemize}
		\item recognize the identity of the face in image
		\item refuse to recognize if the face does NOT belongs to any stored identity
		\end{itemize}
	\item Liveness Detection
		\begin{itemize}
		\item make sure the face in image are from a live human \\
		(instead of picture etc.)
		\end{itemize}
	\end{itemize}
\item Face Verification
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image from camera \& identity
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item true/false, regarding whether the image content belongs to the identity
		\end{itemize}
	\end{itemize}
	
\item Challenge
	\begin{itemize}
	\item One-shot Learning
		\begin{itemize}
		\item given only single (at most, few) face-identity pair for each identity
		\item still, need to build a robust system for recognition task
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Siamese Network as Encoder
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item CNN + dense layer to encode the input image $x^i$ as a vector $f(x^i)$
		\end{itemize}
	\item Learning Goal
		\begin{itemize}
		\item minimize $\norm{f(x^i)-f(x^j)}^2$ if $x^i, x^j$ from same identity
		\item maximize $\norm{f(x^i)-f(x^j)}^2$ if $x^i, x^j$ from different identity
		\item $\Rightarrow$ learning encoding given a fixed distance function $d(x_1, x_2) \ge 0$\\
		(here, $d(x_1, x_2)=\norm{x_1-x_2}^2$)
		\end{itemize}
	\item Triplet Loss
		\begin{itemize}
		\item given an anchor image $A$ representing the identity $I$
		\item take a positive image $P\in$ identity $I$; an negative image $N\not\in$ identity $I$
		\item $\Rightarrow L(A,P,N) = \max\left( d(f(A), f(P)) - d(f(A), f(N)) + \alpha, 0 \right)$, where \\
		$\alpha$ an hyperparamter to make sure the net differentiate them by a margin; \\
		$\max()$ to make the loss $=0$ as long as the requirement satisfied
		\end{itemize}
	\item Training: Hard Negative Mining
		\begin{itemize}
		\item due to large variance in the dataset $\Rightarrow$ $d(A,P) << d(A,N)$ in most case
		\item due to large number of identities $\Rightarrow$ permutation explosion
		\item $\Rightarrow$ evaluate current net on dataset, use mistakes for the next epoch
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn a encoder towards a selected distance function \\
		$\Rightarrow$ use permutation to have more training examples
		\item able to precomputing the encoded vector for fast recognition
		\end{itemize}
	\end{itemize}

\item Encoding + Binary Classification
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item still, CNN + dense layer to encode input image
		\end{itemize}
	\item Learning Goal
		\begin{itemize}
		\item given two encoded vectors, another net (or logistic regression) to perform binary classification \\
		$\Rightarrow$ $1$ for two image has same identity; $0$ for different identities
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item still, utilize permutation for larger training set \\ 
		(use pair, instead of triplet)
		\item learn the similarity function as well: output directly the result of comparison
		\item pre-compute the encoding of Siamese net \\
		$\Rightarrow$ enable flexible deployment (device performs only bi-classification)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Stereo Vision}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Visual Perception
		\begin{itemize}
		\item image from mono-camera
		\item images pair from stereo-/multi-cameras
		\item video sequence from mono-/stereo-cameras
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Depth Perception
		\begin{itemize}
		\item directly output a depth map
		\item output a disparity map as indirect depth perception
		\end{itemize}
	\end{itemize} 
\item Approach Understanding
	\begin{itemize}
	\item Unsupervised Learning
		\begin{itemize}
		\item $f: \text{input}\rightarrow\text{output}$ hard \& $f^{-1}: \text{output}\rightarrow\text{input}$ easy \\
		$\Rightarrow$ analog to $P$-$NP$ problem \\
		(easy to check/describe the error solution \& hard to find solution)
		\item designed error should be highly correlated with desired prediction error
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Unsupervised Learning}
\begin{itemize}
\item Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item avoid systematic error in collected image-disparity dataset \\
		$\Rightarrow$ use deep net to jointly deal with motion blur, etc. 
		\end{itemize}
	\end{itemize}
\item Unsupervised Monocular Depth Estimation with Left-Right Consistency
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item epipolar constraint with image reconstruction loss \\
		$\Rightarrow$ unsupervised depth from mono-camera \\
		$\Rightarrow$ avoid stereo-/multi-camera in practice use \& avoid pixel-level labeling
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Recognition at a Distance}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Video
		\begin{itemize}
		\item hot standby camera with stationary \& active vision
		\item stationary vision: wide field of view with low resolution for detection
		\item active vision: narrow field of view with high resolution for detail analysis
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Recognition in the Wild
		\begin{itemize}
		\item large coverage areas: $>100$m range \\
		$\Rightarrow$ scale beyond the theoretical analysis and practical design advice
		\item with NO subject cooperation
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item None Cooperative Subject
		\begin{itemize}
		\item not cooperating, may even be evading the system
		\end{itemize}
	\item Resolution
		\begin{itemize}
		\item low resolution due to the very long object distance
		\item restricted by lens resolution, which is then restricted by price
		\item trade-off between wideness (coverage) \& depth (zooming)
		\end{itemize}
	\item Illumination
		\begin{itemize}
		\item dynamic illumination in the wild scene
		\item maximum light intensity restricted by aperture size \\
		(given a fixed exposure time)
		\end{itemize}
	\item Distortion and Blur
		\begin{itemize}
		\item amplified noise, due to: \\
		low brightness $\Rightarrow$ low signal-to-noise ratio $\Rightarrow$ ISO amplification
		\item motion blur (if trade-off between ISO \& exposure time)
		\item blur from sensor tilt, as a hot standby system
		\item fog, haze \& atmosphere blur for very long distance recognition
		\end{itemize}
	\item Pose
		\begin{itemize}
		\item the view angle due the camera position \\ 
		(usually overhead for less occlusion $\Rightarrow$ downward tilt)
		\end{itemize}
	\item Multi-Object
		\begin{itemize}
		\item schedule the limited high-resolution vision resource for multiple candidates \\
		$\Rightarrow$ time window prediction, scheduling \& resource allocation
		\end{itemize}
	\item Physical Coupling
		\begin{itemize}
		\item expensive field test (mitigated by virtual environment)
		\end{itemize}
	\end{itemize}
\item Application
	\begin{itemize}
	\item Watch-list Recognition
		\begin{itemize}
		\item an alert when person of interest appear/approach
		\end{itemize}
	\item Re-recognition
		\begin{itemize}
		\item cross-camera tracking, long-range persistent tracking
		\end{itemize}
	\item Logging
		\begin{itemize}
		\item catalog best recognized feature (e.g. face) for each person entering a region
		\item marketing: understand customer activities and behavior
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Stationary Vision}
\begin{itemize}
\item 
\end{itemize}

\subsubsection{3D Imaging}
\begin{itemize}
\item 
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsection{Re-Identification}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Images
		\begin{itemize}
		\item captured by camera networks across multiple areas
		\item may assume to be a crop over interested target \\
		(e.g. a bounding box crop, instead of the whole image)
		\item a single image or a images sequence of target
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Identification
		\begin{itemize}
		\item retrieve images from gallery/database that has same identification as input
		\end{itemize}
	\item Ranking
		\begin{itemize}
		\item list out the most probable ID, to ensure recall \\
		(e.g. in security scenario)
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Varying Appearance
		\begin{itemize}
		\item due to different view point, pose, background change (indoor v.s. outdoor), etc. \\
		$\Rightarrow$ hard to realize inter-class similarity \& intra-class difference
		\end{itemize}
	\item Limited Data
		\begin{itemize}
		\item small dataset, hard to learn to use rare feature \\
		(though rare feature should be a strong identification sign)
		\end{itemize}
	\end{itemize}
\item Metrics
	\begin{itemize}
	\item Rank-1 Accuracy
		\begin{itemize}
		\item measure the match of ID with highest predict prob
		\end{itemize}
	\item mean Average Precision
		\begin{itemize}
		\item measure the accuracy of match of whole ranking \\
		$\Rightarrow$ account for the hard case (same ID can be multi-viewed in camera net)
		\end{itemize}
	\end{itemize}
\item Trends
	\begin{itemize}
	\item Local \& Distributed Representation
		\begin{itemize}
		\item feature on human body with human body model (e.g. skeleton model) as a prior
		\item region proposal for different local area on human body
		\item salient partitions with attention
		\end{itemize}
	\item Multi-scale Representation
		\begin{itemize}
		\item concat local feature, features at multi-scales
		\item spatial attention
		\end{itemize}
	\item Surpassing Human Performance
	\end{itemize}
\end{itemize}
\subsubsection{Re-Ranking}
\begin{itemize}
\item Re-ranking Person Re-identification with k-reciprocal Encoding
	\begin{itemize}
	\item Analysis \& Derivation
		\begin{itemize}
		\item given probe $p$, retrieved image $g\in$ gallery $G$, $\abs{G}=N$ \\
		$\Rightarrow N(p,k)=\{ g_{1}, ..., g_{k} \}$ the $k$-nearest neighbors of $p$,
		where $p$ the probe image \\ 
		(i.e. top-$k$ sample in the ranking result)
		\item yet, can often introduce false matches (noise)
		\item $\Rightarrow$ require $p \in \text{k\_near}(g) \&\& g \in \text{k\_near}(p)$ (similar to the thought in re-projection error) \\
		i.e. both rank top-k when the other image is taken as probe \\
		(hence, reciprocal)
		\item $\Rightarrow$ the $k$-nearest reciprocal neighbors $R(p,k) = \{ g_i \mid g_i \in N(p,k) \and p\in N(g_i, k) \}$
		\item yet, can also have positive image excluded
		\begin{align*} \Rightarrow \forall q\in R(p,k), &R^* \leftarrow R(p,k) \cup R(q, {\scriptstyle\frac12} k)  \\ &\text{s.t. } \abs{R(p,k) \cap R(q,{\scriptstyle\frac12}k)} \ge \frac23 \abs{R(q, {\scriptstyle\frac12}k)}
		\end{align*}
		i.e. expand the set by considering the $k$-reciprocal neighbors of the set member \\
		(expansion confirmed by voting)
		\end{itemize}
	\item Distance Derivation
		\begin{itemize}
		\item Jaccard distance \begin{align*} \displaystyle d_J(p,g_i)&=1-\text{IoU}^{R^*(p,k)}_{R^*(g_i,k)} \\ &= 1 - \frac{\abs{R^*(p,k)\cap R^*(g_i,k)}}{\abs{R^*(p,k)\cup R^*(g_i,k)}} \end{align*}
		\item to speed-up: encoding $R^*$ into vector \\
		$\Rightarrow V_p=[V_{p,g_1}, ..., V_{p,g_N}]$, \\
		where $V_{p,g_i} = 1 \text{ if } g_i\in R^*(p,k), \text{ else } 0$
		\item to consider original distance measurement (weighting image) \\
		$\Rightarrow V_p=[V_{p,g_1}, ..., V_{p,g_N}]$, \\
		where $V_{p,g_i} = e^{-d(p,g_i)} \text{ if } g_i\in R^*(p,k), \text{ else } 0$, with $d(p,g_i)$ the original distance
		\Item \begin{align*} \Rightarrow & \abs{R^*(p,k)\cap R^*(g_i,k)} = \norm{\min(V_p, V_{g_i})}_1 \\ & \abs{R^*(p,k)\cup R^*(g_i,k)} = \norm{\max(V_p. V_{g_i})}, \end{align*} where $\min, \max$ being element-wise \\
		(i.e. element-wise logical and \& count the num of resulting true)
		\item $\displaystyle \Rightarrow d_J(p,g_i) = 1-\frac{\sum_{j=1}^N \min(V_{p,g_j}, V_{g_i, g_j})} {\sum_{j=1}^N \max(V_{p,g_j, V_{g_i, g_j}}) }$
		\item accounting for original distance (which contains context info) \\
		$\Rightarrow d^*(p, g_i) = (1-\lambda)d_J(p, g_i) + \lambda d(p, g_i)$, with $\lambda \in [0,1]$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item need to have $\ge1$ positive in the gallery to better improve result \\
		(yet, does not harm even in a single-shot setting, e.g. CUHK03)
		\item a fully automated \& unsupervised approach
		\item significantly improve mAP \& improve rank-1 accuracy, after re-ranking \\
		(verified with various model: robust to various distance metrics) \\
		(better than $k$-nearest neighbors as expected)
		\item $k=20$ in most dataset to avoid including false matches with too large $k$
		\item $\lambda=0.3$ (empirically) to further boost performance \\
		(original distance is also important in re-ranking)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{People Re-ID}
\begin{itemize}
\item PCB-RPP: Person Retrieval with Refined Part Pooling
	\begin{itemize}
	\item Inference 
		\begin{itemize}
		\item CNN extracts feature map, vertically "soft" partitioned into $p$ parts
		\item each of $p$ branch independently performs average pooling, processing \& output 
		\item $p$ output concat into together to measure overall similarity
		\end{itemize}
	\item PCB (Part-based Convolution Baseline) Structure
		\begin{itemize}
		\item ResNet-50 as backbone, before the global average pooling \\
		(last downsampling removed $\Rightarrow$ richer feature granularity)
		\item vertically partitioned into $p$ parts, average pooling on each part \\
		$\Rightarrow$ part average pooling (centroid representation as part-level feature)
		\item 1x1 conv over $p$ vec to extract channel-wise info \& dimension reduction \\
		(sharing weights between parts here)
		\item each vec a separate dense layer for output \\
		$\Rightarrow p$ vector, each able to independently measure similarity / predict ID
		\end{itemize}
	\item RPP (Refined Part Pooling) Structure
		\begin{itemize}
		\item observe within-part INconsistency in hard-uniform partition \\
		$\Rightarrow$ violate part-level feature assumption \\
		(violate: feature vec in same part are similar \& dissimilar to vec in other parts)
		\item $\Rightarrow$ a softmax classifier to predict the part a feature vec should belong to \\
		$\Rightarrow$ model $P(P_i\mid f)$, where $P_i$ $i^\text{th}$ part, $f$ the feature vec
		\item feature representation for $i^\text{th}$ part $\displaystyle = \sum_{f\in\{P_i\}} P(P_i\mid f)\cdot f$ \\
		(weighted average, instead of simple part average pooling) \\
		$\Rightarrow$ essentially, use attention for a "soft-adaptive" partition
		\end{itemize}
	\item Training
		\begin{itemize}
		\item train initial PCB net with all $p$ cross-entropy loss to convergence
		\item replace $p$-part average pooling with $p$ part classifiers (attention generator)
		\item with PCB net fixed, train only part classifiers to convergence \\
		$\Rightarrow$ ensure part-based attention is learned \\
		(since there lacks direct supervision for attention to be part-focused) \\
		$\Rightarrow$ force to collect part-consistent feature (close to original part feature)
		\item train whole net to convergence for fine-tunning \\ 
		$\Rightarrow$ further jointly refine consistency \& performance
		\item augmented by horizontal flip, 60+10+10 epochs
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item model prior on people: more diverse in vertical, more similar in horizontal (as symmetric) \\
		(can be visualized by activation testing on their corresponding final classifier - dense layer)
		\item separate supervision \& independent classifier on each branch is superior \\
		$\Rightarrow$ vital to learn\&use discriminative \underline{part-level features}
		\item waive the need of learning part partitioning algorithm \\
		$\Rightarrow$ less noise source \& NOT depends on other realms e.g. human pose estimation \\
		(as there are gaps between pose estimation \& re-ID)
		\item setting $p$ needs validation $\Rightarrow p=6$ empirically \\
		(small $p$: just global feature, large $p$: some redundant parts being repeated/empty)
		\item within-part INconsistency observed in PCB: \\ 
		by clustering vec \& compute similarity with the average pooling of each part
		\begin{minipage}[r]{0.5\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/topic-reid pcb-rpp within-part inconsistency".png}
		\end{minipage}
		\begin{minipage}[l]{\linewidth}
		where, \\
		partitioned to $p=6$ parts, \\ 
		vec in feature map colored to its closest part \\
		(by similarity between vec \& per-part average pooling) \\
		$\Rightarrow$ overall consistent, yet with some outliers
		\end{minipage}
		\item RPP emphasize \text{within-part consistency}, by refining pre-partitioned parts \\
		$\Rightarrow$ protect part-level feature assumption \\
		(s.t. feature vec within same part are similar \& dissimilar to vec in other parts)
		\item attention as "soft-adaptive" partition \\ 
		$\Rightarrow$ over whole feature maps to aggregate feature for each part \\ 
		$\Rightarrow$ avoid outliers in "hard-uniform" partition \\
		$\Rightarrow$ hence better partition for deep part feature
		\item: \underline{EM-like iterative training for desired effect} (better than only joint training) \\
		(more supervision)
		\end{itemize}
	\end{itemize}
\item MGNet: Discriminative Features with Multiple Granularities \label{DL_CV_ReID_MGNet}
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item CNN extracts feature map, each branch vertically partitioned with various granularity
		\item each branch output part/global-level representation accordingly
		\item all (global + multi-granularity) representation concat as final representation
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item ResNet-50 as backbone (before and including res\_conv4\_1)
		\item global branch: res\_conv5\_1 block + global max pooling + 1x1 conv
		\item part-$n$ branch: split into $n$ strips, each strip a max pooling + 1x1 conv \\
		(max pooling \& 1x1 conv before split to generate branch-level global feature $f^{P_n}_g$) \\
		$\Rightarrow$ employ part-$2$ \& part-$3$ branch \\
		\begin{minipage}[r]{0.5\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/topic-reid PGN".png}
		\end{minipage}
		\begin{minipage}[l]{\linewidth}
		res\_conv5\_1 block + max pooling + 1x1 conv \\ \\
		$n$ strips, each a max pooling + 1x1 conv \\
		\phantom{x}\hspace{1cm} + \\
		max pooling \& 1x1 conv before split into strips \\ 
		($\Rightarrow$ branch-level global feature $f^{P_n}_g$) \\
		\end{minipage}
		\end{itemize}
	\item Training
		\begin{itemize}
		\item pre-trained on ImageNet \& horizontal flip for data augmentation
		\item each branch a softmax loss \& triplet loss \\
		\item softmax: learning basic discrimination in ReID as multi-classification problem \\ 
		(classification to all number of class in dataset) \\
		$\Rightarrow$ no bias/activation for better discrimination
		\item batch-hard triplet: metric learning \& better ranking performance \\
		(embed on final feature after 1x1 conv on each branch)
		\item classfication-before-metric, as shown \\
		(softmax loss on part feature \& before each global feature) \\
		(triplet loss after softmax \& only on each global feature) \\
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multi-scale feature matters $\Rightarrow$ global + multi-granularity local feature \\
		$\Rightarrow$ global branch with downsampling: \underline{global \& coarse feature} \\
		$\Rightarrow$ local branch with no strided conv: \underline{local \& fine feature} \\
		(better than PCB-RPP by a large margin, especially mAP and in hard scene)
		\item able to learn to focus on various part based on its split region \\
		e.g. global branch: main body, part-$3$: small salient feature \& limbs \\
		$\Rightarrow$ enhanced ability to notice \underline{infrequent yet discriminative} feature \\
		(by fine-local feature)
		\item NOT applying triplet loss on local feature of any branch \\
		(local feature not enough for identification, hence not bother to confuse model)
		\item NOT using single feature map for different split of granularity \\
		$\Rightarrow$ s.t. each branch can further mining better feature for its split setting
		\item extra weights are NOT main contributor (experimented)
		\item ensembling helps, yet still better if multi-branch jointly trained \\
		(sharing backbone for joint goal: branches mutually complement others)
		\item $\Rightarrow$ overlap in split matter: branches part-$2/4$ worse than branches part-$3/4$
		\item triplet loss further help network to capture fine-local feature
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Vehicle Re-ID}
\begin{itemize}
\item Multi-Camera Vehicle Tracking and Re-Identification \\ 
based on visual and spatial-temporal features
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item detection on each frame \& multi-object tracking on each camera
		\item extract visual feature for each track
		\item match track across multi-camera $\Rightarrow$ multi-cam tracking
		\item re-ID: visual feature of given probe
		\item re-ranking: use multi-tracking info
		\end{itemize}
	\item Detection
		\begin{itemize}
		\item cascade R-CNN + FPN \& soft NMS
		\end{itemize}
	\item Feature Extraction from Bbox
		\begin{itemize}
		\item global feature: last conv layer \& self-attention to focus on vehicle pixels (masking)
		\item region feature from image parts: 2 vertical + 2 horizontal strips \\
		(from \hyperref[DL_CV_ReID_MGNet]{MGN}: multi-granularity net)
		\item key point feature: extract feature around key points with the help of heat map for key points pred
		\end{itemize}
	\item Single Camera Tracking
		\begin{itemize}
		\item re-project detection to 3D (calibrated cam) $\Rightarrow$ only 3D tracking
		\item Kalman filter \& hungarian association with Mahalanobis distance \\
		$\Rightarrow$ formulate small tracks from frames of target
		\item short tracks association/merge $\Rightarrow$ long tracks
		\end{itemize}
	\item Multi-Camera Tracking
		\begin{itemize}
		\item similarity $S=W_aS_a + W_l(S_l+S_v) + W_dS_d + W_tS_t$, \\ 
		where $W_a, W_l, W_d, W_t$ the weight to balance terms, \\
		$S_a$ the appearance similarity for each track, \\
		$S_l, S_v$ the location\&velocity similarity (if cam overlaps), \\
		$S_d$ the similarity of vehicle motion direction (in a uniform coord), \\
		$S_t$ the temporal constraints: estimated-vs-true travel time (if cam not overlaps)
		\item clustering tracks from different cameras $\Rightarrow$ each cluster is a 3D track
		\end{itemize}
	\item Video-based Re-ID
		\begin{itemize}
		\item identify the track of query \& prefer image also belongs to that track
		\item meta data constraint: using car-type classification, etc.
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item a systematic approach: assemble multiple models \& constraints
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Image Styling}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Content Image $C$
		\begin{itemize}
		\item the image containing the spatial info (content)
		\end{itemize}
	\item Style Image $S$
		\begin{itemize}
		\item the image containing the style of presenting the content
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Generated Image $G$
		\begin{itemize}
		\item a image with content from $C$ drawn in style of $S$
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/topic-image style transfer nst overview".png}
		\end{figure}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Texture Synthesis}
\subsubsection{Neural Style Transfer}
\begin{itemize}
\item Neural Style Transfer
	\begin{itemize}
	\item Learning Goal
		\begin{itemize}
		\item given input $C,S$ with output $G$, loss $L = \sum_{l} \left[\alpha L_\text{content}(C,G) + \beta L_\text{style}(C,S)\right]$, \\
		where $l$ is sum over chosen hidden layers of the CNN
		\item $\Rightarrow$ minimize content \& style difference
		\end{itemize}
	\item Content
		\begin{itemize}
		\item given input, the activations from a set of (hidden) layers of the net
		\item $\Rightarrow$ similarity of $C, G$ measured as $\sum_l d(a^{l(C)}, a^{l(G)})$, \\ 
		where $a^{l(\cdot)}$ the feature maps at layer $l$ given the input, $d(\cdot)$ a distance function \\
		(e.g. $d(x_1,x_2) = \norm{x_1 - x_2}^2$)
		\end{itemize}
	\item Style
		\begin{itemize}
		\item given input, the correlation between activations across channels, for chosen layers \\
		$\Rightarrow$ correlation matrix across feature map at each channel as style matrix \\
		(actually, gram matrix)
		\item let $a_{i,j,k}^l$ the activation at a $h\times w\times c$ conv kernel location $i,j,k$ in layer $l$ \\ 
		$\Rightarrow$ style (gram) matrix $M^l_{k,k'} = \sum_{i,j}a^l_{ijk}\cdot a^l_{ijk'}$, for all $k,k'\in\{1,...,c\}$
		\item $\Rightarrow$ similarity of $S, G$ measured as $\sum_l \left[ \frac 1 {(2 h^l w^l c^l)^2} d(M^{l(S)}, M^{l(G)}) \right] $ \\
		where $M^{l(\cdot)}$ the gram matrix at layer $l$ given the input, $d(\cdot)$ a distance function, with normalization term $\frac 1 {(2 h^l w^l c^l)^2}$ \\
		(e.g. $d(x_1, x_2) = \norm{x_1-x_2}^2_F$, the euclidean norm between matrices)
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/topic-image style transfer nst style matrix".png}
		\end{figure}	
		\end{itemize}
	\end{itemize}
\item Fast Style Transfer
\item Fast and Multiple Style Transfer
\end{itemize}

\subsection{Point Cloud Data Processing}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Point Cloud
		\begin{itemize}
		\item from lidar $\Rightarrow$ $3$-D position x,y,z \& reflection intensity
		\item from radar $\Rightarrow$ $2$-D bird-view x,y \& intensity (rcs) \& speed (along radial direction)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item $3$-D Environment Modeling
		\begin{itemize}
		\item bounding box
		\item segmentation (per-point classification)
		\item instance segmentation, etc.
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Sparsity
		\begin{itemize}
		\item point gets much sparser in distance (e.g. $>$ 40m)
		\end{itemize}
	\item Varying Density
		\begin{itemize}
		\item due to occlusion, distance etc.
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Common Preprocessing}
\begin{itemize}
\item Voxelization
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item create $3$-D pixel, the voxel
		\end{itemize}
	\item Procedure
		\begin{itemize}
		\item apply $3$-D grid on the space \\ 
		$\Rightarrow$ each point resides in a spatial cell, the voxel
		\end{itemize}
	\end{itemize}
\item Bird-view Projection
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item project onto $2$-D map of a bird-view perspective \\
		$\Rightarrow$ better resolution due to reduced dimension
		\end{itemize}
	\item Procedure
		\begin{itemize}
		\item apply $2$-D grid on the ground \\
		$\Rightarrow$ each point resides in a cell, or, each cell consists of several point
		\item extract height information from points in each cell \\
		$\Rightarrow$ each pixel with a height $h$
		\end{itemize}
	\end{itemize}
\item Cylindrical Projection (Frontal View)
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item project onto $2$-D map of pilot perspective \\
		$\Rightarrow$ better align with camera perspective
		\end{itemize}
	\item Procedure
		\begin{itemize}
		\item given $3$-D cartesian coord $x,y,z$ \\ 
		$\Rightarrow$ spherical coord $r = \sqrt{x^2+y^2+z^2}, \theta = \arctan{\frac y x}, \phi=\arcsin{\frac {z} {r}}$
		\item slicing on horizontal \& vertical angle: each point resides in a $3$-D slice \\ 
		$\Rightarrow$ normalized by angle resolution $\theta' = \floor*{\frac \theta {\delta \theta}}, \phi' = \floor*{\frac \phi {\delta \phi}}$
		\item each pixel $(\theta', \phi')$ extract depth $d=\sqrt{x^2+y^2}$ \& height $z$ from its point(s)
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Point Cloud}
\begin{itemize}
\item LMNet
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item cylindrical projection to generate feature map from point cloud
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item given a pixel (corresponding to a lidar point) $p$ in a bounding box \\ 
		$\Rightarrow$ encode box under coord originated at $p$ \\
		$\Rightarrow$ axises: $x$ along radial direction; $y$ parallel with horizontal plane; $z$ accordingly
		\item encode $8$ corner $\Rightarrow$ $24$ channel encoding for each box
		\end{itemize}
	\item Classification Encoding
		\begin{itemize}
		\item per-pixel classification (car, pedestrian, cyclist, none)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item per-pixel regression \& classification $\Rightarrow$ standard CV detection
		\item non-max suppression as postprocessing \\
		(score modified to be the num of nearby-box)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item convs - max pooling - dilated convs - unpooling - branch for regression/classification
		\end{itemize}
	\item Training
		\begin{itemize}
		\item point-wise weighting \\
		$\Rightarrow$ regression: consider box size for each class \\
		$\Rightarrow$ classification: downsample background pixel (standard)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item real-time timing due to simple structure with dilated conv \\
		(though performance hurt...) \\
		$\Rightarrow$ enable further fusion with image, pertaining real-time timing
		\end{itemize}
	\end{itemize}
\item PointNet
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item normalize point cloud (x,y,z) into unit sphere
		\end{itemize}
	\item Inference \& Structure \\
	\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-point cloud pointnet structure".png}
		\begin{itemize}
		\item input: list of points as unordered set
		\item mlp: shared dense layer to process each point (1x1 conv on 1D point list) \\
		$\Rightarrow$ symmetric processing to respect unordered set
		\item max pooling: summarize global feature
		\item T-net the simplified transformer to apply spatial transform on points \\
		$\Rightarrow$ directly regress transformation matrix \& matrix multiplication to apply \\
		(also consists of 1x1 conv + max pooling)
		\end{itemize}
	\item Inference - Detection
		\begin{itemize}
		\item inference as segmentation \\ 
		i.e. concat global feature \& per-point feature to produce per-point pred
		\item generate bbox for group of connected points of same label (i.e. a post processing)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item loss = task specific loss + constraint $\norm{I-AA^T}^2$, \\
		where $A$ the transform matrix pred by T-net \\
		$\Rightarrow$ avoid info loss in non-orthogonal transform \\
		(also to have a more stable optimization)
		\item augmentation: point set rotation \& point jitter
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item respect unorderness via shared (1x1 conv / dense layer) \& symmetric (pooling) function \\
		$\Rightarrow$ avoid voxelization
			\begin{itemize}
			\item easier to learn transformation invariance (via T-net)
			\item not constrained by resolution
			\item not affected by variable point density in space \\
			$\Rightarrow$ processing complexity $\mathcal O(n)$, where $n$ the point number
			\end{itemize}
		\item max pooling to summarize the point cloud \\
		$\Rightarrow$ select informative point sparse critical points \\
		(v.s. avg pooling / attention, which correlates to all the input points)
			\begin{itemize}
			\item critical point: point with element activates the max pooling \\
			(contributes to the global feature) \\
			$\Rightarrow$ pred will NOT change if critical points not missing
			\item upper-bound point: point that will not activates the max pooling \\
			(given a fixed set of critical points) \\
			$\Rightarrow$ pred will NOT change even if those outlier exist
			\end{itemize}
		$\Rightarrow$ persist robustness to missing data \& outlier \\
		(affected by the dimension of max pooling i.e. len of global feature)
		\item $\Rightarrow$ \underline{efficient \& robust point cloud processing}
		\item yet, lack of hierarchical local$\rightarrow$global context mining \\
		$\Rightarrow$ easy to lose fine/local geometry detail \\
		(e.g. object/instance level)
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item point net able to mine semantic info by concat [point, global] feature
			\begin{itemize}
			\item able to learn point-to-point matching between 2 point cloud \\
			(corresponding critical point should activate the same location)
			\item able to predict point normal
			\item loc in dense layer output is activated by points in different region \\
			(by visualization)
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item PointNet++
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item normalize all points to be 0-mean \& within unit sphere
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item input: unordered set of points
		\item multiple downsampling stage to extract local$\rightarrow$global multi-scale info
		\item for segmentation, upsampling back to generate per-point pred
		\end{itemize}
	\item Structure - Downsampling
		\begin{itemize}
		\item fartherest point sampling to sample set of centroid points \\
		(need to have distance measurement between points)
		\item generate local group for each centroid, by sampling $\le K$ points inside the ball \\
		(still, based on distance measure)
		\item shared point net to process each group into a feature vector \\
		(point feature = $[d, C]$, where $d$ the coord relative to centroid, $C$ the context feature)
		\item for multi-scale feature inside each group: \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-point cloud pointnet++ multi-scale in local group".png}
			\begin{itemize}
			\item MSG: use different ball size \& concat all feature Vectors
			\item MRG: concat [feature from point group, feature from corresponding points of last layer]
			\end{itemize}
		$\Rightarrow$ to avoid invalide info from too few points
		\end{itemize}
	\item Structure - Upsampling
		\begin{itemize}
		\item interpolation context feature based on coord $d$
		\item for each point, concat [interpolated feature, correponding low-level point feature]
		\item for each point, multiple shared dense layer (1x1 conv on point list) to refine feature
		\end{itemize}
	\item Training
		\begin{itemize}
		\item augmentation: 
		\item random point drop out to learn ability to handle missing data (effective)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item hierarchical point net $\Rightarrow$ able to capture multi-scale context (local$\rightarrow$globa)
		\item sampling to produce local context for point \\
		$\Rightarrow$ need to consider which metric space (distance measure) to use
		\item able to use various metric space to better handle non-rigid object/scene \\
		$\Rightarrow$ yet, introduce extra engineering effort
		\item INefficient \& slow, due to too much sampling in a single forward (1 for each downsampling stage)
		\item still, NOT able to naturally incorporate with tasks that acquire instance info (e.g. detection) 
		\end{itemize}
	\end{itemize}
\item VoxelNet
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item voxelization
		\item random sampling points in each voxel to be at most $T$ points a voxel
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $x,y,z$ for center position; $l,w,h$ for size; $\theta$ for orientation, the yaw rate \\
		$\Rightarrow x_g,y_g,z_g,l_g,w_g,h_g,\theta^g$ the ground truth box \\
		$\Rightarrow x_a,y_a,z_a,l_a,w_a,h_a,\theta^a$ an anchor box
		\item normalized residual position: $\Delta x=\frac{x_g-x_a}{\sqrt{l_a^2+w_a^2}}, \Delta y=\frac{y_g-y_a} {\sqrt{l_a^2+w_a^2}}, \Delta z=\frac{z_g-z_a}{h_a}$
		\item normalized size ratio: $\Delta l=\log(\frac {l_g}{l_a}), \Delta w = \log(\frac{w_g}{w_a}), \Delta h=\log(\frac{h_g}{h_a})$
		\item residual orientation: $\Delta \theta = \theta_g - \theta_a$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item voxelization \& sampling: randomize the points \& fill in the voxel \\
		(each voxel contain at most $T$ points) \\
		$\Rightarrow$ avoid compute on empty voxel, which can be $ge 90\%$ due to hily variable point density
		\item sparse tensor representation: each voxel = $[d, C]$, where d the coord, $C=[p...]$ a list of points \\
		$\Rightarrow$ parallelize per-voxel feature extraction (avoid empty voxel)
		\item per-voxel feature extraction \& fill in the 3D space \\
		$\Rightarrow$ generate feature volume (4D tensor)
		\item 3D conv for context mining
		\item project to bird-view by concat the feature volume along the height-axis \\
		i.e. reshape [64 channel, 2 height, 400 width, 352 depth] $\rightarrow$ [128, 400, 352]
		\item RPN to predict classification \& regression for 3D bbox
		\end{itemize}
	\item Voxel Feature Encoding Layer
		\begin{itemize}
		\item voxel $V=\{p_i=[x_i,y_i,z_i,r_i]\in \mathbb R^4 \}_{i=1,...t}, t<T$
		\item augment each point with its offset to the centroid $(v_x,v_y,v_z)$, the mean of $p\in V$ \\
		$\Rightarrow p'_i = [x_i,y_i,z_i,r_i,x_i-v_x, y_i-v_y, z_i-v_z]$ \\
		(only for raw points)
		\item 1x1 conv (with batch norm, ReLu) on points in voxel: $p'_i\rightarrow f_i$
		\item element-wise max pooling on $f_i\in V$: voxel-wise feature $\hat f$
		\item augment each processed feature point with voxel feature: $f_i^\text{out}=[f_i,\hat f]$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stacked voxel feature encoding layer: deep net to extract feature for each voxel
		\item 3D conv middel layer (to reduce from 3D feature volume to 2D feature map)
		\item modified RPN for 3D detection: a downsample \& upsample pipeline
		\end{itemize}
	\item Training
		\begin{itemize}
		\item normalize pos/neg classification loss (entropy) by their num
		\item smooth L1 for regression loss
		\item augmentation: box (collision-free) perturbation in location, size, rotation
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item voxel feature extraction: voxel-level PointNet \\ 
		$\Rightarrow$ able to concat with various detection models
		\item sampling via voxelization + conv for local context mining \\
		(instead of repeated sampling in pointnet++)
		\item generalize 3D point cloud to detection net (RPN), with 3D anchor
		\item yet, 3D conv too slow, even slower than pointnet++
		\end{itemize}
	\end{itemize}
\item Point R-CNN
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}
\subsubsection{Point Cloud + Image Fusion}
\begin{itemize}
\item MV3D: Multi-view 3D Object Detection
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item 
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item location
		\item size
		\item orientation
		\end{itemize}
	\item 3D Proposal Network
		\begin{itemize}
		\item bird-view of point cloud $\Rightarrow$ propose reliable 3D bbox
		\end{itemize}
	\item Region-based Fusion Network
		\begin{itemize}
		\item project 3D proposal to feature maps of multi-view
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\item Frustum PointNet
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item 2-stage detection pipeline to produce 2D bbox
		\item project to 3D point cloud $\Rightarrow$ a slice (frustum) of point cloud \\
		\item rotate axis to be aligned with center line of frustum \\
		$\Rightarrow$ reduce input pose variance \& improve rotation invariance
		\item point segmentation to select target-related points \\
		(provided with object category pred by 2D detector)
		\item shift origin to be at the central of selected points \\
		$\Rightarrow$ local coord to reduce variance \\ 
		(offset to local origin $<<$ offset ot sensor)
		\item 3D localization to predict object (3D box) center, based on selected points
		\item further shift origin to the predicted center
		\item produce 3D box [center, size, orientation] \\
		(based on selected points under transformed coord)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item 2-stage detector: RPN based
		\item point segmentation: point net (++) based
		\item center prediction: point net based
		\item 3D box regression: point net (++) based
		\end{itemize}
	\item Training
		\begin{itemize}
		\item hybrid classification \& regression loss for prediction box \\
			\begin{itemize}
			\item classify the target into predefined size \& orientation
			\item regress the offset to every anchor size \& orientation
			\end{itemize}
		(box center pred by regression)
		\item additional corner loss: sum of offset of 8 corners between pred \& label box \\
		$\Rightarrow$ provide joint optimization for center, size, orientation
		\item smooth L1 loss for reg \& entropy for cls
		\item augmentation: resample and perturb points
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item detector produce 2D box + point net to translate into 3D box
		\item instance segmentation on 3D space \\
		$\Rightarrow$ easier to remove fore-/back- ground clutter \\
		(as 3D points can be more separated v.s. perspective view in img)
		\item multiple normalization between sub-nets to reduce input transformation \\
		(thus easier to learn) \\
		$\Rightarrow$ canonical \& aligned representation is important
		\item may fail when multiple same-class object in one frustum \\
		(as binary segmentation assume only 1 target) \\
		$\Rightarrow$ improved when combined with proposal from bird-view
		\item more like an engineering approach: pipeline with multiple steps
		\end{itemize}
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Natural Language Processing}
\subsection{Language Representation} \label{DL_NLP_Langrep}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Language Token/Corpus
		\begin{itemize}
		\item words, sentences, paragraphs, ... $\Rightarrow$ can be language at various level
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Distributed Vector Representations as Embedding Matrix $E_{M\times N} = [e_1,...,e_N]$
		\begin{itemize}
		\item $e$ the column vectors, $M$ the desired embedding length, $N$ the total tokens num \\
		$\Rightarrow$ look up for the desired embedding \\
		(NOT using matrix multiplication due to sparsity from one-hot encoding)
		\item distributed representation: decomposed yet meaningful \\
		$\Rightarrow$ fight the curse of dimensionality
		\end{itemize}
	\item $\Rightarrow$ Meaningful Vector
		\begin{itemize}
		\item able to measure the (dis-)similarity of between tokens (words) \\
		$\Rightarrow$ semantic meaning: "Germany"-"Berlin" \& "France"-"Paris" \\
		$\Rightarrow$ syntactic meaning: "quick"-"quickly" \& "slow"-"slowly" \\
		(e.g. $e_\text{man} - e_\text{woman} \approx e_\text{king} - e_\text{queen}$, where $e_\text{text}$ the embedding for word "text") 
		\item $\Rightarrow$ allow NLP model to be more robust \& generalize better
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Problem of Bias
		\begin{itemize}
		\item word embedding reflect biases of text used to train the model \\
		e.g. "father-doctor" as "mother-nurse" $\Rightarrow$ gender bias
		\item $\Rightarrow$ can cause discrimination when making decision
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Overview}
\begin{itemize}
\item Character Embedding
	\begin{itemize}
	\item One-hot Encoding
		\begin{itemize}
		\item a one-hot vector with length $26$
		\end{itemize}
	\end{itemize}
\item Word Embedding
	\begin{itemize}
	\item Word Dictionary
		\begin{itemize}
		\item a collection of high-frequency word, embedded as one-hot vector
		\item special token \textless{}UNK\textgreater{} for unknown word
		\end{itemize}
	\item Features from Rule Model
		\begin{itemize}
		\item number at each vector location denotes the score for the word matching a rule \\
		(e.g. location for "is\textunderscore{}food" contains score $s\rightarrow1$ for "apple", $s\rightarrow0$ for "man")
		\end{itemize}
	\item Part-of-Speech (POS) Tag
		\begin{itemize}
		\item 
		\end{itemize}
	\item \hyperref[DL_NLP_Langrep_word2vec]{Word2Vec Embedding}
		\begin{itemize}
		\item construct supervised learning from UNlabeled corpus
		\end{itemize}
	\item \hyperref[DL_NLP_Langrep_GloVe]{Global Vector for Word Embedding (GloVe)}
		\begin{itemize}
		\item linear model with simple optimization goal
		\end{itemize}
	\item RNN Encoder
		\begin{itemize}
		\item apply RNN model as encoder on characters in the word \\
		$\Rightarrow$ no more <UNK> or unknown word
		\end{itemize}
	\end{itemize}
\item Sentence/paragraph Embedding
	\begin{itemize}
	\item RNN Encoder
		\begin{itemize}
		\item apply RNN model as encoder on words in the sentence \\ 
		(last hidden layer as encoding)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Word2Vec Embedding} \label{DL_NLP_Langrep_word2vec}
\begin{itemize}
\item N-gram Model
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item simple network to predict the $N+1^\text{th}$ word given previous $N$ words as input \\
		(e.g. using single softmax layer)
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item $E$ randomly initialized \& all words in corpus encoded in one-hot vector
		\item forward prop: word in one-hot $\rightarrow$ lookup $E$ $\rightarrow$ linear layer $\rightarrow$ softmax to predict
		\item training: update linear layer parameters \& matrix $E$ as weights \\ 
		(with cross-entropy loss)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn $P(t|c)$, where $t$ the target word, $c$ the previous $N$ context words
		\item setup a even larger training set from a large corpus
		\end{itemize}
	\item Generalization
		\begin{itemize}
		\item more context: take input from both previous and after words
		\item less \& close context: take only the last word as input $\Rightarrow$ 1-gram model
		\end{itemize}
	\end{itemize}
\item Skip-gram Model
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item choose only $1$ single word as context word \\
		$\Rightarrow$ balance sampling w.r.t. word frequency (e.g. prevent tons of "the", "a", ...)
		\item randomly choose other word(s) in the sentence as target word(s)
		\item $\Rightarrow$ to predict target word(s) given only context word as input \\ 
		$\Rightarrow$ learn word vector representations that are good at predicting the nearby words
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item same as N-gram model
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item harder supervised learning task, yet goal is to learn $E$
		\item better reflect the statistic: similar word appear in similar context \\
		(e.g. "soviet"-"union" appears much more often than "soviet"-"sasquatch") \\
		$\Rightarrow$ embedding for similar target word adjusted with similar gradients \\
		$\Rightarrow$ lie closer in vector space
		\item cons: softmax over large word dict $\Rightarrow$ low computation \\
		$\Rightarrow$ mitigated by hierarchical softmax, noise contrastive estimation (NCE)
		\end{itemize}
	\end{itemize}
\item Skip-gram with Negative Sampling
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item choose a pair of context and target word $(c,t)$ as positive example
		\item generate $k$ negative examples by: same context word $c$ \& random word $t'$ as target
		\item given a pair of words, binary classification: is a (context, target) pair? \\
		$\Rightarrow$ distinguish valid target word from $k$ draws from noise distribution
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item detect meaningful $(c,t)$ pair/phrase by heuristic method \\ 
		e.g. if $c,t$ co-appear within 10-words distance more than a threshold, etc.
		\item $k=5-20$ for small train set; $k=2-5$ for large train set \\
		(larger noise to avoid overfitting)
		\item sample random word $t'$ from modified uniform distribution $\frac 1 Z U(t)^{3/4}$ over words
		\item subsample frequent words: sampled $t'$ discarded by probability $P(t') = 1-\sqrt{\frac {thr} {f(t')}}$ \\
		where $thr$ a threshold, $f(t')$ the frequency of $t'$ in corpus \\
		(to avoid meaningless words like "the", "a", etc.)
		\item $E$ randomly initialized \& all words in corpus encoded in one-hot vector \\
		(forward prop similarly)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn $P(y=1|c,t)$ via logistic regression \\ 
		$\Rightarrow$ much computationally affordable compared to giant softmax (less weights) \\
		$\Rightarrow$ much simpler approach than hierarchical softmax \& NCE
		\item non-linear model (logistic reg) also prefers linear structure of word embedding \\
		$\Rightarrow$ cosine distance still measures (dis-)similarity
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Global Vector for Word Embedding (GloVe)} \label{DL_NLP_Langrep_GloVe}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Context-Target Matrix $X$
		\begin{itemize}
		\item $x_{ij}$: the count of times word $w_i$ appear in the context of word $w_j$ \\
		(context definition can be non-symmetric)
		\end{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item given embedding matrix $E$, minimize $\sum_{i,j}f(x_{ij})(\theta_i^Te_j-\log x_{i,j})^2$, \\
		where $e_j$ the embedding for $w_j$, $\theta_i$ the weights associated with $w_i$
		\item $f(x_{ij})$ a weighting term to balance infrequent-frequent words \\
		($f(x_{ij})$ for $x_{i,j}=0$, preventing $-\inf$ from $\log0$)
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item gradient decent directly optimize the simple objective
		\item final embedding for word $w, w_e = \frac 1 2 (e_w+\theta_w)$ \\
		$\Rightarrow$ as $\theta_w, e_w$ in objective interchangeable
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item directly model the linear structure in word representation \\ 
		(project input $e$ directly to output $\theta^T e$)
		\item final linear structure probably NOT align with human interpretable axis \\
		$\Rightarrow$ yet probably a combination of them (from a higher view)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Addressing Bias in Word Embedding}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item a trained word embedding
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item identify the bias in embedding
		\item eliminate the bias if it appears in undesired places
		\end{itemize}
	\item Identify Bias Direction
		\begin{itemize}
		\item singular value decomposition to identify the axises where biases lie \\
		(similar to a PCA)
		\item e.g. principle component of $e_\text{man}-e_\text{woman}, e_\text{male}-e_\text{female}, ...$
		\end{itemize}
	\item Neutralize
		\begin{itemize}
		\item for all NOT definitional word (where bias should NOT appear) \\
		$\Rightarrow$ project to axises orthogonal to bias axises (to get rid of bias)
		\item e.g. project $e_\text{doctor}$ to the axises to reduce component in bias axises
		\end{itemize}
	\item Equalize Pairs
		\begin{itemize}
		\item for all definitional word (where bias should appear) \\
		$\Rightarrow$ adjust their distance towards non-definitional word to be the same \\
		(may train/handpick all definitional words, which is only a small set)
		\item e.g. make sure $d(e_\text{boy},e_\text{doctor}) = d(e_\text{girl},e_\text{doctor})$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Contextualized Word Embedding}
\begin{itemize}
\item ELMo
\item BERT
\end{itemize}

\subsection{Language Modeling}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item a word with sequence of characters
		\item a sentence with sequence of words/characters
		\item a paragraph with sequence of sentences/words/characters
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Probability Distribution
		\begin{itemize}
		\item model the appearance probability of the sequence $P(\text{z}^1,...,\text{z}^{t})$, \\
		where $z^t$ the token at time $t$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item at each time, output token distribution conditional on previous token(s) \\
		$\Rightarrow y^t = p(z^t|z^1,...,z^{t-1})$, where $z^t$ the token at time $t$
		\item $\Rightarrow$ sequence probability $\displaystyle P(z^1,...,z^{t}) = P(z^1)P(z^2|z^1)...P(z^T|z^1,...,z^{T-1})= \prod^T_{t}y^t$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item $\mathbf 0$ vector as both (initial) hidden state \& input at time $0$ \\
		$\Rightarrow$ estimate $y^1=p(z^1)$, the distribution for being the $1^\text{st}$ token
		\item for time $t=2,...,T$, take input $x^2,...,x^T$ with hidden state $h^1,...,h^{T-1}$ \\ 
		$\Rightarrow$ estimate each conditional distribution (conditioning by passing hidden state)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item for time $1$, input $x^1=\mathbf 0$, previous hidden state $h^0=\mathbf 0$
		\item for time $t=2,...,T$, input $x^t = {z^*}^{t-1}$ the true token of $t-1$ in the given sequence
		\end{itemize}
	\item Generative Model: Sampling New Sequence
		\begin{itemize}
		\item sampling the first token $\hat{z}^1$ according to the distribution $y^1$
		\item for $t=2,...$, take input $x^t=\hat{\text{z}}^{t-1}$, the token sampled from $y^{t-1}$
		\item until $t>T$ or the end signal sampled (e.g. the period "." in a sentence)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Masked Language Model}

\subsection{Name-Entity Recognition}
\subsubsection{}
\begin{itemize}
\item 
\end{itemize}

\subsection{Sentiment Classification}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item a sentence / paragraph
	\end{itemize}
\item Goal
	\begin{itemize}
	\item predict the degree of positive/negative attitude
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item small training set
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Many-to-one RNN
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item each word encoded by word embedding
		\item RNN scanning through paragraphs
		\item last hidden layer as paragraph representation \& used to classify/regress
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Neural Machine Translation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence 
		\begin{itemize}
		\item typically sentence, can be also multiple sentences (paragraph)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item generated sentences in desired language
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Approaches}
\begin{itemize}
\item RNN Encoder-Decoder
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item an RNN encodes the input sequence by its last hidden layer
		\item input encoding used as initial hidden state for decoder RNN
		\item decoder RNN generates (conditional) distribution over words at each step \\
		$\Rightarrow$ for time $t, y^t = p(z^t|x^1,...,x^{T_x}, \hat{z}^1,...,\hat{z}^{t-1})$, \\
		where $z^t$ the token at time $t$, $\hat{z}^1,...,\hat{z}^{t-1}$ the tokens chosen from $y^1,...,y^{t-1}$ \\
		($z^t$ a random variable, $\hat{z}^t$ a concrete assignment, $y^t$ a conditional distribution)
		\item unroll until stop sign generated
		\end{itemize}
	\item Understanding: Conditional Language Model
		\begin{itemize}
		\item decoder functions like language modeling, only different in its initial hidden state
		\item $\Rightarrow$ measure the conditional distribution $\displaystyle p(z^1,...,z^{T_y}|x^1,...,x^{T_x})=\prod_{t=1}^{T_y}y_t$, \\
		where $z^1,...,z^{T_y}$ the generated sequence, $x^1,...,x^{T_x}$ the input sequence
		\end{itemize}
	\item Improvement
		\begin{itemize}
		\item combined with attention model
		\end{itemize}
	\end{itemize}
	
\item Transformer: Attention is All You Need
	\begin{itemize}
	\item Embedding
		\begin{itemize}
		\item word embedding: classic embedding + positional embedding
		\item positional embedding: \\
		$\left\{\begin{alignedat}{2} &PE(pos, 2i) &&= \sin\left( \frac{pos}{10000^{2i/d_\text{model}}} \right)  \\ &PE(pos, 2i+1) &&= \cos\left( \frac{pos}{10000^{2i/d_\text{model}}} \right), \end{alignedat}\right.$\\
		where $d_\text{model}=512$ the embedding len, $pos$ the word position, $i$ the dim of vector \\
		$\Rightarrow$ any offset $k, PE(pos+k)$ can be represented by a linear func of $PE(pos)$ \\
		$\Rightarrow$ provide positional info easy for model to use \\
		(also allow model to extrapolate sequence longer than training examples)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item encoder maps input $(x_1,...,x_n) \rightarrow \mathbf z=(z_1,...,z_n)$ (in parallel)
		\item decoder maps $\mathbf z \rightarrow (y_1,...,y_m)$ one at a time (sequentially)
		\item beam search with bean size $=4$ \& length penalty $\alpha=0.6$
		\item maximal output len $=$ input len + 50
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item encoder: stack of $N=6$ layers, each: [multi-headed self-attention, dense layer], \\
		with residual connection around both sub-layers (attention and dense) \\
		\& layer normalization after each residual connection
		\item multi-headed self-attention: $8$ heads with $64$-dim vector for key, query \& output \\
		(vs. word embedding of dim $d_\text{model} = 512$: same complexity with more representability)
		\item encoder-decoder attention: \\
		self-attention with key-value from encoder \& query from last decoder \\
		$\Rightarrow$ mimic the classic encoder-decoder attention
		\item decoder: a stack of layers similar to encoder + encoder-decoder attention \\
		$\Rightarrow$ restricted attention (via masking) to output sequentially
		\item overview: (scaled dot-product attention = self attention) \\
		\includegraphics[width=0.7\linewidth, center]{"./Deep Learning/plot/topic-nmt transformer".png}
		\end{itemize}
	\item Training
		\begin{itemize}
		\item 8 NVIDIA P100 for 3.5 days...4000 steps warm up + 300,000 steps training \\
		(still, much faster training than other models...)
		\item adam optimizer \& batching based on approximate sentence length
		\item drop-out (dropout rate = $0.1$) after each sublayer (before the residual add)
		\item label smoothing with value $\epsilon_{ls} = 0.1$ ???
		\item final model = the average of the last 20 checkpoints
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item no sequential component use in encoding \\
		(entirely replace the RNN with attention in encoder-decoder structure) \\
		$\Rightarrow$ parallelized encoding phase
		\item the dimension of key-value should NOT be too small \\
		$\Rightarrow$ determining relations can be difficult \\
		$\Rightarrow$ may use non-linearity
		\item model structure can generalize to other challenging NLP tasks \\
		e.g. constituency parsing: structural constraint \& much longer output than input \\
		(also, little annotated data, $\sim 40K$ sentences)
		\end{itemize}
	\end{itemize}

\item BERT
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsubsection{Choosing Output Sequence}
\begin{itemize}
\item Greedy Search
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item choose the words of highest (conditional) probability at each time step
		\end{itemize}
	\end{itemize}
\item Beam Search
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item with a vocabulary size of $N$, a bean with size $b$, input sequence $\mathbf x = x^1,...,x^{T_x}$
		\item to start, choose top-$b$ tokens (among $N$ tokens) at the $1^\text{st}$ step
		\item for step $t$, input each previous stored $b$ tokens to have $b$ conditional distributions
		\item choose the top-$b$ token pairs (among $b\times N$ pairs) regarding joint probability \\
		$\Rightarrow P(z^1,...,z^t|\mathbf x) = P(z^t|z^1,...,z^{t-1},\mathbf x)P(z^1, ..., z^{t-1}|\mathbf x)$ 
		\end{itemize}
	\item Normalization by Length
		\begin{itemize}
		\item reason: short sequence with less $y^t\in[0,1] \Rightarrow$ larger in general
		\item choose $t^\text{th}$ pair regarding the normalized probability $\frac 1 t P(z^1,...,z^t|\mathbf x)$
		\item $\Rightarrow$ more numerically stable
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item approximately search the sequence with highest joint (conditional) probability \\
		$\Rightarrow$ try to maximize $P(z^1,...,z^{T_y}|x^1,...x^{T_x})$
		\item similar to viterbi algorithm in HMM $\Rightarrow b = 1$ reduce to greedy search
		\item $B$ usually chosen in $10$ in research, $>1000$ in commercial system \\
		(still faster than BFS/DFS, yet no guarantee on finding best result)
		\end{itemize}
	\end{itemize}
\item 
\end{itemize}

\subsection{Speech Recognition}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item an audio sample, with each frame as a time step
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item text (words/sentences) corresponding to the audio
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Variable Timing
		\begin{itemize}
		\item output (letter/words) usually has much less time steps than input (audio frames) \\
		$\Rightarrow$ multiple input time steps corresponding to same output time step
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Learning Objective}
\begin{itemize}
\item Connectionist Temporal Classification (CTC) Loss
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item avoid learning boundaries and timings
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item two sequences considered equivalent if they differ only in alignment, ignoring blanks \\
		$\Rightarrow$ remove duplicate token (e.g. letters) from both sequence before comparison
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN Encoder-Decoder
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item encoder scan through audio frames \& decoder output letter/punctuation/"blank"/"space"
		\item "blank": no symbol v.s. "space": delimiter for letters $\rightarrow$ words
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Trigger Word Detection}
\begin{itemize}
\item Goal
	\begin{itemize}
	\item trigger word: a specific predefined audio signal to invoke system (e.g. xiaodu xiaodu)
	\item detect where trigger word included in an audio (if any)
	\end{itemize}
\item Train Set Setup
	\begin{itemize}
	\item Basic
		\begin{itemize}
		\item $0$ for frames not corresponding to trigger word; $1$ for frames consisting trigger words
		\end{itemize}
	\item upsampling
		\begin{itemize}
		\item upsampling positive example: extends $1$ label a few frames after the trigger words \\
		(as trigger word often appears once in an interaction with system)
		\end{itemize}
	\end{itemize}
\item Classic Approach
	\begin{itemize}
	\item RNN Encoder-Decoder
		\begin{itemize}
		\item encoder scan through audio \& decoder output 0-1 classification at each step
		\end{itemize}
	\item Conv RNN
		\begin{itemize}
		\item a fixed window to better capture context for detecting trigger word
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Machine Reading Comprehension}
\subsubsection{RNN with Attention}
\subsubsection{Convolution with Self-attention - QAnet}

\section{CV \& NLP}
\subsection{Image Caption}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input as the target of description
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Natural Expression
		\begin{itemize}
		\item description of the image in natural language, e.g. English
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Scene Understanding
		\begin{itemize}
		\item basic: determine objects in image
		\item further: realize relationships \& connection inside the image
		\end{itemize}
	\item Descriptive Representation
		\begin{itemize}
		\item able to map such understanding into a descriptive representation \\
		i.e. natural language
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Approaches}
\begin{itemize}
\item Neural Image Caption
	\begin{itemize}
	\item Visual Information
		\begin{itemize}
		\item encoded by CNN backbone into a $1$-D vector
		\end{itemize}
	\item Word Information
		\begin{itemize}
		\item a set of word selected beforehand
		\item word embedding performed
		\end{itemize}
	\item Language Generation
		\begin{itemize}
		\item generated by an LSTM decoder
		\item combining info: visual encoding as initial state of LSTM
		\item process: LSTM gives each word a to-be-selected probability at each time step
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item sampling: sample each word according to the distribution given by LSTM
		\item beam search: iteratively consider extending $k$ best sentence of length $t$ to $t+1$ \\
		$\Rightarrow$ select $k$ best sentence of length $t+1$ from all resulted sentences
		\end{itemize}
		(beam search selected in the paper)
	\end{itemize}

\item \underline{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention} \label{DL_CVNLP_Imgcap_show_attend_tell}
	\begin{itemize}
	\item Motivation \& Analysis
		\begin{itemize}
		\item does NOT explicitly use object detection net \\ 
		$\Rightarrow$ model visual relations beyond objectness
		\item better capture low-level detail for descriptive language \\
		$\Rightarrow$ better represent visual perception than a single feature vector \\
		(which is usually used as starting state for RNN-decoder in previous works)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item CNN downsample image to $n\times n=L$ feature map, with channel dimension $D$ \\
		$\Rightarrow$ feature map $\mathbf a = \{\mathbf a_1, ..., \mathbf a_L \}, \mathbf a_i\in\mathbb R^D$: spatial region for attention to choose
		\item LSTM decoder with init states predicted by 2 separate small nets (\small{init,c \& init,h}) \\
		$\Rightarrow$ memory state $c_0 = f_\text{init,c}(\bar {\mathbf a})$ \& hidden state $h_0 = f_\text{init,h}(\bar {\mathbf a})$, \\
		where $\bar {\mathbf a}=\frac 1 L \sum \mathbf{a_i}$ (the avg pool of $\mathbf a$)
		\item generate current spatial attention $\alpha_t$ from previous hidden state $h_{t-1}$ \& feature map $\mathbf a$
		$\Rightarrow (\mathbf a_i, h_{t-1}) \xrightarrow{f_\text{att}}$ score $e_{ti} \xrightarrow{\text{softmax}} \alpha_{ti}$
		\item generate current context vector $\hat{\mathbf z}_t =\phi(\mathbf a, \alpha_t)$ \\
		(different $\phi$ for hard-/soft- attention)
		\item generate word prob with current context $\hat{\mathbf z}_t$, hidden state $h_t$ \& previous word $y_{t-1}$ \\
		$\Rightarrow p(y_t \mid \mathbf a, y_1,...,y_{t-1}) \propto \exp(\mathbf L_o\left( E y_{t-1} + \mathbf L_h h_t + \mathbf L_z \hat {\mathbf z}_t \right) )$, \\
		where $\mathbf E$ the word embedding matrix, $\mathbf L_o, \mathbf L_h, \mathbf L_z$ the weights matrices to be learned 
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item CNN encoder: VGG net to the last conv ($14\times14\times512$ feature map)
		\item LSTM decoder takes [previous generated word, context vector] as input \\
		$\Rightarrow$ context vector $\hat{\mathbf z}_t$ involves in both: LSTM input \& projecting LSTM output
		\end{itemize}
	\item Training: Stochastic Hard-Attention
		\begin{itemize}
		\item let one-hot variable $s_t\in\{1,...,L\}$ indicate the focused location at time $t$ \\
		$\Rightarrow \hat{\mathbf z}_t = \sum_{i} s_{ti}\mathbf a_i$ 
		\item view attention $\alpha$ a multinoulli distribution over location ${1,...,L}$ \\
		(multinoulli: discrete distribution over $k$ variables) \\
		$\Rightarrow p(s_{ti} = 1 | \mathbf a, s_1,...,s_{t-1}) = \alpha_{ti}$
 		\item loss: marginal log-likelihood of word sequence conditioned on image $\log p(\mathbf y | \mathbf a)$ \\
 		$\Rightarrow$ maximize its variational lower bound with $s$ introduced as latent var
 		\begin{align*} \displaystyle L_s &= \sum_s  p(s|\mathbf a) \log p(\mathbf y|,s,\mathbf a) \\
 		&\le \log \sum_s p(s|\mathbf a) p(\mathbf y|s,\mathbf a) \\ &= \log p(\mathbf y|\mathbf a) \end{align*}
		\item $\displaystyle \frac {\partial L_s} {\partial W} = \sum_s p(s|\mathbf a) \left[ \frac {\partial \log p(\mathbf y|s, \mathbf a)}{\partial W} + \log p(\mathbf y | s,\mathbf a) \frac {\partial \log p(s|\mathbf a)}{\partial W} \right]$
		\item $\Rightarrow$ approximated by Monte Carlo: \\
		$\displaystyle \frac{\partial L_s}{\partial W} \approx \frac 1 N \sum_{n=1}^N \left[ \frac {\partial \log p(\mathbf y|\tilde s^n, \mathbf a)}{\partial W} + \log p(\mathbf y | \tilde s^n,\mathbf a) \frac {\partial \log p(\tilde s^n|\mathbf a)}{\partial W} \right]$, \\
		where $\tilde s^n = (s_1^n,...,s_t^n,...)$ a sequence of attention locations, \\
		which is sampled from multinoulli distribution described by $\alpha$
		\item to reduce the variance in Monte-Carlo estimation:
			\begin{itemize}
			\item moving average baseline for $k$-th batch: $b_k = 0.9b_{k-1} + 0.1\log p(\mathbf y|\tilde s_k, \mathbf a)$
			\item include entropy $H[s]$ of the multinoulli distribution into the loss
			\item with $0.5$ prob to set $\tilde s$ to its expected value $\alpha$ (turn into soft-attention)
			\end{itemize}
		\item $\Rightarrow$ final gradient on loss \\
		$\displaystyle \frac{\partial L_s}{\partial W} \approx \frac 1 N \sum_{n=1}^N \left[ \frac {\partial \log p(\mathbf y|\tilde s^n, \mathbf a)}{\partial W} + \lambda_r \left( \log p(\mathbf y | \tilde s^n,\mathbf a) - b \right) \frac {\partial \log p(\tilde s^n|\mathbf a)}{\partial W} + \lambda_e \frac{\partial H[\tilde s^n]}{\partial W} \right]$
		\end{itemize}
	\item Training: Soft Attention
		\begin{itemize}
		\item use the expectation $\displaystyle \mathbb E_{p(s_t|\mathbf a)}[\mathbf z_t] = \sum_{i=1}^L \alpha_{ti} \mathbf a_i$ \\
		(equivalent to using $\alpha$ weighted context) \\
		$\Rightarrow$ approximately optimize marginal likelihood $p(\mathbf y|\mathbf a)$ under random variable $s_t$
		\item $\mathbb E[\hat{\mathbf z}_t]$ approximates the normalized weighted geometric mean of the softmax of $k$-th generated word, \\ 
		which further approximates $\mathbb E[p(y_t=k|\mathbf a)]$ over all possible attention locations (induced by $s_t$)
		\item additional gate for $\phi(\mathbf a, \alpha_t) = \beta_t \sum_{i=1}^L \alpha_i \mathbf a_i$, where gate $\beta_t = \sigma(f_\beta(h_{t-1}))$ \\
		$\Rightarrow $ enable model to decide when to emphasize on language model or image context
		\item temporal constrain: encourage $\sum_{t} \alpha_{ti} = \tau$ \\
		(normalization for each spatial location across time)
		\item $\Rightarrow$ end-to-end maximize negative log-likelihood: \\
		$\displaystyle L_d = -\log (p(\mathbf y|\mathbf a)) + \lambda \sum_{i}^L(\tau-\sum_{t}^T \alpha_{ti})^2$, where $\tau=1$ for simplicity
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item spatial attention to capture low-level detail for more accurate description \\
		$\Rightarrow$ dynamically select useful features \& avoid info loss \\
		(using last-conv feature map vs. using last-dense layer)
		\item provide more interpretable result \\
		$\Rightarrow$ able to interpret relationships over image-language
		\item enable model to attend to non-salient features by soft-attention \\
		(e.g. large area of background)
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Referring Segmentation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input for segmentation
		\end{itemize}
	\item Natural Language Expression
		\begin{itemize}
		\item expression to denote the interested object(s)/stuff(s)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Segmentation Mask of Referred Object(s)
		\begin{itemize}
		\item currently (till early 2019), mostly binary segmentation
		\end{itemize}
	\end{itemize}
\item Related Area
	\begin{itemize}
	\item NLP + CV
		\begin{itemize}
		\item referring localization
		\item image caption
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Segmentation from Natural Language Expressions
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item FCN-32s to encode the image into $2$-D feature maps (the last conv layer)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM to encode the sentence into $1$-D vector (the last hidden state)
		\end{itemize}
	\item Combining Info and Output
		\begin{itemize}
		\item per-pixel info: concat [coordinates of current pixel (coord info), language info]
		\item tile the per-pixel info into a feature map, then concat to the spatial info \\
		(per-pixel info concatenated at every pixel of spatial info)
		\item followed by a series of conv and finally a deconv for upsampling
		\end{itemize}
	\item Training
		\begin{itemize}
		\item per-pixel cross-entropy loss
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item special spatial info: coord of each pixel
		\item standard info combination: concatenation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no powerful spatial info encoder: FCN-32s instead of Resnet/Unet...
		\item weak upsampler, compared to encoder-decoder architecture
		\item language info comes late: after downsampling
		\item weak language info: only integrated once
		\end{itemize}
	\end{itemize}

\item Recurrent Multimodal Interaction for Referring Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder (Resnet as backbone, with atrous conv)
		\item then tiled (concat at every pixel) by coord info (coordinate of current pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item word embedding $w_t$ for $t=1,...,T$
		\item LSTM scanning the sentence, with hidden state $h_t$ at time $t$
		\item language info $l_t=$ concat [$h_t$, $w_t$]
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item $l_t$ tiled to spatial info, at each time step \\
		$\Rightarrow$ creating combined feature maps $F_t$ (of shape $[\text{height}, \text{wide}, \text{channel}])$
		\item combined feature maps $F_1,...,F_T$ fed to an convolutional LSTM, \\
		where the ConvLSTM shares weight over both space and time \\
		$\Rightarrow$ feature vector of $F_t[i,j]$ is the input of the ConvLSTM at time $t$ \\
		$\Rightarrow$ conv in ConvLSTM implemented as $1\times1$ conv
		\item a series of conv following the last hidden state of the ConvLSTM
		\end{itemize}
	\item Output
		\begin{itemize}
		\item bilinear interpolated to original input size
		\item optionally post-processed by dense CRF, using pydensecrf (hence inference only)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item more powerful spatial info extractor: DeepLab-101
		\item better language info: integrated at every time step, maintained by an ConvLSTM
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item weak architecture for spatial info: still no upsampling (blur segmentation)
		\item no spatial relation considered in ConvLSTM (?)
		\item weak language representation \\ 
		(better with pos tag, word2vec, word dict, biLSTM, and maybe even attention)
		\item language info still comes late: still after downsampling
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Current State-of-The-Art (early 2019)}
\begin{itemize}
\item Key-Word-Aware Network for Referring Expression Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder for comparability
		\item then tiled by coord info (coordinate of each pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, each hidden state as word info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item attention mask from combined info (spatial info with language info tiled) \\
		(at each time step)
		\item attention weighting over spatial info at each time step \\
		$\Rightarrow$ an $1$-D global encoding for each time step (via weighted mean over space) \\
		$\Rightarrow$ filling feature maps: global encoding if attention here $>$ threshold; else $\mathbf 0$ \\
		$\Rightarrow$ summing filled feature maps over time for the global spatial maps $c$
		\item attention weighting over tiled language info at each time step, correspondingly \\
		$\Rightarrow$ tiled language info maps summed over time for the global language maps $q$
		\item concat [spatial info, $c$, $q$], followed by $1\times1$ conv
		\end{itemize}
	\item Output
		\begin{itemize}
		\item upsampling performed
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item attention introduced: from combined info
		\item better combination: attention masked interact with both spatial \& language info
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item blur segmentation: no encoder-decoder architecture
		\item attention mask obtained sequentially: only last mask has complete language info
		\item language info comes late: after downsampling
		\end{itemize}
	\end{itemize}
\item Referring Image Segmentation via Recurrent Refinement Networks
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLabl ResNet-101 as encoder
		\item last feature maps tiled (concat at each pixel) with coord info
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, generating word info at each time step
		\item last hidden layer as language info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item combined info $=$ spatial info tiled with language info
		\item selecting set of feature maps from downsampling stages
		\item all selected feature maps resized and fed to $1\times1$ conv \\ 
		$\Rightarrow$ to match the dimensions of combined info
		\item convolutional LSTM applied to refine the combined info \\ 
		(with matched selected feature maps as input at each time step)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a conv after final hidden state of ConvLSTM for segmentation
		\item upsampled to original image size
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item ConvLSTM integrating info at dowsampling stage $\Rightarrow$ segmentation refined
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no upsampling: blur segmentation, mitigated by ConvLSTM though \\
		(yet no language info introduced in refinement)
		\item CNN fixed during training: relying on ConvLSTM
		\item single info combination: only by tiling \\
		(though, currently performing the best in all dataset)
		\end{itemize}
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adversarial Attack and Defend}
\subsection{Attacking CNN}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Attacking Image Classification
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item content image, recognized as class $c$ by human
		\item label class $l, l\neq c$
		\item network trained on specific tasks, with params fixed (to be attacked)
		\end{itemize}
	\item Output Attack
		\begin{itemize}
		\item image that should be recognized as $c$ but recognized as $l$ by network
		\end{itemize}
	\end{itemize}
\item Overview and Development
	\begin{itemize}
	\item Type of Valid Attacks \\
	\includegraphics[width=0.3\linewidth, left]{"./Deep Learning/plot/topic-vision attack whole image space".png}
		\begin{itemize}
		\item unconstrained: as long as network is fooled
		\item constrained: image need to be realistic, etc.
		\end{itemize}
	\item Limits in Generating Attacks
		\begin{itemize}
		\item white box: full access to the network \\
		$\Rightarrow$ optimize input by backprop
		\item black box: no access to the network, but can query the net \\
		$\Rightarrow$ numerical gradients by nudging input
		\item one-time attack: can only make one query \\
		$\Rightarrow$ find transferable attack
		\end{itemize}
	\item Finding Cause of Existence of Attacks
		\begin{itemize}
		\item network not understanding data (underfitting)
		\item preserve piecewise linearity in input - output mapping
		\end{itemize}
	\item Defends
		\begin{itemize}
		\item another guard network to predict if input is qualified
		\item adversarial training: involve adversarial example in training
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Attack Generation}
\begin{itemize}
\item Fast Gradient Sign method
	\begin{itemize}
	\item maximize $J(\widetilde x, W) = J(x, W) + (\widetilde x-x)^T\nabla_x J(x)$ s.t. $\norm{\widetilde x - x}_2 < \epsilon$, \\
	$\Rightarrow$ maxnorm constraint to avoid large change in any pixel \\
	(i.e. ensure not changing the label of image)
	\item $\Rightarrow \widetilde x = x+\epsilon \sign(\nabla J(x))$
	\end{itemize}
\end{itemize}

\subsubsection{Transfering Attack}
\begin{itemize}
\item Searchin g the Input Space
\item Attack on Model Ensembling
\item Attacking Physical Model
\end{itemize}

\subsubsection{Hijacking RL System}
\begin{itemize}
\item Goal: modify the input to combine the available action into an action serires of another defined target 
\end{itemize}

\subsection{Cause of Attacks}
\begin{itemize}
\item NOT Overfitting
	\begin{itemize}
	\item Experiment
		\begin{itemize}
		\item test differently trained network \& find attacks \\
		$\Rightarrow$ similar mistakes on the same attacks \\
		\item can even use the offset vector (attacks - original input) to attacks other networks \\
		$\Rightarrow$ systematic effect, not the overfitting (random effect)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item can be underfitting i.e. not capturing true underlying structure, due to being too linear \\
		as linear model force high confidence on example away from decision plain
		\item network is non-linear in weight $\rightarrow$ output relation \\
		(why it is hard to train) \\
		yet, has piece-wise linear in input $\rightarrow$ output
		\end{itemize}
	\end{itemize}
\item Linearity Cause Vulnerability
	\begin{itemize}
	\item Formulation
		\begin{itemize}
		\item given input $\mathbf x=[x_1,...]^T$, a linear layer has $y=\mathbf w^T \mathbf x +b$, where $\mathbf w$ a col vec
		\item $\Rightarrow$ find attack $x^\star \simeq x$ that can have $\hat y$ deviate from $y$ largely
		\end{itemize}
	\item Attack Generation
		\begin{itemize}
		\item let $\mathbf x^\star=\mathbf x+\epsilon\mathbf w^T$, then $y^\star=\mathbf \mathbf \mathbf w^T \mathbf x^\star + b = y+ \epsilon\mathbf w^T\mathbf w$
		\item as $\epsilon$ controls magnitude of $\mathbf w$, while $y$ influenced by $\mathbf w^T \mathbf w$\\
		$\Rightarrow$ can find valid attack with $\epsilon\rightarrow 0$
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item more dimension leads to more vulnerable network (as $\mathbf w^T\mathbf w$ grows with dimension)
		\item trying to linearize network for fast training (e.g. relu, ...) $\Rightarrow$ more vulnerable
		\end{itemize}
	\end{itemize}
\item Inter-class \& Intra-class Distance
\item Not Modeling Posterior Distribution
\end{itemize}

\subsubsection{Defending Attacks}
\begin{itemize}
\item Non-Linear Model
	\begin{itemize}
	\item RBF Model
		\begin{itemize}
		\item yet, hard to train a deep model due to gradient vanishing, etc.
		\end{itemize}
	\end{itemize}
\item Adversarial Training
\item Virtual Adversarial Training
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generative Networks}
\subsection{Model Formulation}
\subsubsection{Explicit Density Estimation}
\begin{itemize}
\item Direct Generalization
	\begin{itemize}
	\item Derivation
		\begin{itemize}
		\item $p(X) = \prod_{n=1}^N p(x_n|x_1,...,x_{n-1})$, where $X$ the input, $x$ the pixel
		\end{itemize}
	\item Development
		\begin{itemize}
		\item pixelRNN, pixelCNN: pred distribution for each output loc \\
		e.g. pixel value $0\sim2555$ if generating img
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item inefficient in inference (sequential generation)
		\end{itemize}
	\end{itemize}

\item Variational Autoencoder
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item assume data (img) $x$ generated from $p(x|z)$, with latent $z$ also sampled from a distrbution
		\item encoder $q(z|x)$ approximates posterior $p(z|x)$ (Gaussian)
		\item decoder $p(x|z)$ estimate the likelihood (Gaussian)
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\Item \begin{align*}\log p(x) &= \mathbb E_{z\sim q(z|x)} [\log p(x) ] \text{, as } z \text{ sampled from } q(z|x) \\
		&= \mathbb E_z \left[ \log \frac{p(x|z) p(z) \cdot q(z|x)}{p(z|x) \cdot q(z|x)} \right], \text{ bayes'} \\
		&= \mathbb E\left[ \log p(x|z) \right] + \mathbb E\left[ \log\frac{q(z|x)}{p(z)} \right] + \mathbb E\left[\log\frac{q(z|x)}{p(z|x)} \right] \\
		&= \underbrace {\mathbb E[ \underbrace {\log p(x|z)}_\text{decoder} ] - D_{KL}(\underbrace {q(z|x)}_\text{Gaussian from encoder} || \underbrace {p(z)}_\text{Guassian prior})}_\text{tractable - loss L} + \underbrace {D_{KL} (q(z|x) || p(z|x))}_{\text{intractable yet }>0} \end{align*}
		\item $\Rightarrow$ loss $L$ as a lower bound estimation (as $D_{KL}>0$)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item introduce latent variable
		\item principled under Bayes' framework
		\item learning to estimate \& sample from parameterized distribution
		\end{itemize}
	\item Development
		\begin{itemize}
		\item incorporate structural info in latent $z$
		\item richer approximation ability (e.g. not diagonal var in Gaussian)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Generative Adversarial Networks}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Encoding 
		\begin{itemize}
		\item random vector sampled from a distribution
		\end{itemize}
	\end{itemize}
\item Output
	\begin{itemize}
	\item Data
		\begin{itemize}
		\item desired data format, e.g. image, video, audio, etc.
		\end{itemize}
	\end{itemize}
\item Training Scheme
	\begin{itemize}
	\item Unsupervised Training
		\begin{itemize}
		\item generator network output/transform the given encoding to desired data
		\item another discriminator network as the loss on generated data \\
		$\Rightarrow$ classify if a given data is generated or real \\
		(checking if the quality matches the real-word data)
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item $L_D [D(G(z))]$ a binary classification loss for discriminator
		\item $L_G = -L_D$, trying to maximize the discriminator loss
		\item minimax objective $\displaystyle \min_{\theta_g}\max_{\theta_d}\left[ \mathbb E_{x\sim p_\text{real data}} \log D(x) + \mathbb E_{z\sim p(z)} \log (1-D(G(z)))\right]$, \\
		where $\theta_{d} / \theta_g$ the weigh for discriminator $D$ / generator $G$, $z$ encoding 
		\end{itemize}
	\item Altering Training
		\begin{itemize}
		\item training $D$ for k times and $G$ for 1 time \\
		$\Rightarrow$ D controls the update direction and the upper bound of the G \\
		$\Rightarrow$ a more stable guiding for generator
		\end{itemize}
	\end{itemize}
\item Understanding
	\begin{itemize}
	\item Sample Approach
		\begin{itemize}
		\item mapping a simple distribution to desired one \\
		(instead of explicit density estimation of output space) \\
		$\Rightarrow$ focus on sampling
		\end{itemize}
	\end{itemize}
\item Overview and Development
	\begin{itemize}
	\item Motivation
		\begin{itemize}
		\item construct computer's understanding of the world \\
		$\Rightarrow$ match real-world distribution from a init distr \\
		(i.e. density estimation)
		\end{itemize}
	\item Better Loss
		\begin{itemize}
		\item non-saturating loss: initial generation is far from reality \\
		$\Rightarrow$ use $-\log(1-x)$ instead of $\log(x)$ \\
		i.e. larger gradient when $D(G(z))\rightarrow 0$ (when generated data is bad)
		\end{itemize}
	\item Better Training
		\begin{itemize}
		\item 
		\end{itemize}
	\item More Constraint
		\begin{itemize}
		\item cycle GAN: two generator able to recover generated data back to original input \\
		(enforce 1-to-1 mapping between encoding and generated data)
		\item conditional GAN
		\end{itemize}
	\item Encoding Interpretation
		\begin{itemize}
		\item linear transformation in encoding space results in non-linear trans in data space \\
		(similar to word-embedding, but data can be image, etc.)
		\item altering one loc in encoding $z$ \& observe its effect on generated data
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Hard Training
		\begin{itemize}
		\item may compete to be worse, instead of better
		\item slow \& compute hungry training procedure
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Image Generation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item 
\end{itemize}

\subsection{Video Generation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image
		\begin{itemize}
		\item as the first frame of the video
		\end{itemize}
	\item Caption
		\begin{itemize}
		\item specifying the video content
		\end{itemize}
	\end{itemize}
\item Output
	\begin{itemize}
	\item a video sequence
	\end{itemize}
\item Metrics
	\begin{itemize}
	\item Similarity / Distance
		\begin{itemize}
		\item between generated \& original video \\
		(original video usually uniformly sampled \& with its image downsampled)
		\end{itemize}
	\item Use Study
		\begin{itemize}
		\item unique human answering question \\
		e.g. which video more realistic / more suitable for the given caption? etc.
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Consistency
		\begin{itemize}
		\item consistent scenes between frames
		\end{itemize}
	\item Realistic Motion
		\begin{itemize}
		\item especially human/animal motion due to their high complexity
		\end{itemize}
	\item Conditioning Video Generation
		\begin{itemize}
		\item control the content, style, etc. \\
		(given initial frame: overlap with video prediction realm) \\
		(while caption provides more control)
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Generation from Caption}
\begin{itemize}
\item CFT-GAN: Conditional Video Generation Using Action-Appearance Captions
	\begin{itemize}
	\item Inference (Generation)
		\begin{itemize}
		\item encode caption with randomness: encode the description of subject, action \& background, etc.
		\item generate optical flow to represent action given $\psi, z_\text{flow}\sim p_z$
		\end{itemize}
	\item Caption Encoding
		\begin{itemize}
		\item caption feature: $\psi$ by Fisher Vectors with HGLMM
		\item conditioning augmentation: $\mathbf c = \psi*(1+\epsilon)$, where $*$ element-wise product \\
		$\Rightarrow$ avoid coarse distribution of $\psi$ by adding noise $\epsilon\sim\mathcal N(0,1)$ \\
		$\Rightarrow \mathbf c \sim \mathcal N(\psi, 1)$
		\end{itemize}
	\item Latent Variables
		\begin{itemize}
		\item random variable (vector) $z\sim\mathcal N(0,1)$
		\end{itemize}
	\item Motion Generator FlowGAN
		\begin{itemize}
		\item input: concat [sampled $\mathbf c=\mathbf c_\text{flow}$, sampled $z=z_\text{flow}$]
		\item based on VGAN: generate optical flow $\mathbf f = m(z)*f(z)$, ($*$ element-wise product)\\ 
		where $m(z)$ a mask to fuse foreground $f(z)$ \& background $b(z)=0$ \\
		($b(z)=0$ due to static camera, as background NOT generating optic flow)
		\item for each time step, upsampled to $64\times 64$ feature map \& mask \\
		$\Rightarrow$ 3D conv for volume of $64\times64\times t$
		\end{itemize}
	\item Motion Discriminator
		\begin{itemize}
		\item concat an input flow \& a corresponding real flow, downsample
		\item dense layer to compress original caption feature $\psi$ as $\psi'$
		\item tile $\psi'$ into downsampled feature map \& discriminate if input flow is real or fake \\
		(further discriminate if flow related to caption)
		\end{itemize}
	\item Appearance Generator TextureGAN
		\begin{itemize}
		\item input: optic flow $\mathbf f$ \& concat [sampled $\mathbf c=\mathbf c_\text{tex}$, sampled $z=z_\text{tex}$]
		\item condition variables map $\mathbf f_c$: upsample [$\mathbf c, z$] by 2 upsampling blocks
		\item $\mathbf f$ as U-net input \& $\mathbf f_c$ concat in downsampling stage \\
		$\Rightarrow$ retain spatial info: reflect edges in $\mathbf f$ as shape of moving target \\
		$\Rightarrow$ output both foreground $f$ \& mask $m$ of shape $64\times64\times t$
		\item background $b$: upsampled from [$\mathbf c, z$] for a single frame \& replicated $t$ times
		\item fusion: video $v = f*m + (1-m)*b$
		\end{itemize}
	\item Video Discriminator
		\begin{itemize}
		\item concat input video \& a corresponding real video, downsample
		\item downsample the optic flow for input video $\mathbf f$ as $\mathbf f'$ \& further downsample
		\item dense layer compress caption $\psi$ as $\psi'$ \& concat to downsampled feature map \\
		(also discriminate if video related to caption)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item textureGAN trained initially using real optic flow calculated from real video \\
		(until flowGAN more stable \& trained to a extend, to avoid wasted training)
		\item use existing video dataset \& adding action-appearance caption
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item demonstrate ability for finer control on video content \\ 
		(not only action, but background, appearance) \\ 
		$\Rightarrow$ two-stage (action + appearance) generation for each frame for realistic complex scene\\
		$\Rightarrow$ caption for more descriptive control (v.s. initial frame)
		\item more abstract \& general control \\ 
		$\Rightarrow$ reflect more variety of complex actions and appearances by captions \\ 
		(compared to use image as initialization)
		\end{itemize}
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Special Learning}
\subsection{Transfer Learning} \label{DL_Learning_Transfer}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Source Data
		\begin{itemize}
		\item a large amount of labeled data
		\item having different distribution then the desired target data
		\end{itemize}
	\item Target Data
		\begin{itemize}
		\item a small amount of labeled data, with a large amount of unlabeled data \\
		(due to hardness of labeling, etc.)
		\item from the distribution where model need to handle
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Model Performance
		\begin{itemize}
		\item good performance on val\&test set (containing target data)
		\item good generalization ability on the target distribution
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Standard Baseline}
\begin{itemize}
\item Pre-training \& Fine-tunning
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item distribution of source \& target data share some common features \\
		$\Rightarrow$ different task shares some common knowledge
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item train network on source data only
		\item swap/modify the last few layers (including prediction layer)
		\item retrain the last layer (limited target data) / all net (enough target data)
		\end{itemize}
	\item Guideline
		\begin{itemize}
		\item small dataset: freeze pretrained network \& use it as fixed feature extractor \\ 
		$\Rightarrow$ only train the last prediction layer
		\item medium dataset: freeze fewer layers, design some own last layers
		\item exceptionally large dataset with large computation budget: train from scratch \\
		$\Rightarrow$ pretrained weights as initialization (nor preferred in most cases)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item sharing weights/structure: low level feature extraction useful for both \\
		$\Rightarrow$ based on model ability
		\end{itemize}
	\end{itemize}
\item Transfer Ada Boost (trAdaBoost)
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item distribution of target \& source overlap more or less \\
		$\Rightarrow$ able to extract helpful guides from source data
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item setup train set with mixed target \& source data
		\item weighting example from target \& source differently: \\ 
		for source data weight $= \frac 1 {N_\text{source}}$; target data weight $= \frac 1 {N_\text{target}}$ \\
		$\Rightarrow$ target data more important (as smaller in number)
		\item for each weight-update iteration (may contain multiple epochs), update the weight: \\
		$\Rightarrow$ shift the weight (importance) towards target data \& normalize all the weight
		\end{itemize}
		$\Rightarrow$ based on data distribution
	\item Understanding
		\begin{itemize}
		\item learn the shared feature/knowledge with the help of source data
		\item focus more on target as making progress
		\end{itemize}
	\end{itemize}
\item Feature Projection
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item few or NO overlap between source \& target (as data examples directly)
		\item source \& target can be mapped onto a shared feature space, where overlap can be discovered
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item project/map the source \& target data onto the same feature space
		\item transfer learning in the shared space
		\end{itemize}
		$\Rightarrow$ based on distribution transformation
	\item Understanding
		\begin{itemize}
		\item try discover common feature through transformation \\ 
		(may need a decoder to map back to desired output space)
		\end{itemize}
	\item Example
		\begin{itemize}
		\item \hyperref[DL_NLP_Langrep]{word embedding}: learn word relation as unsupervised learning \\
		$\Rightarrow$ a shared feature space to represent word
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Multi-task Learning}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Multi-labeled Data
		\begin{itemize}
		\item one input data corresponds to multiple desired outputs
		\item $\Rightarrow$ require similarity/common knowledge in different tasks \\
		(e.g. object detection for multiple object types)
		\end{itemize}
	\item Partial-labeled Data
		\begin{itemize}
		\item desired outputs may not be all labeled in the input \\ 
		(i.e. some may be missed)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item General Solution to Multi-task
		\begin{itemize}
		\item give all desired outputs from a single network
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Standard Baseline}
\begin{itemize}
\item Single Networks with Multiple Predictions
	\begin{itemize}
	\item Sharing
		\begin{itemize}
		\item shared low-level layers to extract features from the input
		\item shared loss as a sum over all prediction for corresponding label
		\item shared training as back-prop computed as a single network
		\item shared input data as trained together \\
		$\Rightarrow$ shared knowledge discovered when training on data for other tasks
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item each task help each other, by contributing to the common knowledge
		\item overcome data shortage: augmented by data for other tasks
		\item partial labeled still useful: help train the shared layers
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{K-shot Learning}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Training Set
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\section{Interpretable Machine Learning}

\subsection{Visualization}
\subsubsection{Input-Output Correspondence}
\begin{itemize}
\item Occlusion Sensitivity
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item slide an occlusion window across the input \& collect network output \\
		$\Rightarrow$ different output prob for different occlusion location
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item show which component influences network the most
		$\Rightarrow$ can also show where the background noise lies
		\end{itemize}
	\end{itemize}
\item Class Activation Map
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item add average pooling + softmax layer after the last conv \& fine-tune \\
		$\Rightarrow$ to retain correspondence between pred and channels in feature map \\
		(remove dense layer on flatten conv feature, which hides the correspondence)
		\item given an input \& skip the average pooling \\
		$\Rightarrow$ weighted sum over channels of feature map $\Rightarrow$ a weighted heatmap for each class pred \\ 
		(though usually caring only the label class of the input)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item avg pooling + dense layer to fine-tune the weight for weighted sum on feature map channels \\
		$\Rightarrow$ visualize the activation map to analyze channel sensitivity to specific group of input
		\item show that CNN has localization ability, even if only trained on classification
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Visualizing Network Inside}
\begin{itemize}
\item Weight Visualization
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item directly visualize the weights as is data \\
		e.g. CNN kernel weights as tiny image
		\item reveals the pattern that the weight is looking for \\
		(as inner-product activated the most by itself)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item only meaningful for the first layer \\
		e.g. first layer in CNN are pattern matching on the input image \\
		$\Rightarrow$ hard to directly interpret the feature in the middle \\
		(as operating on intermediate feature maps)
		\end{itemize}
	\end{itemize}
\item Feature Map Visualization
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item for each input visualize the activation of a layer (e.g. feature map)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item visualize which (type/group of) input activates the chosen intermediate layer/channel the most \\
		$\Rightarrow$ review its desired feature in input
		\end{itemize}
	\end{itemize}
\item Feature Space Interpretation
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item forward the input image through network \& select a layer to use its feature as encoding
		\item aggregate the features $\Rightarrow$ dataset projected to the feature space
		\item perform clustering (e.g. KNN) / dimension reduction (t-SNE) \\
		$\Rightarrow$ visualize the geometry of dataset under the given layer of feature
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item similar to dataset search, yet can understanding the dataset geometry that the network learn \\
		(instead of the meaning of a single hidden unit)
		\item usually use the last layer (feature with highest semantic info)
		\end{itemize}
	\end{itemize}
\item Maximally Activated Input (Dataset Search)
	\item Procedure
		\begin{itemize}
		\item treat a single hidden unit / feature map channel in network as a score
		\item forward all samples in dataset \& rank samples using its activation as score \\
		$\Rightarrow$ test out which the feature is sensing (by analyzing the high-score samples)
	\item Understanding
		\begin{itemize}
		\item able to reveal which feature in the input highly activate the unit \\
		(need to consider receptive fields in a CNN)
		\end{itemize}
	\item Precedure - Detection Network
		\begin{itemize}
		\item use a fixed set of proposals
		\item rank final output using its activation (pred boxes after NMS) \\
		$\Rightarrow$ analyze the content inside the bbox, instead of the whole image
		\end{itemize}
	\item Understanding on CNN
		\begin{itemize}
		\item each feature sensing a different object (done on R-CNN) \\
		e.g. face, red blob, square objects, light reflectance, array of dots, text (like OCR), etc. \\
		\item show CNN can learn: class-specific shape \& abstract attributes \\
		$\Rightarrow$ construct a \underline{distributed representation} of object \\ 
		(e.g. shape, color, texture, text, material properties, etc.)
		\item $\Rightarrow$ later dense layer in CNN can model a large range of object \\
		(as combining different distributed object attributes)
		\end{itemize}
	\end{itemize}
\item Saliency Map - Recover Input from Activation
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item take the derivative of label (or a chose hidden unit) w.r.t the input \\
		(similar to generating adversarial attacks) \\
		i.e. $\frac{\partial s_i}{\partial \mathbf x}$, where \\
		$s_i$ the $i^\text{th}$ loc in score $\mathbf s$ (before softmax layer); $\mathbf x$ the input
		\item resulting gradient is the same size of $\mathbf x$ i.e. the saliency map \\
		$\Rightarrow$ reconstruct the input
		\end{itemize}
	\item Procedure - Guided Gradient in CNN
		\begin{itemize}
		\item reconstruct input by taking inverse operations \\
		e.g. unpooling, deconv, un-ReLU (which is also ReLu), etc.
		$\Rightarrow$ construct an inversed network
		\item assume weights in conv kernel $\mathbf w$ is orthogonal matrix, to have $\mathbf w^T = \mathbf w^{-1}$ \\
		$\Rightarrow$ use transposed conv with re-arranged kernel as deconv \\
		(as NOT wanting to train new weights for transposed conv)
		\item with a chosen activation, set all other loc on feature map to be 0
		\item run the corresponding inversed net to reconstruct the input img
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item the most influential part receives the largest gradient \\
		$\Rightarrow$ reveals the image pattern corresponding to specific activation
		\item softmax introduce cross-label interaction \\ 
		$\Rightarrow$ pre-softmax score to ensure saliency is related to desired output loc \\
		(network can minimize score at other location to increase the softmax output at desired loc)
		\end{itemize}
	\end{itemize}
\item Gradianet Accent
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item fix network params \& init a random input (or choose an input) \\
		$\Rightarrow$ optimize \& update the input for a chosen output (e.g. score for class $k$, or a hidden unit)
		\item e.g. $L=s_\text{dog}(x) + \lambda \norm{x}$, where regularization is for better visualization \\
		$\Rightarrow$ to have a smoother image with less abrupt change (can use Gaussian blur as well)
		\end{itemize}
	\item Understanding
		\begin{itemize} 
		\item again, similar to adversarial attack, but with benign objective \\
		$\Rightarrow$ generate the input that network thinks to be a chosen output 
		\end{itemize}
	\end{itemize}
\item Deep Dream - Gradient Accent with Strong Prior
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item fix network params \& choose an input and a specific activation
		\item set the activation score as backward prop for that activation \\
		$\Rightarrow$ enhance the network pattern of that activation \& update the input
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item similar to gradient accent, but create a enhancing feedback loop \\
		$\Rightarrow$ pose the pattern of that activation on the input
		\item become a tool to create art...
		\end{itemize}
	\end{itemize}
\item Feature Inversion
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item given a feature of input (e.f. feature map), reconstruct the input image \\
		$\Rightarrow$ learn a decoder to decode the selected feature 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item reconstructed image shows the info retained in the feature \\
			\begin{itemize}
			\item almost perfect reconstruction from low level CNN feature
			\item detail blurred reconstruction from high level CNN feature
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Ablation Study}
\begin{itemize}
\item Layer Ablation on Conv Net
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item after trained, reduce the num of dense layer, until fully conv \\
		i.e. using previous layer output as prediction at test time
		\item on test set: evaluate performance drop \\
		on other dataset: evaluate generalization ability drop
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item CNN ability relies on convolution layer, instead of dense layer \\
		$\Rightarrow$ \underline{conv layer as feature map extractor} for other domain-model
		\item fine-tuning improves a lot $\Rightarrow$ fine-tunning the dense layer \\
		(while conv layer are learned to be quite generic)
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Acceleration}
\subsection{Model Reduction}
\begin{itemize}
\item Pruning
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item remove connection with too small weights (set weight to 0)
		\item retrain the net
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 90\% weights can be redundant
		\end{itemize}
	\end{itemize}
\item Weight Sharing
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item cluster the weight in a layer \& use the centroid for each cluster \\
		$\Rightarrow$ each layer store a index to the weight table, instead of weight \\
		(can even use Hoffman tree)
		\item aggregate the gradient \& update on the value in the table
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item can have 4/2-bit index to retain most performance in conv / fully-connected net
		\end{itemize}
	\end{itemize}
\item Quantization
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item gather the statistic of each layer: range of activation, weights, etc. \\
		$\Rightarrow$ new \& more compact bit representation for the given range \\
		(static quantization)
		\item inference with fixed point number \& backprop with normal floating number \\
		(quantization training)
		\end{itemize}
	\end{itemize}
\item Low-Rank Compression
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item depthewise conv + 1x1 conv to approximate normal conv
		\item break dense layer into a tree of dense layer, using e.g. SVD
		\end{itemize}
	\end{itemize}
\item Binary Network
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item three weights for inference, a extreme quantization
		\end{itemize}
	\end{itemize}
\item Mixed Precision Training
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item inference \& gradient flow calc on FP16 \\
		yet, weights update performed with FP32
		\end{itemize}
	\end{itemize}
\item Model Distillation
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item use large \& teacher model to create soft label
		\item train a simplified student model
		\end{itemize}
	\end{itemize}
\item Fused Ops
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item fuse the matrix $\times$ \& $+$ ops into one single clock cycle
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Tactile Sensing}
\subsection{Tactile Sensor}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Single-point Contact Sensors (single tactile cells)
		\begin{itemize}
		\item able to confirm object-sensor contact
		\item force sensor to detect force at contact point
		\item biomimetic whiskers (dynamic tactile sensors) to detect vibration at contact point
		\end{itemize}
	\item High Resolution Tactile Arrays (fingertips)
		\begin{itemize}
		\item sensors array of tactile sensing elements (usually planar array)
		\item sensing elements: fiber optics, embedded camera, barometers, etc.
		\end{itemize}
	\item Large-Area Tactile Sensors (body skins)
		\begin{itemize}
		\item usually able to be curved for large robotic skin
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsection{Tactile Sensing and Perception}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Tactile Data
		\begin{itemize}
		\item the data after on-board processing on the raw tactile sensor
		\item come with different form/meaning, depending on the sensor
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Detail Object Recognition
		\begin{itemize}
		\item recognize the object shape, material properties, in-hand pose and etc.
		\item action related: grasp control, slippage detection, etc.
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Feature Extraction}
\begin{itemize}
\item Acoustic Signal
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item microphone to collect signal from tapping/sliding/scratching
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item filter/function (e.g. fast Fourier transform) to map to frequency domain
		\item then analyzed/classified into texture properties
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fast, low-cost, light-burden
		\item need interaction
		\end{itemize}
	\end{itemize}
\item Vibration and Force Data
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item robot finger sliding with various speed to detect vibration/friction
		\item strain gauges sensor to detect stiffness, compliance, elasticity and etc.
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item transform into frequency domain, then analyze the vibration intensities
		\item or, directly use kNN, SVM, etc on mechanical impedance data to classify
		\end{itemize}
	\end{itemize}
\item Micro-structure Patterns
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item tactile sensors array for pressed/imprinted surface
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item sensor data with spatial relation $\Rightarrow$ a small image (usually $14\times6$) \\
		$\Rightarrow$ image processing (with hand-crafted/learned features)
		\item learning features: end-to-end learned for various task
		\end{itemize}
	\end{itemize}
\item Spatial-Temporal Data
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item a series acquired local tactile data, each with sensor-object contact location \& time
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item viewed as a spatial points cloud, then recognize the object contour
		\item incorporate with LSTM algorithms: e.g. landmarks, grid based, particle filter, etc.
		\item incorporate sequential model: e.g. KF to sequentially refine object properties
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Application in Robot Tasks}
\begin{itemize}
\item Robot Manipulation
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image from vision sensor, data from tactile sensor
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item successfully perform task involving object manipulation \\
		e.g. open a door via door handle
		\end{itemize}
	\item Single-Point Contact
		\begin{itemize}
		\item structure as spatial-temporal points
		$\Rightarrow$ in-hand pose estimation (usually with LSTM methods)
		\item challenge: sensor-object contact influence the environment \\
		(use lidar?)
		\end{itemize}
	\item Tactile Arrays
		\begin{itemize}
		\item image processing \& SLAM
		\end{itemize}
	\item Tactile \& Vision
		\begin{itemize}
		\item 3D vision (from lidar/camera) $\Rightarrow$ sensor fusion for localization
		\item vision for haptic\&location estimation + tactile for refinement
		\item vision \& tactile both treated as image: potentially sharing weights (multi-tasking)
		\end{itemize}
	\end{itemize}
\item Sensor Fusion for Environment Modeling
	\begin{itemize}
	\item Contact Verification
		\begin{itemize}
		\item verify robot-object contact
		\item active searching ambiguous region in visual perception
		\item jointly estimate likelihood of contact at each location in the environment
		\end{itemize}
	\item Object Properties Refinement
		\begin{itemize}
		\item model the in-hand object as in robot manipulation (can be end-to-end) \\
		e.g. estimate the angle of door handler while gripping
		\end{itemize}
	\end{itemize}
\end{itemize}