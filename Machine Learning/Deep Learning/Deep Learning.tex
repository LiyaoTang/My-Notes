\chapter{Deep Learning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interview of Fame}

\subsection{Geoffrey Hinton}
\subsubsection{Knowledge Embedding}
\begin{itemize}
	\item BP
	\begin{itemize}
		\item psychology view: knowledge in vectors
		\item semantic AI: knowledge graph
		\item BP algorithm can interpret \& convert between feature vector and graph representation (with some embedding)
	\end{itemize}
	\item Boltzmann Machine
	\begin{itemize}
	\item Leaning Algorithm on Density Net
		\begin{itemize}
		\item same information in forward \& backward propagation to learn feature embedding
		\end{itemize}
	\item Restricted Boltzmann Machine (RBM)
		\begin{itemize}
		\item ways of learning in deep dense net with fast inference
		\item iterative learning (adding layer after the above trained)
		\item ReLU $\Leftrightarrow$ a stack of sigmoid functions (approximately) in RBM
		\item ReLU units initialized to identity for efficient learning
		\end{itemize}
	\end{itemize}
	
	\item EM
		\begin{itemize}
		\item EM with Approximate E Step
		\end{itemize}
	
	\item vs. Symbolic AI
		\begin{itemize}
		\item Symbolic AI: symbolic logic-like expression to do reasoning
		\item yet, maybe state vector to represent knowledge
		\end{itemize}
\end{itemize}

\subsubsection{Brain Science}
\begin{itemize}
\item Brain: Nets Implemented by Evolution
	\begin{itemize}
	\item trying to train without BP
	\item doing BP (get derivatives) with re-construction error (auto-encoder)
	\end{itemize}
\end{itemize}

\subsubsection{Memory in Nets}
\begin{itemize}
\item Fast Weights for Short-term Memory
\item Capsule Net
	\begin{itemize}
	\item structured knowledge representation in each unit (feature with sets of property)
	\item $\Rightarrow$ enable nets to vote rather than filtering - thus better generalization
	\item now working: published in \textbf{2017 NIPS}
	\end{itemize}
\end{itemize}

\subsubsection{Unsupervised Learning}
\begin{itemize}
\item Importance
	\begin{itemize}
	\item better than human eventually (as supervised learning has limited maximum)
	\item GAN as a breakthrough
	\end{itemize}
\end{itemize}

\subsubsection{"Slow" Feature}
\begin{itemize}
\item Non-linear Transform to Find Linear Transform
	\begin{itemize}
	\item find a latent representation containing linear transform to do the work
	\item e.g. change viewpoints: pixels $\rightarrow$ coordinates $\rightarrow$ linear transform $\rightarrow$ back to pixels
	\end{itemize}
\end{itemize}

\subsubsection{Relations between Computers}
\begin{itemize}
\item showing computer data to work
	\begin{itemize}
	\item instead of programming it to work
	\end{itemize}
\end{itemize}


\subsection{Pieter Abbeel}
\subsubsection{Deep Reinforcement Learning}
\begin{itemize}
\item Overall Challenge 
	\begin{itemize}
	\item Representation
	\item Exploration Problem
	\item Credit Assignment
	\item Worst Case Performance
	\end{itemize}
\item Advantage (Deep Nets in RL)
	\begin{itemize}
	\item network capturing the representation (state vector)
	\end{itemize}
\item Question in DRL
	\begin{itemize}
	\item how to learn safely
	\item how to keep learning (under small negative samples) e.g. better than human
	\item can we learn the reinforcement learning program (RL in the RL)
	\item long time horizon
	\item use experience across tasks
	\end{itemize}
\item Success of DRL
	\begin{itemize}
	\item simulated robot inventing walking... $\Rightarrow$ single general algorithms to learn
	\end{itemize}
\end{itemize}

\subsection{Ian Goodfellow}
\subsubsection{Generative Adversarial Networks}
\begin{itemize}
\item Generative Models
	\begin{itemize}
	\item Resembling
		\begin{itemize}
		\item trained to optimized the distribution behind training data \\ 
		(then sampled from that distribution to get more imaginary training data)
		\item $\Rightarrow$ produce data to resemble the training data
		\end{itemize}
	\item Usage
		\begin{itemize}
		\item semi-supervised learning
		\item data augmentation
		\item simulating scientific experiment
		\end{itemize}
	\item Previous Ways
		\begin{itemize}
		\item Boltzmann Machine
		\item Sparse Coding
		\end{itemize}
	\item Now: Generative Adversarial Networks (GANs)
	\item Future
		\begin{itemize}
		\item increase reliability of GANs (stabilizing)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Yoshua Bengio}
\subsubsection{Thoughts}
\begin{itemize}
\item Fallacy
	\begin{itemize}
	\item Smoothness in Nonlinearity
		\begin{itemize}
		\item to ensure non-zero gradients every where
		\end{itemize}
	\end{itemize}
\item Surprising Fact
	\begin{itemize}
	\item ReLU in Deep Net
		\begin{itemize}
		\item inspired initially by biological connection
		\end{itemize}
	\end{itemize}
\item \textbf{Distribution v.s. Symbolic Representation}
	\begin{itemize}
	\item Distributed Representation 
		\begin{itemize}
		\item distributed in lots of units, instead of a symbolic representation in a single cell \\
		(agree on Geoffrey Hinton)
		\end{itemize}
	\item Curse of Dimensionality
		\begin{itemize}
		\item neural net's distributed representation for joint distribution over random variables
		\end{itemize}
	\item $\Rightarrow$ Word Embedding
		\begin{itemize}
		\item generalized to joint distribution over sequence of words
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Works}
\begin{itemize}
\item Piecewise Linear Activation (PLU)
\item Unsupervised Learning
	\begin{itemize}
	\item Focus
		\begin{itemize}
		\item Denoising auto-encoder
		\item GANs
		\end{itemize}
	\item Importance
		\begin{itemize}
		\item human ability: self-teaching, building world-model from perception
		\end{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item underlying concept across two fields: machine can learn through interactions \\
		$\Rightarrow$ learning "good" representation (yet, what is "good")
		\end{itemize}
	\item Possible Directions
		\begin{itemize}
		\item loss function: not even defined for each task \\ 
		(not knowing which is good for what?)
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Machine Translation (Founder)
	\item Generalized into Other Fields
	\end{itemize}
\item Back-prop in Brains (Neural Science
	\begin{itemize}
	\item Reasons for Efficiency of Backprop
	\item Larger Family behind Credit Assignment
	\end{itemize})
\end{itemize}

\subsection{Yuanqing Lin}
\subsubsection{National Deep Learning Lab}
\begin{itemize}
\item Paddle Paddle
\item Baidu Lab
\end{itemize}

\subsection{Andrej Karpathy}
\subsubsection{Human Benchmark}
\begin{itemize}
\item Programming by Showing
	\begin{itemize}
	\item Requirement
		\begin{itemize}
		\item input + output as specification
		\item metric as goal
		\end{itemize}
	\item Writer
		\begin{itemize}
		\item the optimizer
		\end{itemize}
	\end{itemize}
\item Understanding Importance of Benchmark
	\begin{itemize}
	\item importance to do better given the current performance on the dataset \\
	(as important increase after passing human error)
	\end{itemize}
\item Understanding Network Behavior
	\begin{itemize}
	\item compared to the process of human decision
	\end{itemize}
\end{itemize}
\subsubsection{Transfer Learning}
\begin{itemize}
\item Image Task
	\begin{itemize}
	\item feature extractor + fine tune/modification onto various task
	\end{itemize}
\end{itemize}

\subsection{Ruslan Salakhutdinov}
\subsubsection{Restricted Boltzmann Machine}
\begin{itemize}
\item Auto Encoder
	\begin{itemize}
	\item Encoding All Kind of Data
		\begin{itemize}
		\item from digit to face, document, etc...
		\item deeper and deeper structure
		\end{itemize}
	\end{itemize}
\item Training Boltzmann Machine
	\begin{itemize}
	\item Pretraining
		\begin{itemize}
		\item increase the low boundary by training the previous layer
		\item then add another layer to train, ...
		\end{itemize}
	\item Direct Training (with GPU)
		\begin{itemize}
		\item similar, or better result
		\end{itemize}
	\end{itemize}
\item Boltzmann Machine Ability
	\begin{itemize}
	\item Generative Model
		\begin{itemize}
		\item model coupling distributions in data \\
		$\Rightarrow$ scalable (more scalable than current model\&operation)
		\item only way to train the model in the early age
		\end{itemize}
	\end{itemize}
\item Progress on Generative Model
	\begin{itemize}
	\item probabilistic max pooling
	\item variational encoder
	\item deep energy model
	\item semi-supervised Model
	\end{itemize}
\end{itemize}

\subsection{Research}
\subsubsection{Topics}
\begin{itemize}
\item Point Cloud
	\begin{itemize}
	\item Operations on Points: how to embed location in operation
		\begin{itemize}
		\item select fixed number of points via coord?: then take weighted average (conv) / max (pooling) on them
		\item need a "select input points" op: like deformable conv?
		\end{itemize}
	\item Bounding Box Directly from Points: no voxel
		\begin{itemize}
		\item clustering + regression on each cluster?
		\end{itemize}
	\end{itemize}
	
\item Unsupervised Learning
	\begin{itemize}
	\item Deep Belief Nets
	\end{itemize}

\item Reinforcement Learning
	\begin{itemize}
	\item Deep Reinforcement Learning
		\begin{itemize}
		\item scalable system
		\item communicative\& cooperating agents
		\end{itemize}
	\end{itemize}

\item One-shot / Transfer Learning
	\begin{itemize}
	\item Learning the Ability to Learn
	\end{itemize}

\item General AI
	\begin{itemize}
	\item Structure for General Task
		\begin{itemize}
		\item neural network or other structure, shared for multiple tasks \\
		(instead of breaking down to different parts like segmentation, detection, etc.) \\
		(instead of the split of cv, nlp, planning, etc.)
		\item $\Rightarrow$ a full agent (instead of decomposed function) \\
		$\Rightarrow$ optimization method/objective need to be carefully defined
		\end{itemize}
	\item Attempt for General AL
		\begin{itemize}
		\item scaling up supervised learning: imitating human
		\item unsupervised learning: AIXI, artificial evolution, etc.
		\end{itemize}
	\end{itemize}

\item AI Security
	\begin{itemize}
	\item Anti Inducing
		\begin{itemize}
		\item NOT to be fooled/induced to do unappropriated things \\
		(even if algorithm is right)
		\end{itemize}
	\item Built-in Security
	\end{itemize}
\item Fairness in AI
	\begin{itemize}
	\item Dealing Societal Issue
	\item Reflecting Preferred Bias
	\end{itemize}
\item Auto Optimization (Hyperparameter Tunning)
	\begin{itemize}
	\item Swarm Optimization
	\item Expectation Maximization
		\begin{itemize}
		\item target variable $\theta=$ hyperparameters
		\item hidden variable $Z=$ weights of network
		\item data $X=$ dataset
		\end{itemize}
	$\Rightarrow$ 
		\begin{itemize}
		\item E-step: evaluate $\displaystyle \mathbb E_{Z|\theta_n,X}(\ln P(Z,X|\theta))$
			\begin{itemize}
			\item $\ln P(Z, X|\theta)$: log likelihood of hyperparam $\theta$ (for weights \& data to be observed)
			\item $P(Z|\theta_n, X)$: posterior of weights $Z$ 
			\end{itemize}
		$\Rightarrow$ evaluate (approximate) the expectation of the log likelihood of hyperprarm $\theta$ \\
		(from a functional view, train with $\theta_0-\theta_N$, evaluate model $M$ times in training, thus with weights $Z_{00}-Z_{NM}$) \\
		$\Rightarrow$ a matrix with $n$ as row entry, $m$ as column entry, mapping to both $\ln P(Z, X|\theta), P(Z|\theta_n, X)$
		$\Rightarrow$ then marginalize (taking the expectation) over $Z$, to get a (sampled) function over $\theta$
		\item M-step: maximize the result function from E-step
			\begin{itemize}
			\item fit a curve \& maximize w.r.t hyperparams $\theta$
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item World Understanding: after perception
	\begin{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item machine learns from interactions
		\item machine builds a representation of world (like human ability, without fine label)
		\end{itemize}
	$\Rightarrow$ building world-model from perception
	\item Causality Mining
	\end{itemize}
\item Model Interpretation
	\begin{itemize}
	\item Logical Formalization
		\begin{itemize}
		\item deep learning can be understood logically \\
			e.g. what make deep net training harder? understand the limit of current algorithm/model and \textbf{why}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Advises}
\begin{itemize}
\item Learning Direction
	\begin{itemize}
	\item Math
		\begin{itemize}
		\item statistic
		\item linear algebra
		\item calculus
		\item optimization
		\end{itemize}
	\end{itemize}
\item Reading
	\begin{itemize}
	\item read a little bit \& find somewhere intuitively not right
	\begin{itemize}
		\item good intuition: eventually work; \\ 
		bad intuition: not working no matter what it is doing
		\item if other doubts your idea as bullshit $\Rightarrow$ a sign for real good result
	\end{itemize}
	\item a supervisor with similar belief
	\item PhD vs. Company
		\begin{itemize}
		\item amount of mentoring
		\item faster if dedicated supervisor available
		\item resource
		\end{itemize}
	\end{itemize}

\item Practice
	\begin{itemize}
	\item open-source learning resource
	\item open source contribution
		\begin{itemize}
		\item contribute to open source framework (e.g. conv on sparse matrix in TF)
		\item implement the paper, the open source it (as a tool for other)
		\item work on a projected and open source it \\
		$\Rightarrow$ the stage (e.g. github) will bring people to you
		\end{itemize}
	\item implement the tools: to find out how \& why it works \\
	$\Rightarrow$ derive theories from the 
	\item full stack of understanding \\ 
	$\Rightarrow$ understand the implementation under the deep learning framework
	\end{itemize}
\end{itemize}

\subsubsection{Direction - Segmentation}
\begin{itemize}
\item  Multi-scale in Segmentation
	\begin{itemize}
	\item Per-pixel Classification
		\begin{itemize}
		\item not only skipping from encoder to decoder, but also skipping from tunnel to decoder final output layer, by e.g. tiling.
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Direction - Detection}
\begin{itemize}
\item Faster One-stage
	\begin{itemize}
	\item Segmentation as Detection
		\begin{itemize}
		\item for output mask, giving classification ($p_e$) + localization (regression) \\
		(instead of giving class probability as per-pixel classification)
		\item at most as many objects as pixel number \\ 
		$\Rightarrow$ not possible to lose detection due to griding
		\item one-step further from ExtremeNet: \\
		directly segmentation, then use boundary pixel to re-organize into bbox
		\end{itemize}
	\end{itemize}
\item Two-stage in One-stage
	\begin{itemize}
	\item Attention
		\begin{itemize}
		\item attention map as 1st box proposal, as extreme net (instead of RoI)
		\end{itemize}
	\end{itemize}
\item \textbf{No Non-Maximum Suppression}
	\begin{itemize}
	\item Auto Filtering
		\begin{itemize}
		\item regress threshold of objectness as well \\
		$\Rightarrow$ objectness under threshold not considered by encoder-decoder \\
		(under the GAN framework ?) \\
		(under multi-tasking ?)
		\end{itemize}
	\item CRF
		\begin{itemize}
		\item still not end2end, yet trainable
		\end{itemize}
	\item RNN Encoder-Decoder
		\begin{itemize}
		\item encode all bbox / spatial feature, then decode (generate) till end-of-sequence \\
		(as YOLOv1 still use dense layer...)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Direction - Tracking}
\begin{itemize}
\item Current State - 2019
	\begin{itemize}
	\item NN in Data Association
		\begin{itemize}
		\item pred association prob of prediction-detection in JPDA
		\item pred prob of detection being true / false alarm (the existence prob for bbox), used in MHT
		\end{itemize}
	\item Tracking Initiation \& Deletion
		\begin{itemize}
		\item currently, by rules... \\
		e.g. 3 observations in 4 consecutive frames; observation missed for 4-7 frames
		\end{itemize}
	\item Prediction Encoding
		\begin{itemize}
		\item crop the image according the predicted area (gating)
		\end{itemize}
	\item Deep Backbone
		\begin{itemize}
		\item deeper wider siamrpn
		\end{itemize}
	\end{itemize}
\item NN in Gating
	\begin{itemize}
	\item Region Proposal Network
		\begin{itemize}
		\item propose the search region (gating) by RPN \\ 
		(instead of using rule based on current track location) \\
		$\Rightarrow$ predict a large region with RPN based on track info
		\end{itemize}
	\end{itemize}
\item End-to-end / unified framework
	\begin{itemize}
	\item End-to-End Regression for Data Association in MOT
		\begin{itemize}
		\item cite: Online Multi-Target Tracking Using Recurrent Neural Networks
		\item directly output association decision
		\item input: raw image (semantic), current detection, track prediction, track history
		\item track-level info: extended Siamese for relation of: track - current detection \\
		(instead of only det-det in $2$ frames)
		\item fully NN approach for JPDA \\
		$\Rightarrow$ track - all det: concat (tile) track+pred to each det $\Rightarrow$ use fcn, then pooling to regress to a bbox for update \\
		(direct regress the box, instead of Siamese net + weighted sum) \\
		(cite: Data-Driven Approximations to NP-Hard Problems)
		\item $\Rightarrow$ fast JPDA: single RNN encode all detections as $D$, then regress for each track-$D$ for update \\
		(instead of num of track $\times$ num of det, now only num of track + num of det)
		\item fully NN approach for MHT ??? \\
		trace-back available ... encoding past detection as well? (in the tracker?) \\ 
		N-scan pruning?
		\end{itemize}
	\item Track Initiation/Deletion
		\begin{itemize}
		\item directly output decision result \\
		$\Rightarrow$ regress the decision boundary as well (instead of universal $0.5$)
		\end{itemize}
	\item \underline{Interesting Predicted Bounding Box Encoding}: attention instead of cropping
		\begin{itemize}
		\item direct encoding: x,y,w,h, o, + encoding of full image
		\item mask: bounding box plot onto a separate mask, concat to the (encoding of) full image \\
		further, attention mechanism \\
		(soft-crop ???) \\
		$\Rightarrow$ naturally develop into \underline{instance tracking}, even tracking in point cloud data
		\item $\Rightarrow$ \textbf{crop with extension} can significantly draw back the wall time \\
		(due to: 1. crop in cpu \& for each object; 2. crop is not generally gpu accelerated) \\
		especially, when object is large in the image $\Rightarrow$ need to pad a lot \\
		(e.g. baidu field-end tracking: car emerging/leaving right under the camera)
		\item two masks: one for all bbox from detector, one for current track
		\end{itemize}
		$\Rightarrow$ enable global track for multiple obj track (single RNN for all track in an image)
	\item \underline{Scale Invariance}
		\begin{itemize}
		\item current siamRPN++: trained with a fixed-scale resized image crop as input \\
		$\Rightarrow$ if object is non-rigid: pixel size can change significantly between frames \\
		$\Rightarrow$ crop image according to last frame would contain crop out far more/less background than expected \\
		$\Rightarrow$ wrong percentage of fore-/back-ground in the image crop \\
		$\Rightarrow$ model input (as resized to fixed size) has different object scale than training \\
		$\Rightarrow$ failed to track \& fall back to only detection (as containing RPN) \\
		(reproduced by having different context\_amount in inference-training)
		\item siamFC: in test time, extract template at multiple (e.g. 3) scales for each target
		\item HENCE, try to have scale invariance to address abrupt size change in target
		\end{itemize}
	\item SOT
		\begin{itemize}
		\item directly use cnn + conv-lstm
		\end{itemize}
	\item single Obj Tracking for MOT
		\begin{itemize}
		\item common cnn encodes image $t$ as $f_t$, detected bbox as $D_t$ (given or trained) \\
		(prefer to train, as auxiliary loss for multi-tasking ?)
		\item encode each detection $d_t \in D_t$ as $c(d_t) \in c(D_t)$ \\
		empty detection always $\in$ $D_t$ \\
		$\Rightarrow$ track may always have NO compatible det
		\item one rnn tracker for each $c(d_t)$
		\item rnn takes in encoded previous prediction $c(p_{t-1})$ (produced by itself), with $f_t$, and each encoded det $c(d_t) \in c(D_t)$ \\
		$\Rightarrow$ produce as many track as $D_t$, each with a probability \\ 
		(control the track's death) \\
		$\Rightarrow$ only the highest remains, others deleted
		\item remaining track rnn regress pred bbox $p_{t+1}$ (an encoded bbox as well) \\
		(a decoder at $t$ trying to induce the $d_{t+1}$) \\
		(GAN ?)
		\end{itemize}
	\item MOT as SOT
		\begin{itemize}
		\item attention mask contain multiple interest \\
		$\Rightarrow$ not able to handle birth/death of independent object \\
		(as modeled all as a whole)
		\end{itemize}
	\item End-to-End MOT
		\begin{itemize}
		\item design requirement
			\begin{itemize}
			\item arbitrary start of track
			\item arbitrary num of track
			\item arbitrary end of track
			\end{itemize}
		\end{itemize}
	\item End-to-end MOT with States (Markov Decision Process / Deterministic Finite Automaton)
		\begin{itemize}
		\item action at each state given by NN
		\item history of track in RNN $\Rightarrow$ provided when making decision \\
		$\Rightarrow$ RNN modeling all the state ??? \\ 
		(update with different set of weights on different chosen actions ?) \\
		(design transition instead of states: states as the closure of all actions ?)
		\item design state for each challenge scene separately (out-of scene, occlusion, long-term lost, etc.) \\
		$\Rightarrow$ directly tackle each scene \\
		(state growing: auto-discover state ???)
		\item RNN for actions ? \\
		output score for all actions, only legal actions (given current state) considered\&selected, then transfer state accordingly
		\item trained with RL
		\end{itemize}
	\end{itemize}
\item Training
	\begin{itemize}
	\item MDP as Hard-Negative Mining
	\item Sparse Detection
		\begin{itemize}
		\item feed only a few detection, demand NN to fill up using track history\&prediction
		\end{itemize}
	\item Other NLP Training
		\begin{itemize}
		\item training of n-gram model used on rnn traker ?
		\end{itemize}
	\end{itemize}
\item Fixable Track
	\begin{itemize}
	\item Fixing After Lost
		\begin{itemize}
		\item backward rnn for re-associated track after lost \\
		$\Rightarrow$ to fix the previous prediction given current observation \\
		(than recompute the forward rnn for consecutive tracking)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Directoin - Referring Seg}
\begin{itemize}
\item Integrating Encoder-Decoder Architecture
	\begin{itemize}
	\item Upsampling
		\begin{itemize}
		\item similar to Unet, concat low-level spatial info
		\item introduce language info as well \\
		(e.g. early combination, explicit introducing, ...)
		\end{itemize}
	\end{itemize}
\item Info Early Fusion
	\begin{itemize}
	\item Tiling at First Conv
		\begin{itemize}
		\item as siamese net for joint input
		\item downsampling more responsible for language info processing \\
		$\Rightarrow$ hopefully get more fine-tuning alone with conv filters
		\item can be used with pre-trained net: \\ 
		$ReLU(conv_1*X_1 + conv_2*X_2) = ReLU( [conv_1,conv_2]*[X_1,X_2] )$
		\end{itemize}
	\item Multiple Entries
		\begin{itemize}
		\item combining info at different stages of downsampling / upsampling
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Attention from Combined Info
		\begin{itemize}
		\item as key-word-aware net
		\end{itemize}
	\item Attention on Language Info
		\begin{itemize}
		\item $1$-D spatial pyramid pooling / attention mask on the sentence encoding
		\end{itemize}
	\end{itemize}
\item Language Info Throughout Network
	\begin{itemize}
	\item Encoder-Decoder for Language Info
		\begin{itemize}
		\item network asked to recover language info after processing combined info \\ 
		(potentially via a separate branch only at training time) \\ 
		$\Rightarrow$ auxiliary loss
		\end{itemize}
	\item Language as Conv Filter
		\begin{itemize}
		\item Language Info, through a subnet, becoming a set of conv filters \\
		$\Rightarrow$ then imposed in downsampling, tunnel, upsampling or bridge stage(s)
		\end{itemize}
	\end{itemize}
\item Data Augmentation
	\begin{itemize}
	\item Translation Module
		\begin{itemize}
		\item using the same image
		\item expression translated to a middle language and then back to English \\
		$\Rightarrow$ language info trained more finely
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Directoin - Tactile Perception}
\begin{itemize}
\item Close-range Camera as Tactile Sensor
	\begin{itemize}
	\item directly location mapping between camera \& tactile sensor via IMU \\
	$\Rightarrow$ jointly process in cnn
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Basic Neutral Network}

\subsection{Advantages}
\subsubsection{Large/Big Data}
\begin{itemize}
\item Larger Maximum Capability
	\begin{itemize}
	\item Curve given Amount of Data
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/background-largedata".png}
	\end{figure}
	\item Reasons
		\begin{itemize}
		\item the scale of data (labeled)
		\item the scale of neural network (computability)
		\item the scale of efficiency: e.g. ReLu, faster parallel algorithm
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Flexibility}
\begin{itemize}
\item Different Structures for Different Tasks
	\begin{itemize}
	\item Same Data \& Task
		\begin{itemize}
		\item changing settings/structures of deep learning model can make a difference \\
		(v.s. SVM, etc.)
		\end{itemize}
	\end{itemize}

\item Ability to Choose Basis Functions
	\begin{itemize}
	\item Functional View
		\begin{itemize}
		\item $\displaystyle y(\mathbf {x}, \mathbf w)=f(\mathbf w^T\phi(\mathbf x)), \text{ where } \phi \text{ is basis function }, f(\cdot) \text{ is net as a function}$
		\end{itemize}
	\item Learning $\phi$: choose embedding $\Rightarrow$ choose basis function
	\item Learning $\mathbf w$: choose which feature / basis functions more useful
	\end{itemize}
	
\item Solving Bias-Variance Trade-off
	\begin{itemize}
	\item Complexity + Data/Regularization
		\begin{itemize}
		\item easy complexity via depth, size \\
		$\Rightarrow$ reduce bias, without hurting variance by utilizing big data
		\item easy regularization via L$2$ ant etc.\\ 
		$\Rightarrow$ prevent high variance without hurting bias much in a deep/big net
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Power of Depth}
\begin{itemize}
\item Deep Representation
	\begin{itemize}
	\item Low-level $\rightarrow$ High-level
		\begin{itemize}	
		\item multiple layers to choose \& combine useful information (creating new feature/basis) \\
		$\Rightarrow$ next layer use chosen/combined simple basis to build more complex one
		\item $\Rightarrow$ an hierarchy from low-level information to high-level information
		\end{itemize}
	\end{itemize}

\item Circuit Theory
	\begin{itemize}
	\item Power of Combination
		\begin{itemize}
		\item functions that can be compactly represented by a depth $k$ architecture might require an exponential number of computational nodes using a depth $k-1$ architecture \\
		(from the perspective of factorization)
		\end{itemize}
	\end{itemize}
\textbf{Yet, start from the SHALLOW (logistic regression) before trying the deep}
\end{itemize}

\subsection{Problem}
($n$ units in one hidden layer)

\subsubsection{Weight-space Symmetries} 
\begin{itemize}
\item Symmetries in Activation Function
	\begin{itemize}
	\item $\mathcal{O}(2^n)$, e.g. $\arctan(-x) = -\arctan(x) \Rightarrow$ changing signs of all input \& output has the same mapping (reduce effective data)
	\end{itemize}
\item Positional Combination in One Layer
	\begin{itemize}
	\item $\mathcal{O}(n!)$ exchange unit with each other (together with their input output weights) $\Rightarrow$ mapping stay the same
	\end{itemize}
\end{itemize}
$\Rightarrow \mathcal O(n!2^n)$ overall weight-space symmetries

\subsubsection{High-Dimension Search Space}
\begin{itemize}
\item Multiple Critical Points
	\begin{itemize}
	\item Symmetries
		\begin{itemize}
		\item at least $\mathcal O (n!2^n)$ critical points ($\nabla E(w) = 0$), where $E(w)$ is error function \\
		due to weight-space symmetries
		\end{itemize}
	\item Saddle Points
		\begin{itemize}
		\item both the bottom (in one dimension) and the top for another
		\item due to high-dimension weight space \\ 
		$\Rightarrow$ more likely to have functions being convey/convex in different dimensions
		\end{itemize}
	\item Local Optima
		\begin{itemize}
		\item less then saddle points in amount, due to high-dimension weight space \\ 
		e.g. usually $\ge 10^4$-D for modern deep nets
		\end{itemize}
	\end{itemize}
\item Plateaus
	\begin{itemize}
	\item a large flat region where gradient $\rightarrow 0$ \\
	$\Rightarrow$ gradient descent slowly down the flat surface (before exiting)
	\item $\Rightarrow$ slow down gradient descent significantly
	\end{itemize}
\item Expensive in Finding Critical Point
	\begin{itemize}
	\item expensive for even local optima with gradient decent
	\item as expensive as $\mathcal O(n^3)$ if using Laplace approximation
	\end{itemize}	
\end{itemize}

\subsubsection{Gradient Vanishing/Exploding}
\begin{itemize}
\item Gradient Vanishing
	\begin{itemize}
	\item Saturated Function
		\begin{itemize}
		\item sigmoid/tanh function: gradient $\rightarrow 0$ when input $\rightarrow \pm \infty$
		\end{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item with depth $L$, each activation (e.g. tanh) output $a^l < 1$ and weight $\mathbf w^l <1$ \\ 
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'<1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $<1$ \\
		$\Rightarrow$ gradient exponentially decayed in back-prop
		\end{itemize}
	\end{itemize}
\item Gradient Exploding
	\begin{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item similarly, each activation (e.g. ReLU) output $a^l>1$ and weight $\mathbf w^l > 1$ \\
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'>1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $>1$ \\
		$\Rightarrow$ gradient exponentially augmented in back-prop
		\end{itemize}
	\end{itemize}
\item Possible Solutions
	\begin{itemize}
	\item Random Initialization
		\begin{itemize}
		\item \hyperref[DL_Init_Xavier]{Xavier Initialization}: for gradient vanishing \& exploding
		\end{itemize}
	\item Activation 
		\begin{itemize}
		\item \hyperref[DL_Act_ReLU]{ReLU}: for gradient vanishing
		\end{itemize}
	\item Skip/Concat Connection
		\begin{itemize}
		\item \hyperref[DL_Block_Res]{residual block}
		\item 
		\end{itemize}
	\end{itemize}	
\end{itemize}

\subsection{Learning}
\subsubsection{Forward-Backward Propagation}
\begin{itemize}
\item Representation
	\begin{itemize}
	\item Layers
		\begin{itemize}
		\item input layer
		\item hidden layer(s): layer with NO ground truth (for the associated weights) available \\
		note: input \& hidden layers have associated biases as well (usually)
		\item output layer
		\end{itemize}
	\item Neuron (Unit)
		\begin{itemize}
		\item $s_l$: num of units in layer $l$
		\item $w^l$: weight matrix of mapping from layer $l$ to $l+1$, with shape of $\left( s_{l+1}, s_l + 1 \right)$
		\item $h(\cdot)$: activation function (usually shared)
		\item $a_j^l$: activation output of unit $j$ at layer $l$
		\item $z_j^l$: output of unit $j$ at layer $l$ \\ 
		(represent parameterized basis, also the input for layer $l+1$)
		
		\end{itemize}
	\item Intuition
		\begin{itemize}
		\item all stacked vertically (vertical vector) \\
		$\Rightarrow$ horizontally for different examples; vertically for different units
		\end{itemize}
	\end{itemize}
	
\item Forward Propagation (Inference)
	\begin{itemize}
	\item Activation $a^{j+1} = w^j \cdot [z_0^j, ..., z_{s_j}^j]^T, \text{ with } z_0=1$
	\item Unit Output $z^{j+1} = h(a^{j+1}) = [z_1^{1}, ..., z^{j+1}_{s_{j+1}}]^T$
	\end{itemize}

\item Backward Propagation
	\begin{itemize}
	\item Loss $\mathcal L(W) = $
	\end{itemize}

\item Practice of Back Prop
	\begin{itemize}
	\item Caching Intermediate Result
		\begin{itemize}
		\item naturally cached: input $a^0=x$, weights matrix $w$ and bias $b$
		\item activation input/output $a/z$ \\
		(since will be used in back-prop)
		\end{itemize}
	\item Auto Difference
		\begin{itemize}
		\item achievement: calculate the derivatives along the forward prop \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Operations \& Layers Structure}
\subsection{Operations in Network}
\subsubsection{Activations}
\begin{itemize}
\item Sigmoid $a=\sigma(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mapping to $(0,1)$, with $\sigma(0)=0.5$
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item gradient vanishing: $\displaystyle \sigma(z)' = \sigma(z)(1-\sigma(z)) \Rightarrow \lim_{z\rightarrow \pm \infty} \sigma(z)' \rightarrow 0$ \\
		(as the gradient passed through (via chain rule) $=\frac{a}{z} \frac{z}{w}$)
		\end{itemize}
	\end{itemize}
\item Tangent $a=\tanh(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item empirically, almost always better than sigmoid (in hidden layers)
		\item maps to $(-1,1)$, with $\tan(0)=0 \Rightarrow$ help centering data ($0$-mean) \\ $\Rightarrow$ make the learning of next layer easier
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item still, gradient vanishing when $z\rightarrow \pm \infty$
		\end{itemize}
	\end{itemize}
\item Rectified Linear Unit (ReLU) $\max(0, z)$ \label{DL_Act_ReLU}
	\begin{itemize}
	\item Derivation: approximated by a stack of sigmoid
		\begin{itemize}
		\item 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate gradient vanishing: $\forall z>0, a=z \Rightarrow$ learn much faster
		\end{itemize}
		$\Rightarrow$ the default choice!
	\item Cons
		\begin{itemize}
		\item undefined behavior at $x=0$ (actually, gradient becomes the sub-gradient)
		\item gradient totally vanished for $x<0$
		\item $\Rightarrow$ dead units: weights learned/initialized to always output negatives \\ 
		$\Rightarrow$ activation always output $0$ \\
		$\Rightarrow$ the unit always output $0$
		\end{itemize}		
	\end{itemize}
\item Leaky Relu $a=\max(\alpha z, z), \alpha \rightarrow 0^+$ (e.g. $\alpha=0.01$)
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate the gradient vanishing problem for $(-\infty, +\infty)$
		\item avoid dead units problem
		\end{itemize}
		(yet not that popular as ReLU)
	\end{itemize}

\item Piecewise Linear Unit (PLU) $a=\max(\alpha(z+\beta)-\beta, \min(\alpha(z-\beta)+\beta, z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item hybrid of tanh \& ReLU: three linear pieces approximating tanh in a given range
		\item more expressive than ReLU: more nonlinear, better to fit smooth nonlinear function
		\item mitigate gradient vanishing problem: due to linearity
		\end{itemize}
	\item Cons
	\end{itemize}

\item Linear (Identity) Activation $a=z$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item used in regression to output real number $\in (-\infty, +\infty)$
		\item used in compression net
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item stacked units with linear activation $\Leftrightarrow$ single linear transformation
		\item logistic regression with linear activation in hidden layer is NO more expressive than logistic regression with no hidden layer \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Normalization in Network}
\begin{itemize}
\item Batch Normalization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for an activation in hidden layer with input $z$, a batch with size $N_b$
		\item calculate the mean of current batch $\displaystyle \mu=\frac 1 {N_b} \sum_n z_n$, where $z_n$ for the $n^{th}$ example 
		\item calculate the deviation of current batch $\displaystyle \sigma = \sqrt{\frac 1 {N_b} \sum_n(z_n-\mu)^2}$
		\item normalize to be $z'_n = \frac {z_n-\mu}{\sigma}$
		\item allow model to recover/manipulate original distribution: $\hat z_n=\gamma z'_n + \beta$, \\
		where $\gamma, \beta$ being trainable (updated by optimizer using gradients)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item preferred to apply batch norm on $z$ (before activation), instead of after it
		\item for math stability, $z'_n=\frac {z_n-\mu}{\sigma+\epsilon}$, with $\epsilon\rightarrow 0+$
		\item (usually) with mini-batch, calculate the mean \& variance from only the mini-batch
		\item with batch norm, original bias $b$ in calculating $z=wx+b$ becomes pointless \\
		$\Rightarrow$ integrated into the $\beta$ in batch norm
		\item at test time ($1$ example a time): need an estimation for $\mu, \sigma$ \\
		$\Rightarrow$ exponentially weighted average over $\beta, \sigma$ in training time
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item normalize the intermediate data to have $0$ mean, unit variance \\
		$\Rightarrow$ to speed up the training from some hidden layers (as normalization does)
		\item remain the ability to transfer the data to have other mean \& variance \\
		(controlled by $\gamma,\beta$)
		\item control the distribution of data in hidden layer \\ 
		$\Rightarrow$ suppress the change of input data distribution for the layer after it \\
		$\Rightarrow$ increase robustness for later layers, against covariate shift \\ 
		(from both the weight update in early layers and the input data change)
		\item regularize the net by adding noise to the input data of hidden layer \\ 
		(due to computing mean/variance only on mini-batch) \\
		$\Rightarrow$ enforce robustness against noise, hence unintended slight regularization effect
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Operations on Network}
\subsubsection{Initialization}
\begin{itemize}
\item Random Initialization for Weights
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item weights initialized to a random variable in a small range e.g. $(-0.03, 0.03)$
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid symmetry problem: \\
		if identical initialization for weights $\Rightarrow$ units in same layer computing exactly same function \\
		$\Rightarrow$ get the same learning step propagated back \\
		$\Rightarrow$ then always compute exactly the same function (by induction)
		\item avoid gradient vanishing: especially for gradient of sigmoid/tanh activation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item NOT concern various nets: sampling in a fixed range may not work for all nets
		\end{itemize}
	\end{itemize}

\item Xavier Initialization for Weights \label{DL_Init_Xavier}
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item set $\forall l\in [1,L], \text{Var}(w^l) = \frac 1{n_l}$ for tanh, $\frac 2{n_l}$ for ReLU, \\
		where $n_l$ is the number of unit in layer $l$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item draw random variable $r\sim \mathcal N(0,1)$
		\item set each of $w^l=r\cdot \sqrt{\frac 2 {n_l}}$ for ReLU, $r\cdot \sqrt{\frac 2 {n_l}}$ for tanh \\ 
		or $r\cdot \sqrt{\frac 2 {n_{l-1}+n_l}}$ proposed by 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item theoretically justified to initialized weights to be around $\pm 1$ \\
		$\Rightarrow$ mitigate gradient vanishing\& exploding problem statistically 
		\end{itemize}
	\end{itemize}
\item Zero Initialization for Bias
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item default to use $0$ bias \\
		(can NOT used for weights as explained)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Regularization}
\begin{itemize}
\item $L2$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $\Rightarrow$ also called "weight decay" \\
		(as in gradient decent, weight is multiplied by a $<1$ number due to L2 term)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item forcing weights to be smaller
			\begin{itemize}
			\item single node has smaller effect
			\item input of activation closer to $0$ \\
			$\Rightarrow$ activation becomes more linear-alike (e.g. sigmoid, tanh) \\
			$\Rightarrow$ layers perform more linear-alike transformation
			\end{itemize}
		$\Rightarrow$ simpler network, less able to fit extreme curly decision boundary \\
		(hence less able to overfit)

		\end{itemize}
	\end{itemize}
	
\item $L1$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each weight ww we add the term $\lambda \abs w$ to the objective. It is possible to combine the L1 regularization with the L2 regularization: $\lambda_1 \abs w + \lambda_2 w^2$ (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Dropout Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each of selected units, set a drop probability \\
		i.e. for each forward/back-prop, nodes are "dropped" according to the probability \\
		$\Rightarrow$ for each time, a randomly reduced net is trained
		\end{itemize}
	\item Implementation: Inverted Dropout
		\begin{itemize}
		\item set a keep prob $k$ instead of drop prob, for a selected layer
		\item generate random numbers for all units \& turned into a boolean "keep" vector $\mathbf k$
		\item dropped activation $\mathbf d = \mathbf a \times \mathbf k$ (element-wise), \\
		 where $\mathbf a$ is original activation output vector from the layer
		\item $\Rightarrow$ activation becomes $0$ for dropped units in $\mathbf d$
		\item scaling up by dividing the keep prob: $\mathbf d / k$ \\
		$\Rightarrow$ so that expected output value of each activation remains the same
		\item test time: no dropout $\Rightarrow$ no random output \& consider all robust features learned\\
		(randomness in training, mitigated by big data)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item can NOT rely on any one feature $\Rightarrow$ have to spread out weights \\
		$\Rightarrow$ results in shrinking the squared norm of weights (as $L2$)
		\item used on layers with enormous features as input (e.g. computer vision) \\
		$\Rightarrow$ reduce the chance of relying on small set of features
		\end{itemize}
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/regularization-dropout".jpeg}
	\end{figure}
	\item Cons
		\begin{itemize}
		\item training loss may have bigger glitch $\Rightarrow$ harder to debug \\
		(make sure loss decreasing before introduced dropout)
		\end{itemize}
	\end{itemize}

\item Max norm constraints
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $w^l_n$ of every neuron to satisfy (). Typical values of cc are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Early Stopping
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item stop the training at lowest validation loss (with training loss decreasing) \\
		$\Rightarrow$ at the start point of overfitting
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate both train \& val loss, saving models along the way \\
		$\Rightarrow$ use the model corresponding to the start of overfitting
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item at relatively early stage, weights are still relatively small \\ 
		(due to random initialization in $[0^-, 0^+]$)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item couples task of optimizing loss and task of not overfitting \\
		$\Rightarrow$ no longer one task at a time
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Optimization}
\begin{itemize}
\item Batch Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate on entire training set; then update weights
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item largest optimization every time
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item greedy optimizing
		\item slow \& memory demanding on large dataset
		\end{itemize}
	\end{itemize}

\item Stochastic Gradient Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item shuffle data to have training set $X_\text{train}$, further split into $X_\text{train}^{1}, ..., X_\text{train}^{T}$
		\item train the net iteratively with $\forall t\in[1,T], X_\text{train}^t$ \\
		i.e. one mini-batch for a gradient descent (weights update)
		\item after training through all $T$ batches, an epoch of training is finished \\
		$\Rightarrow 1$ epoch = $1$ full scan of training set
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item faster: more weight upgrade over the same amount of data
		\item better chance to reach global change: not greedy anymore
		\item more affordable for training in GPU memory
		\end{itemize}
		$\Rightarrow$ preferred choice
	\item Cons
		\begin{itemize}
		\item observing noisy loss: not monotonically decreasing (but overall decreasing)
		\end{itemize}
	\end{itemize}
	
\item Gradient Descent with Momentum
	\begin{itemize}
	\item Definition: exponentially weighted average
		\begin{itemize}
		\item calculate the gradient for weight update: $dW'_t = \beta dW'_{t-1} + (1-\beta) dW_t$, \\ 
		where $dW$ the original gradient
		\item $\Rightarrow$ average over past gradients with exponentially decaying weight, \\
		$\Rightarrow$ for past $k\in[0,K]$ gradient, coefficient becomes $\beta(1-\beta)^k$ \\ 
		(with $k=0$ denoting current gradient)
		\item bias correction: avoid slow start \\
		(due to: gradient $dW_0$ initialized to $0$ \& not enough gradients for averaging) \\
		$\Rightarrow$ set $dW_t=\frac {dW_t}{1-\beta^t}$ in the early stage \\
		(after starting stage, bias correction $\rightarrow 0$ for large $t$)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item approximation: weighted average over past $K=\frac 1 {1-\beta}$ gradients \\
		due to $(1-\epsilon)^{1/\epsilon} \approx \frac 1 e$, recognized as small enough \\
		$\Rightarrow$ discard gradients with further exponentially small weights
		\item apply element-wise multiplication on gradients and pre-calculated coefficient
		\item sum up to be the gradient for weight update \\ 
		(include bias correction term if necessary, yet often omitted)
		\item note: $dW'_t = \beta dW'_{t-1}+dW_t$ is another version, yet discouraged \\
		(coupling momentum $\beta$ with learning rate $\alpha$, as $\alpha$ needs to cooperate)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item averaging/smoothing out the regular oscillation in stochastic gradient descent \\
		$\Rightarrow \beta$ popularly chosen to be $0.9$ (averaging over last $10$ gradients) 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid some regular oscillation (slowing down the training \& not true randomness)
		\end{itemize}
	\end{itemize}
	
\item Root Mean Square Propagation (RMS prop)
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $S_t = \beta S_{t-1} + (1-\beta) dW_t^2$ ($S_0$ initialized to $0$),\\ 
		where $dW^2$ the original gradient being element-wisely squared\\
		$\Rightarrow$ exponentially weighted square of gradients
		\item calculate the gradient for weight update $dW'_t=\frac {dW_t}{\sqrt{S_t}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item calculate $S_t$ similarly (as an exponentially weighted average)
		\item $\sqrt{S_t}$ becomes $\sqrt{S_t+\epsilon}$, where $\epsilon\rightarrow 0^+$ for mathematical stability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item for gradients with large variance in training $\Rightarrow S_t$ large $\Rightarrow \frac 1{\sqrt{S_t}}$ small \\ 
		$\Rightarrow$ weighted less, hence stabilized (as it should be noisy \& taking smaller step)
		\item for gradients with small variance \\ 
		$\Rightarrow$ weighted more, encouraged (as it should be on the "trend" towards optimum)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item recognize trend from noise via variance of their gradient $\Rightarrow$ speedup training
		\item auto-fixing learning rate for each weight given the recorded behavior \\ 
		(protect learning process from a too large learning rate)
		\end{itemize}
	\end{itemize}
	
\item Adaptive Momentum (Adam) Optimization Optimization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $M_t = \beta_1 M_{t-1} + (1-\beta_1) M_t$ as momentum
		\item compute $S_t = \beta_2 S_{t-1} + (1-\beta_2) dW_t^2$ as root mean square
		\item apply bias correction on both: $M'_t=\frac {M_t}{1-\beta_1^t}, S'_t=\frac {S_t}{1-\beta_2^t}$
		\item $\Rightarrow$ calculate gradient for update $dW'_t=\frac {M'_t} {\sqrt{S'_t+\epsilon}}$, where $\epsilon\rightarrow 0^+$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item implement $M_t,S_t$ as momentum and root mean square \\
		(popular choice: $\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}$)
		\item do implement bias correction
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item combine momentum with root mean square \\
		$\Rightarrow$ for each weight
			\begin{itemize}
			\item smooth out regular oscillation
			\item encourage the trend \& adapt learning rate given history record
			\end{itemize}
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item effective for a large range of problem
		\end{itemize}
	\end{itemize}

\item Learning Rate Decay
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item update learning rate $\alpha = \frac 1 {1+r\cdot e}$, where $r$ the decay rate, $e$ the epoch number
		\item other decay formula:
			\begin{itemize}
			\item exponential decay: $\alpha=r^e\cdot\alpha_0$, where $\alpha_0$ the base learning rate
			\item $\alpha=\frac k {\sqrt{e}} * \alpha_0$, where $k$ a constant
			\end{itemize}
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item set learning rate for each epoch, or after some global steps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fast learning at the beginning, more cautious when approaching the optimum \\ 
		$\Rightarrow$ in order to finally converge
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Cost}
\subsubsection{Probabilistic Cost}
\begin{itemize}
\item Log Maximum Likelihood / Posterior
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item convert the logits into probability-alike prediction \\ 
		$\Rightarrow$ then interpreted as predicted likelihood $p(\mathbf y|\mathbf w, \mathbf x)$
		\item bayesian regression $\displaystyle L = -\frac 1 2\sum_{\mathbf y\in \mathbf Y} (\mathbf y - \hat {\mathbf y})^2$ \\
		(for $\mathbf y$ real number vector label, $\hat {\mathbf y}$ real number vector prediction)
		\item classification with logistic assumption $\displaystyle L = -\sum_{\mathbf y\in\mathbf Y}(\mathbf y^T \cdot \log \hat {\mathbf y})$ \\
		($t$ one-hot encoded label, $\hat y$ one-hot encoded prediction)
		\item to use posterior with Gaussian distribution: add $L2$ regularization term
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Layers}

\subsubsection{Prediction}
\begin{itemize}
\item Sigmoid

\item Softmax
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item arbitrary input $\mathbf z^{L}$ being logits, containing multiple multi-class predictions $z^{L}$ \\
		$\Rightarrow$ each prediction being the same dimension as one-hot encoded label
		\end{itemize}
	\item Output
		\begin{itemize}
		\item probabilistic-alike prediction $\mathbf a^L$, with the same shape as the input (logits)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item for $K$ classes to predict $\Rightarrow$ $\dim (z^L)=K$
		\item for each dimension $k\in[1,K]$, compute $\displaystyle a^L_k=\frac{e^{(z^L_k)}}{\displaystyle \sum_{k=1}^K e^{(z^L_k)}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item vecotrize the exponential computation $\hat z^L = \exp{(z^L)}$
		\item compute normalization $N=\displaystyle \sum_{k=1}^K {\hat z^L_k}$
		\item normalize as $a^L=\frac 1N \hat z^L$
		\item maximum likelihood with softmax: $\displaystyle L = \frac 1N \sum_{\mathbf Y}-\mathbf y^T \cdot \log \hat {\mathbf y}$, \\
		where $\mathbf y$ the one-hot encoded label, $\hat {\mathbf y}$ the prediction \\
		$\Rightarrow$ easy gradients: $dz^L = \hat{\mathbf y} - \mathbf y$, where $z^L$ the logits (vector)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item contrasting the hard-max function (non differentiable): $a_k = 1 \text{ if } \displaystyle \arg\max_k(z); \text{ else } 0$
		\item exponentially normalizing the output of arbitrary net into probabilistic form \\
		(reduced to logistic for binary class i.e. $K=2$) \\
		$\Rightarrow$ generalize logistic prediction to $K$-class prediction
		\item for maximum likelihood loss, only the gap with true class generate gradients \\ 
		(due to one-hot encoding) \\ 
		$\Rightarrow$ trying to predict the class true with higher probability
		\end{itemize}
	\end{itemize}

\item Hierarchical Softmax \label{DL_Layers_Hisoftmax}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item arbitrary input $\mathbf z^{L}$ being logits, considered as a flatten tree
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a tree structure, flatten into $1$-D array
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item for each node (if not leaf), a softmax to predict prob for its direct children \\
		$\Rightarrow$ multiple softmax connected to various input nodes in $\mathbf z^L$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item create a Bayesian network with a single root node (e.g. "obj") \\
		$\Rightarrow$ each node being a conditional probability conditioned on its direct parent
		\item for absolute probability of each class (represented by a node) \\ 
		$\Rightarrow$ apply sum rule \& product rule accordingly
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item NOT assuming mutual exclusion between class
		\item $\Rightarrow$ enable graceful degrade in classification \\
		e.g. can still recognize by $p(\text{animal})$, if failed with $p(\text{cat}), p(\text{dog})$, etc.
		\item enable joint training with multiple datasets (involving classification) \\
		e.g. \hyperref[DL_CV_Objdet_YOLOv2]{YOLOv2(YOLO9000)}
		\end{itemize}
	\end{itemize}

\item Normalization
\end{itemize}

\subsubsection{Convolution Layer}
\begin{itemize}
\item Convolution $2D$
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item spatially $2D$ feature maps, usually with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given hyperparameter: kernel/filter size, stride, padding
		\item kernel (weights) structured as a matrix (for each input channel)
		\item input maps padded if required
		\item an element-wise weighted sum on the spatially corresponding position
		\item sum across channels: sum over kernel output from each channel $+$ optional bias
		\item kernel strides spatially across the image, with stride along each axis defined \\
		$\Rightarrow$ to calculate $\mathbf 1$ channel in the output feature maps \\
		$\Rightarrow$ for multi-channels output: multiple sets of kernels
		\begin{figure}[ht]
		\centering
		\begin{subfigure}{.7\linewidth}
		\includegraphics[width=.75\linewidth, right]{"./Deep Learning/plot/layer-conv2d exp".png}
		\end{subfigure}%
		\begin{subfigure}{.3\linewidth}
		\includegraphics[width=.5\linewidth, center]{"./Deep Learning/plot/layer-conv2d kernel movement".png}
		\end{subfigure}%
		\end{figure}
		\item activation then taken after convolution operation, element-wisely
		\end{itemize}
	\item Padding
		\begin{itemize}
		\item reason
			\begin{itemize}
			\item prevent output feature maps from spatially shrinking
			\item prevent info lost on the edge\&corner of image \\ 
			(compared to the central part of feature map, multiplied less with the kernel)
			\end{itemize}
		\item convention: $0$-padding on both directions of an axis
		\item valid conv: no padding
		\item same conv: pad so that output size same as input size
		\end{itemize}
	\item Kernel Size
		\begin{itemize}
		\item odd square matrix: avoid asymmetric padding \& a central pixel for filter location \\
		(even becomes a convention)
		\end{itemize}
	\item Stride
		\begin{itemize}
		\item the step for kernel to move its location as striding over feature maps
		\item kernel striding over the edge (after padding): NOT convoluted
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a feature maps with defined output channel \& size \\
		(output channel depends on the number of sets of kernels)
		\item on a $2D$ square feature map, with padding at each edge $p$, stride on all axises $s$ \\ 
		kernel size $k\times k$ , input size $i\times i$, output size $o\times o$ \\
		$\displaystyle \Rightarrow o = \left\lfloor\frac {i+2p-k} {s} \right\rfloor + 1$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item computer vision filter: learn the weights in filters, instead of hand design \\ 
		$\Rightarrow$ guided by data statistics
		\item weights in the $l^\text{th}$ conv layer: $n_c^{l-1} \times k\times k \times n_c^{l}$, where $n_c$ the channel number \\
		(to output $n_c^l$ channels with $n_c^{l-1}$ input channels from previous layer) \\
		$\Rightarrow$ invariant to the input size (number of trainable variables fixed on design) \\
		$\Rightarrow$ less weights (then dense layer), more generalizability, hence less overfitting
		\item sharing weights spatially: apply same weights over the whole space \\
		$\Rightarrow$ NO need for special design at each location \\
		$\Rightarrow$ as need to handle spatial variance in processing images
		\item sparse connection: output connected only to the local input $\Rightarrow$ as a high-pass bandwidth \\
		$\Rightarrow$ robust to spatial variance
		\end{itemize}
	\item Back Propagation
	\item Implementation
		\begin{itemize}
		\item implement cross-correlation instead of convolution \\ 
		(skip the flipping operation: as their results are symmetric) \\
		yet, convolution is associative, due to the flipping
		\end{itemize}
	\end{itemize}

\item $1\times1$ Convolution
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item multi-dimension feature maps with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item integrate channels at each spatial location together \\
		(a weighted sum with bias, as conv definition)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature maps with same dimensions, but different channels
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item shrink the number of channels
		\item add more non-linearity \& info combination (more representability)
		\item 
		\end{itemize}
	\end{itemize}

\item Atrous Convolution
\item Deconvolution

\item Convolution Implementation
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item span the original input in the extra dimension, by duplicating the input \\
		$\Rightarrow$ each input location generate its contribution at corresponding output location \\
		(collected by the sum op in matrix multiplication)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Pooling Layer}
\begin{itemize}
\item Max/Average Pooling
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item feature maps
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item given hyperparameter: kernel size, stride (usually no padding)
		\item compute the max/average of the elements covered by kernel
		\item kernel strides along each axis over the feature maps (like conv) \\
		$\Rightarrow$ does NOT change the channel \\
		$\Rightarrow$ one kernel per channel (compared to conv)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a downsampled feature maps
		\item given a $2D$ feature map with kernel size $k \times k$, strides along each axis $s\times s$ \\
		input size $i\times i$, output size $o\times o$ \\
		$\Rightarrow o = \left\lfloor \frac {i-k} {s} \right\rfloor + 1$ (same as conv)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item downsampling the feature maps (NO weights to learn) \\ 
		$\Rightarrow$ if desired features detected anywhere, represent it by local max/average
		\item average: use the mean as representation of desired region
		\item max: use the outstanding one as the representation of desired region
		\end{itemize}
	\end{itemize}
\item Unpooling
\item Spatial Pyramid Pooling (SPP)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps from CNN
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item apply on feature map a series of grids with cell number predefined
		\item perform pooling on each cell, across all grids, then concat all output \\
		$\Rightarrow$ gird defined as a proportional slicing \\
		$\Rightarrow$ actual gird solved at runtime (w.r.t feature map size)
		\item example: three grids with different cell number; each cell a max pooling \\
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-pooling spp".png}
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a fixed size feature maps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item output shape of pooling layer pre-defined $\Rightarrow$ map arbitrary input size to fixed size \\
		$\Rightarrow$ no more crop/resize on input image \\
		$\Rightarrow$ conv layer need only to handle normal ratio (less burden)
		\end{itemize}
	\end{itemize}
\item Region of Interest Pooling (RoI Pooling)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps from CNN
		\item RoIs i.e. proposal region (from selective search etc.) projected on feature map
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item divide each RoI with grid of desired size (proportional to the RoI size)
		\item max pooling from each cell
		\end{itemize}
		$\Rightarrow$ single-size SPP for each RoI
	\item Output
		\begin{itemize}
		\item a fixed size feature maps for each RoI
		\end{itemize}
	\end{itemize}
\item Probabilistic Max Pooling
\end{itemize}
\subsubsection{RNN Layer}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item sequence data: includes time/precedence (conventionally arrived from left to right)
		\item for each time step, data can be vector, feature maps, etc...
		\end{itemize}
	\item RNN Cell
		\begin{itemize}
		\item consume the input of current time step \& the hidden state from last time step \\
		(hidden state usually initialized to $\mathbf 0$)
		\item calculate a hidden state at each time step
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item RNN cell at time $t$, calculate hidden state (activation) $h^t = g_h(w_{h}[h^{t-1},x^t] + b_h)$, \\ 
		where $g_h(\cdot)$ the activation function, $g_h=\tanh$ by convention \\
		$[h^{t-1}, x^t]$ the concat of $h^{t-1}$ (hidden state of time $t-1$), $x^t$ (input at time $t$)
		\item expose its hidden state at each time steps
		\item calculate its output $y^t=g_y(w_{y}h^t+b_y)$, where $g_y=\sigma, softmax$ or $identity$
		\end{itemize}
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-rnn unrolled".png}
		\end{figure}
	\item Types of RNN Mapping
		\begin{itemize}
		\item many-to-one: encoder scans through the input, only the last output considered
		\item one-to-many: decoder with single input $x^1$, take $x^t=y^{t-1}$, till $y^{t'}=$ stop
		\item many-to-many: a many-to-one encoder, followed by a one-to-many decoder \\
		$\Rightarrow$ able to map between various length
		\end{itemize}
	\item Back Propagation through Time
		\begin{itemize}
		\item unroll the recurrent operation into a sequential network with length $T$
		\item given the loss for each time step $L^1,...,L^T \Rightarrow L = \sum_{t=1}^T L^t$
		\item for time $\displaystyle t=1,...,T-1, \frac {\partial} {\partial a^t} L = \frac {\partial} {\partial a^t} L^t + \sum_{t'=t+1}^T\frac {\partial L^{t'}} {\partial a^{t+1}} \frac {\partial a^{t+1}} {\partial a^t} = \sum_{t'=t}^T \frac {\partial} {\partial a^t} L^{t'}$
		\end{itemize}
	\item Truncated Back Propagation through Time
	\item Challenge
		\begin{itemize}
		\item bad at modeling longterm dependency due to gradient vanishing problem \\
		$\Rightarrow$ loss at late time needs to go through multiple activations to the early time \\
		(similar to the deep plain net, after unrolled) \\
		$\Rightarrow$ loss at late time are hard to affect weights when evaluated at early time \\
		(i.e. larger the $t'$, smaller the $\frac {\partial} {\partial a^t} L^{t'}$) \\
		$\Rightarrow$ hard to find out error in late time due to observation in early time \\ 
		$\Rightarrow$ hard to represent longterm dependency 
		(e.g. Car...is fast vs. Cars...are fast)
		\item easily affect by local dependency (as longterm dependency lost)
		\item gradient exploding, due to multiple / too many updates on the same weights \\
		(solved by gradient clipping)
		\item hard to converge, due to fluctuating gradient in an unroll \\ 
		(noisy intermediate stage) \\
		$\Rightarrow$ initial short sequence to overcome plateaus, then long sequence for dependency
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item sharing weight across time: same weights used on each time step \\
		$\Rightarrow$ solve various length input by applying weights recurrently on each of them
		\item information early in the sequence reserved \& passed through in the hidden state
		\end{itemize}
	\end{itemize}

\item Long Short Term Memory (LSTM)
	\begin{itemize}
	\item Memory Cell
		\begin{itemize}
		\item $c^t$ the memory maintained by LSTM cell at time $t$
		\end{itemize}
	\item Gates
		\begin{itemize}
		\item forget gate $G_f=\sigma(w_f[h^{t-1}, x^t] + b_f)$
		\item input gate $G_i = \sigma(w_i[h^{t-1}, x^t] + b_i)$ \\
		$\Rightarrow$ together control the memory update (how past\&current info fused)
		\item output gate $G_o = \sigma(w_o[h^{t-1}, x^{t}] + b_o)$ \\
		$\Rightarrow$ control the generation of hidden state
		\end{itemize}
	\item Fusing Info
		\begin{itemize}
		\item propose candidate $\hat{c}^{t}=\tanh(w_c[c^{t-1}, x^{t}]+b_c)$ for memory update
		\item update memory as $c^t = G_f c^{t-1} + G_i \hat c^t$
		\end{itemize}
	\item Hidden State (Activation)
		\begin{itemize}
		\item generate as $h^t = G_o \cdot \tanh(c^t)$
		\end{itemize}
	\begin{minipage}[r]{0.5\linewidth}
	\includegraphics[width=\linewidth, center]{"./Deep Learning/plot/layer-rnn lstm".png}
	\end{minipage}
	\begin{minipage}[l]{\linewidth}
	where, \\ 
	joint arrows denotes a concatenation \\ 
	$c_{t-1}, \widetilde c_t, c_t$ the previous/candidate/current memory; \\
	$h_{t-1},h_t$ the previous/current hidden state; \\ 
	$f_t, i_t, o_t$ the forget/input/output gate
	\end{minipage}

	\item Understanding
		\begin{itemize}
		\item easy to learn an identity mapping $c^{t-1}\rightarrow c^t$ \\
		$\Rightarrow$ memory (info) generated early can last for long term \\
		$\Rightarrow$ better model the long-term dependency
		\end{itemize}

	\end{itemize}

\item Gated Recurrent Unit (GRU)
	\begin{itemize}
	\item Memory Cell
		\begin{itemize}
		\item $c^t$ the memory maintained by GRU cell at time $t$
		\end{itemize}
	\item Gates
		\begin{itemize}
		\item relevance gate $G_r=\sigma(w_r[c^{t-1}, x^t]+b_r)$, a mask $\in [0,1]$ \\ 
		$\Rightarrow$ control how memory candidate proposed (fusion of last memory \& input)
		\item update gate $G_u=\sigma(w_u[c^{t-1}, x^t]+b_u)$, a mask $\in [0,1]$ \\
		$\Rightarrow$ control how memory update happen (fusion of last memory \& candidate)
		\end{itemize}
		(two gates computed with the same input, though with different weights)
	\item Fusing Info
		\begin{itemize}
		\item propose memory candidate $\hat{c}^{t}=\tanh(w_c[G_r c^{t-1}, x^{t}]+b_c)$ for the update \\
		$\Rightarrow$ decide whether the previous memory useful (relevant) with the context $x^t$
		\item update memory as: $c^{t+1}=G_u \hat{c}^{t+1} + (1-G_u)c^{t-1}$ \\ 
		$\Rightarrow$ decide how the memory updated \& remained
		\end{itemize}
	\item Hidden State (Activation)
		\begin{itemize}
		\item $h^t = c^t$
		\end{itemize}
		
	\begin{minipage}[r]{0.5\linewidth}
	\includegraphics[width=\linewidth, center]{"./Deep Learning/plot/layer-rnn gru".png}
	\end{minipage}
	\begin{minipage}[l]{\linewidth}
	where, \\ 
	joint arrows denotes a concatenation \\ 
	$h_{t-1}, \widetilde h_t, h_t$ the previous/candidate/current memory; \\ 
	$r_t, z_t$ the relevance/update gate
	\end{minipage}
	
	\item Understanding
		\begin{itemize}
		\item single gate control the generation of current memory
		\item memory directly as hidden state
		\item $\Rightarrow$ less weights, simpler structure $\Rightarrow$ faster \\
		(basic logic inherent from LSTM $\Rightarrow$ similar performance)
		\end{itemize}
	\end{itemize}


\item Bidirectional RNN (BRNN/Bi-RNN)
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item one RNN layer scanning as $t=1\rightarrow T$, hidden state exposed as $h_1^t$
		\item another RNN layer scanning from $t=T\rightarrow 1$, hidden state exposed as $h_2^t$
		\item generate output $y^t = g(w_y[h_1^t, h_2^t] + b_y)$ \\ 
		(concat all hidden states at the same time step from each RNN)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item account for the info from both previous \& latter time step \\
		$\Rightarrow$ global context info acquired
		\item cons: need the entire input sequence before processing \\
		$\Rightarrow$ NOT the case in real-time speech recognition etc.
		\end{itemize}
		
	\begin{figure}[!h]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/layer-rnn bidirection".png}
	\caption{(using LSTM cell)}
	\end{figure}
	
	\end{itemize}

\item Convolutional LSTM (ConvLSTM)
\end{itemize}

\subsubsection{Attention}
\begin{itemize}
\item Global Attention
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item overcome long-term dependency, via considering all input with different attention
		\item focus on different attributes/features based on various requirement
		\end{itemize}
	\item Input
		\begin{itemize}
		\item a vector, feature map, volume, etc.
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item a function over all spatial location: $[f_i, \mathbf r]\rightarrow \text{logits}_i$, \\ 
		where $f_i$ the feature at location $i$, $\mathbf r$ task specific requirement
		\item construct a distribution over all location: $\text{logits} \xrightarrow[]{\text{softmax}} \alpha$, \\
		where $\alpha$ the attention over all location $i$
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a probability distribution over all location of the input
		\end{itemize}
	\item Practice in Seq-to-Seq RNN
		\begin{itemize}
		\item a bi-RNN encode rich input context at each hidden state: $h^1,...,h^{T_x}$
		\item global context after attention at time $t$: $\displaystyle c^t=\sum_{i=1}^{T_x}\alpha^t_i h^i = [h^1,...,h^{T_x}]\alpha^t$, \\
		where hidden states $h^1,...,h^{T_x}$ and attention $\alpha^t$ column vectors \\
		$\Rightarrow$ a weighted sum over all hidden states as context
		\item a small (1-layer) net mapping $[c^{t-1}, h^{i}] \xrightarrow[]{\text{dense}} \text{logits} \xrightarrow[]{\text{softmax}} \alpha^t_{i}$ \\
		$\Rightarrow$ attention on $h^i$ depends on previous global context $c^{t-1}$ \& $h^i$ itself \\
		(softmax to ensure $\sum_{i=1}^{T_x}\alpha^t_i=1$)
		\item a decoder RNN taking $c^{t}$ as input, until stop sign generated \\
		(as $c^t$ can be calculated infinite times)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item seq2seq RNN encoder-decoder may fail if input sequence longer than training seq \\
		$\Rightarrow$ aggregate all location of input seq to feed decoder rnn at each time step \\
		(instead of decoder RNN feed itself using its last prediction) \\
		$\Rightarrow$ capture the relationship between input-output sequence
		\item use the sequence of hidden states as encoding \\ 
		(as sentence encoded in last hidden states can be bias \& have info lost) \\
		$\Rightarrow$ account input sequence (with different attention at different places) when generating output sequence
		\item quadratic cost: attention $\alpha$ a $T_x\times T_y$ matrix to be calculated 
		\end{itemize}
	\end{itemize}
	
\item Attention on Image
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item focus on correct spatial context when generating corresponding caption/sequence
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item calculate weights for a sum over all spatial location of the image encoding \\
		$\Rightarrow$ calculate a 2-D mask over a 3-D tensor
		\end{itemize}
	\end{itemize}

\item Self Attention
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item in seq2seq: capture the relationship inside input/output seq \\ 
		(compare with global attention)
		\end{itemize}
	\end{itemize}
\item Attention in Attention
\end{itemize}

\subsection{Blocks}
\subsubsection{Residual Block} \label{DL_Block_Res}
\begin{itemize}
\item Structure
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for the $l+2$ layer, $a^{l+2} = g(z^{l+2} + a^l)$, \\
		where $g(\cdot)$ the activation, $z^{l+2} = W^{l+2}a^{l+1}+b^{l+2}$ from layer $l+1$ 
		\end{itemize}
	\item Main Path
		\begin{itemize}
		\item the usual passing of $a^l$ to layer $l+1$, then the $z^{l+2}$ at layer $l+2$
		\end{itemize}
	\item Skip/Passthrough Connection (Shourtcut) \label{DL_Block_Res_Passthrough}
		\begin{itemize}
		\item $s(x^l)$: the passing of $x^{l}$, from layer $l$ directly to the layer $l+2$
		\item different spatial size in $l, {l+2}$: adjust $x^l$ by
			\begin{itemize}
			\item padding: pad to the size in layer ${l+2}$
			\item spatial stack: merge local channels into single channel by stacking \\
			(e.g. $26\times26\times512\rightarrow13\times13\times2048$)
			\end{itemize}
		\item different channel size $l, {l+2}$: adjust $x^l$ by linear linear transformation \\ 
		(a $1\times1$ conv / matrix with trainable or fixed weights)
		\end{itemize}
	\item Join
		\begin{itemize}
		\item add the result from two paths $\Rightarrow a^{l+2}=g(z^{l+2}+s(a^l))$
		\item or, concat the input $\Rightarrow a^{l+2}=g([z^{l+2}, s(z^{l})])$
		\end{itemize}
	\end{itemize}

\item Understanding
	\begin{itemize}
	\item Guaranteed Baseline
		\begin{itemize}
		\item with ReLU as activation, easy to learn identity function \\ 
		$\Rightarrow$ layer $l+2$ only need to make $z^{l+2}=0$ (as weights \& bias initialized near $0$)
		\item $\Rightarrow$ deeper net can easily guarantee to be at least as good as its shallow version \\
		(then search for luck to surpass baseline)
		\end{itemize}
	\item Ensemble
		\begin{itemize}
		\item contains much more shallow paths than real deep paths \\
		$\Rightarrow$ ensemble those shallow paths at various level
		\end{itemize}
	\item More Gradient
		\begin{itemize}
		\item gradient more easily passed to the early layers \\ 
		(as shortcuts not attenuating gradients)
		\item $\Rightarrow$ early layers settle down faster $\Rightarrow$ late layers get a more consistent input
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Inception Block}
\begin{itemize}
\item Basic Inception
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps with multiple channels
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item op1 = $1\times1$ conv
		\item op2 = $1\times1$ conv, $3\times3$ conv with same padding
		\item op3 = $1\times1$ conv, $5\times5$ conv with same padding
		\item op4 = max pooling with same padding and stride $1$, $1\times1$ conv
		\item channel concate: concatenate the output channel from each op \\
		(as output size ensured to be the same in each op)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item feature maps with the same spatial size as input
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $1\times1$ conv to shrink
			\begin{itemize}
			\item less computation for afterwards larger kernel (e.g. $3\times3, 5\times5$)
			\item prevent output of other ops from being overwhelmed by pooling
			\end{itemize}
		\item enable network to learn the desired combination of info \\
		(instead of predefined hyperparamter) \\
		$\Rightarrow$ more representability
		\end{itemize}
	\end{itemize}

\item Xception
	\begin{itemize}
	\item 
	\end{itemize}	
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Architectures}

\subsection{Convolutional Networks}
\subsubsection{Classic Convolutional Networks}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Convolution Part
		\begin{itemize}
		\item one or multiple "same" conv layer(s) with stride $s=1$
		\item followed by a max (rarely average) pooling
		\item apply on input \& repeat on feature maps afterwards \\
		$\Rightarrow$ usually decreasing spacial size (width, height), increasing channel
		\end{itemize}
	\item Fully Connected (Dense) Part
		\begin{itemize}
		\item apply flattening / average pooling on last feature maps \\
		$\Rightarrow$ generate a single vector as input
		\item apply fully connected layers \& repeat for $1~2$ times \\
		$\Rightarrow$ usually with decreasing size
		\item finally output prediction probability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item trainable weights are mostly from dense layer \\
		$\Rightarrow$ conv layers, though large in number, contains far less weights
		\item conv stride $s=1$ (compare to $s>1$) \\
		$\Rightarrow$ decouple downsampling into the pooling \\
		$\Rightarrow$ account for more info/possibility before downsampling \\
		(also trade-off between required computability)
		\end{itemize}
	\end{itemize}

\item Receptive Fields
	\begin{itemize}
	\item 
	\end{itemize}

\item VGG-16
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item fixed conv: $3\times3$ kernel, stride $s=1$, same padding, ReLU activation
		\item fixed pooling: $2\times2$ kernel, stride $s=2$, max pooling
	\item Notation
		\begin{itemize}
		\item{} [conv64] to denote a conv layer with 64 output channels
		\item pool to denote max pooling
		\item{} [fc4096] to denote a fully connected layer with output vector of length 4096
		\end{itemize}
	\item Structure
		\item input image: $224\times224\times3$
		\item{} ([conv64]$\times$2, pool) $\rightarrow$ ([conv128]$\times$2, pool) $\rightarrow$ ([conv256]$\times$3, pool) $\rightarrow$ ([conv512]$\times$3+pool)$\times$2
		\item last feature maps ($7\time7\times512$) flatten into a input vector of length 4096
		\item{} [fc4096] $\rightarrow$ [fc4096] $\rightarrow$ logits $\rightarrow$ softmax prediction for $1000$ categories
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fixed conv \& pooling operation \\
		$\Rightarrow$ few hyperparameters, yet large in parameters ($\sim$138 million)
		\item decrease spatial size by $2$ \& increase channel by $2$ on each step \\ 
		(till small or deep enough e.g. 7$\times$7 size, 512 channels)
		\end{itemize}
	\end{itemize}

\item ResNets
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item residual block with addition
		\item batchnorm applied in the conv(s) inside residual block
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stack of residual block, for hundreds, even thousands of layers
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item ensemble nets with shallow \& medium depth $\Rightarrow$ essentially a wide net \\
		(as the real deep net takes only few paths)
		\item $\Rightarrow$ avoid the training of real deep net (instead of solving it)
		\end{itemize}
	\end{itemize}
	
\item Inception Net
	\begin{itemize}
	\item Building Blocks
		\begin{itemize}
		\item inception block
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stack of inception blocks
		\item two auxiliary loss from hidden layers
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Fully Convolutional Networks (FCN)} \label{DL_Arch_FCN}
\begin{itemize}
\item FCN for Classification \& Regression
	\begin{itemize}
	\item Convolution as Flattening
		\begin{itemize}
		\item given input feature maps $i\times i\times c$, with $c$ channels
		\item perform valid conv with kernel $i\times i \times o$ \\ 
		$\Rightarrow$ output size $1\times1 \times o$
		\end{itemize}
	\item $1\times1$ Convolution as Fully Connected Layer
		\begin{itemize}
		\item for flatten vector $1\times1\times i$, perform $1\times1$ conv with kernel size $1\times1\times 0$ \\
		$\Rightarrow$ output size $1\times1\times o$, with same computation as fc layer
		\end{itemize}
	\item Use Case
		\begin{itemize}
		\item \hyperref[DL_CV_Objdet]{object detection}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Interpretation and Visualization}
\begin{itemize}
\item Activation Testing
	\item Procedure
		\begin{itemize}
		\item use a fixed set of bounding box proposals (with different images)
		\item treat a single hidden unit in network as a detector/classifier \\
		$\Rightarrow$ use its activation as scoring
		\item for detection: after non-max suppression, rank the boxes from highest to lowest activation \\
		$\Rightarrow$ test out which visual mode the feature is sensing
	\item Understanding on Network
		\begin{itemize}
		\item each feature sensing a different object \\
		e.g. face, red blob, square objects, light reflectance, array of dots, text (like OCR), etc.
		\item able to learn: class-specific shape \& abstract attributes \\
		$\Rightarrow$ construct a \underline{distributed representation} of object \\ 
		(e.g. shape, color, texture, text, material properties, etc.)
		\item $\Rightarrow$ later dense layer can model a large range of object \\
		(as combining different distributed object attributes)
		\end{itemize}
	\end{itemize}
\item Layer Ablation
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item after trained, reduce the num of dense layer, until fully conv \\
		i.e. using previous layer output as prediction at test time
		\item on test set: evaluate performance drop \\
		on other dataset: evaluate generalization ability drop
		\end{itemize}
	\item Understanding on Network
		\begin{itemize}
		\item CNN ability relies on convolution layer, instead of dense layer \\
		$\Rightarrow$ \underline{conv layer as feature map extractor} for other domain-model
		\item fine-tuning improves a lot $\Rightarrow$ fine-tunning the dense layer \\
		(while conv layer are learned to be quite generic)
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Recurrent Neural Network}
\subsubsection{Classic RNN}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Stacked RNN Layers
		\begin{itemize}
		\item the output of layer $l$ becomes the input for layer $l+1$ \\
		$\Rightarrow$ RNN scanning the output of previous RNN layer
		\item $3\sim5$ layer considered deep: as RNN can be unrolled into a deep plain net
		\end{itemize}
	\item Encoder-Decoder RNN
		\begin{itemize}
		\item encoder RNN scans the input sequence, last hidden layer as sequence encoding
		\item decoder RNN takes encoding as initial hidden state, unrolled the output sequence
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Attention}

\subsubsection{Transformer}

\subsection{Encoder-Decoder Architecture}
\subsubsection{Basic}
\begin{itemize}
\item Encoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item downsample/encode input into rich feature maps/vectors
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual input: CNN backbone
		\item natural expression input: RNN backbone
		\end{itemize}
	\end{itemize}
\item Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item upsample/decode rich feature maps back to the original size
		\item actually, impose requirement onto the encoder
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual output: CNN backbone
		\item natural expression output: RNN backbone
		\end{itemize}
	\end{itemize}
\item Connection
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item combine high level information with low level information
		\item image $\rightarrow$ image: outline refinement ...
		\item language $\rightarrow$ language: sentence style capturing
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item concatenation
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Extension}
\begin{itemize}
\item Siamese Network
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image pair $(p_1,p_2)$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item CNN as feature encoder \\
		$\Rightarrow$ same CNN process each image, yielding $(f_1, f_2)$ as extracted feature
		\item jointly process concat $[f_1,f_2]$
		\end{itemize}
	\item Variants
		\begin{itemize}
		\item joint process from the beginning \\
		$\Rightarrow$ concat $[p_1,p_2]$, then CNN till the end \\
		$\Rightarrow$ generally \textbf{better performance}, especially when detailed structure are vital
		\end{itemize}
	\end{itemize}

\item Multiple Encoder (Pseudo Siamese Network)
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item project different information into the same space \\
		(jointly process those information after feature projection/extraction)
		\end{itemize}		
	\end{itemize}

\item Multiple Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item impose multiple requirements to the encoder (via auxiliary loss) \\
		$\Rightarrow$ multi-task training (multiple output branch)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Generative Network}
\subsubsection{Generative Adversarial Network}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computer Vision}
\subsection{Visual Sensor}
\subsubsection{Camera}
\begin{itemize}
\item Basic Image-forming Theory
	\begin{itemize}
	\item Camera Coordinate
		\begin{itemize}
		\item image axises: same as axises for matrix index
		\item $x,y$ align with the image axises, $z$ align with focal line
		\end{itemize}
	\item Resolution
		\begin{itemize}
		\item the minimum feature size of the object under inspection \\
		$\Rightarrow$ finally, represented as the pixel in the image \\
		(hence, jointly controlled by pixel size, lens, object distance and sensor size)
		\item pixel count: total number of pixels in the row/col of image
		\item pixel density: dots/pixels oer inch (dpi)
		\item sensor resolution: pixel count on sensor and their physical size and spreading
		\item lens resolution: the impact of optics (diffraction, etc) on light info towards sensor
		\end{itemize}
	\item ISO: Light Sensitivity
		\begin{itemize}
		\item used to be the light sensitivity of the film
		\item $\Rightarrow$ in digital camera: the amplification/attenuation for raw sensor value \\
		$\Rightarrow$ can add noise if amplified too much, due to dark current
		\item base ISO: 
		\end{itemize}
	\item Exposure Time
		\begin{itemize}
		\item control incoming light (denoted by shutter speed)
		\end{itemize}
	\item Focal Length $F$
		\begin{itemize}
		\item the distance from last lens central point to focal point, determined by lens physics
		\item given a fixed object distance, larger $F$, larger formed object \\
		(object distance need to be larger than focal length) \\
		\includegraphics[width=0.5\linewidth, left]{"./Deep Learning/plot/topic-vision sensor camera focal len".jpg}
		\end{itemize}
	\item Aperture
		\begin{itemize}
		\item diameter of entrance pupil $D$: control the size of aperture \\
		$\Rightarrow$ aperture stop: aperture setting to restrict input pupil size and hence brightness
		\item larger size ($D$), more incoming light
		\end{itemize}
	\item F-Number $N = \frac F D$
		\begin{itemize}
		\item reveal the signal to noise ratio, contrast, image brightness
		\item larger $N$, smaller $D$, less incoming light, larger DOF, larger depth of focus
		\end{itemize}
	\item Depth of Field (DOF) \& Depth of Focus
		\begin{itemize}
		\item definition: the range on $z$ axis where resolution can be maintained \\
		before the lens: depth of field (in the field) \\
		after the lens: depth of focus (inside camera) \\
		\includegraphics[width=0.5\linewidth, left]{"./Deep Learning/plot/topic-vision sensor  camera dof".jpg}		
		\end{itemize}
	\item Depth of Field
		\begin{itemize}
		\item DOF as the range where $1$ pixel can hold all info from $1$ position \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor  camera n and dof".png} \\
		where, dashed line the incoming light; \\ 
		red line the system resolution (the pixel size, changing with object dist) \\
		$\Rightarrow$ DOF defined by intersection of two type of lines \\
		$\Rightarrow$ with large enough DOF, resolution can be bad at the end of DOF \\ 
		(pixel size too large)
		\item resolution with DOF \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor  camera dof and resolution".png} \\
		where black boxes a series of pixels, beams in colors as light info from an object \\
		$\Rightarrow$ the placement of box on beams shows the information contained in a pixel \\
		$\Rightarrow$ larger DOF, large depth range for acceptable resolution \\ 
		(as pixel contain more separable info) \\
		$\Rightarrow$ out of DOF: pixel can not distinguish beams from diff location
		\end{itemize}
	\item Depth of Focus
		\begin{itemize}
		\item depth of focus as the range where $1$ pixel can hold all info from $1$ position
		\item resulted from tilted sensor $\Rightarrow$ may not in the ideal position to receive light info \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor camera do focus".png} \\
		where right-most line as ideal focal plane, left lines as sensor tilted by $12.5/25\mu m$ \\
		$\Rightarrow$ tilt not influence pixel in the center, but the pixel in the corner \\
		$\Rightarrow$ out of depth of focus: pixel can not distinguish beams from diff location
		\end{itemize}
	\end{itemize}
\item Camera System
	\begin{itemize}
	\item Optics
		\begin{itemize}
		\item lens: determine focal length, control f-number, thus DOF
		\item filter: filter out the influence of infer-red
		\end{itemize}
	\item Image Sensor
		\begin{itemize}
		\item tilted sensor for optic-electric conversion
		\item determine the pixel-size, thus influence resolution
		\item dynamic range: range of full electric capacity - smallest signal-to-noise ratio
		\end{itemize}
	\item Color Filter Array
		\begin{itemize}
		\item a filter for each sensor to allow only light of specified color to reach sensor
		\item interpolation to recover color info for each pixel \\
		\includegraphics[width=0.7\linewidth, left]{"./Deep Learning/plot/topic-vision sensor camera color filter array".png}
		\end{itemize} 
	\item ISP
		\begin{itemize}
		\item optimize the raw sensor data to account for:, e.g. \\
		white balance, motion compensation, tone mapping, denoising, etc.
		\end{itemize}
	\item Serdes
		\begin{itemize}
		\item serialized output the displayed image
		\end{itemize}
	\end{itemize}
\item Noise
	\begin{itemize}
	\item Rolling Shutter
		\begin{itemize}
		\item due to image sensor scanning through across the scene \\
		(instead of snapshot the whole scene) \\
		$\Rightarrow$ speed of shutter $+$ object movement speed: skewed scene
		\item scanning scene takes time \& each time step only scanning a roll of sensor
		\item trade-off: able to continue gather photons, thus more sensitive \\
		(v.s. global shutter)
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Lidar}
\begin{itemize}
\item Time of Flight (ToF) System
	\begin{itemize}
	\item Light Source
		\begin{itemize}
		\item wavelength: 905nm vs. 1550nm
		\item laser peak power: need to avoid eye damage
		\item repetition rate: positively related to fps and points intensity
		\item pulse width
		\item beam quality
		\item background noise: more noise for 905nm than 1550nm
		\end{itemize}
	\item Scanning
		\begin{itemize}
		\item spinning: emit single beam \& scan emit angle across 3D space \\
		$\Rightarrow$ \underline{do need motion compensation if sensing when moving}
		\item optical phase array: use electricity to control beam angle (solid state streamer)
		\item flash light: emit laser like flashlight (consume more power)
		\end{itemize}
	\item Receiver
		\begin{itemize}
		\item sensitivity: determine effective detect distance
		\item saturability: determine noise and dynamic range
		\end{itemize}
	\item A/D Converter
		\begin{itemize}
		\item convert received laser to electricity signal
		\item response curve: usually non-linear
		\end{itemize}
	\end{itemize}
\item 
\end{itemize}
\subsubsection{Radar}
\begin{itemize}
\item Radar System
	\begin{itemize}
	\item Waves
		\begin{itemize}
		\item range resolution inversely $\propto$ bandwidth
		\item frequency $\in (76, 81)$ Ghz
		\end{itemize}
	\item Pulse Radar
		\begin{itemize}
		\item transmit antenna $G_t$ send pulse signal
		\item receiver antenna $A_t$ receive signal
		\item measure range by ToF
		\end{itemize}
	\item FMCW Radar
		\begin{itemize}
		\item send frequency modulated continuous-wave signal, instead of pulse \\
		$\Rightarrow$ measure speed (vector) via observing Doppler frequency shift
		\end{itemize}
	\end{itemize}
\item Understanding
	\begin{itemize}
	\item Ability
		\begin{itemize}
		\item work under extreme condition (-40 to 80$^{\circ}$, rain, dust, etc.)
		\end{itemize}
	\item Features
		\begin{itemize}
		\item sensitive to metal
		\item multiple radar interference: receive signal sent by others
		\item multiple reflectance: waves bounce between parallel metal objects
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Objects Detection} \label{DL_CV_Objdet}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Spatial Information
		\begin{itemize}
		\item image maps as $3$-D tensor: width, height, channel (rgb, etc.)
		\item multi/stereo -images for $3$-D detection
		\end{itemize}
	\end{itemize}

\item Goal
	\begin{itemize}
	\item Bounding Box Prediction
		\begin{itemize}
		\item localization as regression task on box attributes ($x,y,h,w$)
		\item classification on object classes \& object existence
		\end{itemize}
	\item Landmark (Key Point) Prediction
		\begin{itemize}
		\item landmarks: the coordinates of key points (of  each type) in image
		\item label of landmark should be consistent across image
		\item output real number as regression task \\
		$\Rightarrow$ used in pose detection, bounding box detection, etc.
		\end{itemize}
	\end{itemize}

\item Understanding
	\begin{itemize}
	\item multi-object localization \& classification
	\end{itemize}
\end{itemize}

\subsubsection{Common Processing}
\begin{itemize}
\item Non-max Suppression \label{DL_CV_Objdet_nonmax}
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item multiple predicted bounding boxes, with probability of existence ($p_e$) relatively large
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item clean up \& account for duplicate bounding boxes on the same object \\
		$\Rightarrow$ for each (predicted) object, finalize prediction to be a single bounding box
		\end{itemize}
	\item Naive Operation
		\begin{itemize}
		\item choose the bounding boxes with highest (maximal) $p_e$ (for each object classes)
		\item remove those bounding boxes whose IoU with it are large \\
		$\Rightarrow$ remove (suppress) bounding boxes with large overlap
		\item repeat until all bounding boxes have a low IoU with each other \\
		$\Rightarrow$ remaining boxes are the final predictions
		\end{itemize}
	\end{itemize}
\item Selective Search
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item class independent regions to distinguish regions of different objects \\
		$\Rightarrow$ object segmentation: potential region containing object (with classification waived)
		\end{itemize}
	\item Hierarchical Grouping
		\begin{itemize}
		\item initialize the image with a region proposals method
		\item compute similarities for each adjacent region pairs
		\item group the most similar region pair into one region \\
		(similarity include: size, color in color space, sift, etc.)
		\item iterate through the process, until whole image grouped into a single region
		\item diverse settings: color space, similarity measurement, region initialization method
		\end{itemize}
	\item Region Scoring
		\begin{itemize}
		\item the earliest merged (grouped) images get the highest score in current run \\
		e.g. last merge (whole image) scored $1$, $2^\text{nd}$ merge scored $2$
		\item propose region with diverse settings \& accumulate score for the same region
		\item select regions in first $n^\text{th}$ high score
		\end{itemize}
	\item Understand
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Region Proposal Based Approaches}
\begin{itemize}
\item Sliding Window Detection
	\begin{itemize}
	\item Bounding Box Proposal
		\begin{itemize}
		\item slide the window with various size, across the image \\
		$\Rightarrow$ propose bounding boxes with predefined size \& location
		\item feed the window into CNN for classification \\
		$\Rightarrow$ CNN as classifier, window as bounding box
		\end{itemize}
	\item Fast Implementation: Sliding Window as Convolution
		\begin{itemize}
		\item implement CNN classification as \hyperref[DL_Arch_FCN]{FCN} \\
		$\Rightarrow$ as conv independent from input size
		\item run directly the CNN on the input image (instead of on each sliding window) \\
		$\Rightarrow$ output size $m\times n \times k$, \\ 
		where $m\times n$ the total number of sliding windows on the image, $k$ the class number
		\item $\Rightarrow$ one times running for all sliding windows \\ 
		(as sliding window essentially is a crop from image)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item though with fast implementation, still not promising regarding result \\
		$\Rightarrow$ sliding window propose a fixed set of window \\
		$\Rightarrow$ fail if not matching window size; or missing location/rotation
		\end{itemize}
	\end{itemize}

\item R-CNN: Regions with CNN Features
	\begin{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item tightly crop/wrap over regions from region proposal methods
		\item regressor output refinement of $x, y, w, h$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item selective search $\Rightarrow \sim 2000$ regions potentially with object
		\item each region edged with a tight bounding box, resized according to CNN
		\item CNN to extract feature for each bounding box, into $4096$-length vector
		\item class-specific SVMs (one for each class) to classify each box (feature vec)
		\item per-class linear regressor to refine the proposed box \\ 
		(better localization mAP $4\%\uparrow$)
		\end{itemize}
	\item Training CNN
		\begin{itemize}
		\item pre-train on classification
		\item fine-tine on detection: classify resized proposed box into $N+1$ classes \\
		($+1$ for non-objectness)
		\item box cropping: the tightest box + some space for context
		\item $32$ box with object \& $96$ box without object $\Rightarrow$ bias to positive example
		\item neg example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} < 0.5$
		\item pos example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} > 0.5$ \\
		(assigned to the label box with max IoU if multiple matched)
		\item rely on val set, with train set as auxiliary source for pos example \\
		$\Rightarrow$ regenerate more class-balanced data split \\
		(avoid annotation noise in train set)
		\end{itemize}
	\item Training SVM
		\begin{itemize}
		\item input $4096$-length vector, output binary classification for a class
		\item neg example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} < 0.3$
		\item pos example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box} > 0.5$
		\item box in-between are ignored
		\item all label box as pos example, hard neg mining on 5k images \\
		$\Rightarrow$ control data volume, speed up SVM inference
		\end{itemize}
	\item Training Linear Regressor
		\begin{itemize}
		\item input the feature at CNN logits layer, regress location offset
		\item regress $x,y$ directly, $w,h$ in the log space
		\item only regress box nearby:  $\text{IoU}^\text{region proposal}_\text{label box} > 0.6$ \\
		(assigned to label box with max IoU if multiple matched)
		\item heavily regularized (as only performing nearby regression)
		\end{itemize}
	\item Extension in Segmentation
		\begin{itemize}
		\item region proposal switched to that for segmentation
		\item wrapped bounding box on proposed region
		\item get conv features with \& without background in box removed (set to $0$)
		\item output classification for the region
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item first to bridge the gap between classification \& detection
		\item first to show supervised pre-training on auxiliary dataset is effective \\
		$\Rightarrow$ multi-tasking is good
		\item important to distinguish pos-neg examples \\ 
		$\Rightarrow mAP 5\%\uparrow$ when thr$=0.3$, (v.s. thr$=0.5$) \\
		$\Rightarrow$ pos-neg balance important
		\item context in box matters
		\item by ablation study reveals: CNN ability comes from conv layer \\
		$\Rightarrow$ \underline{CNN as feature map extractor} for other domain-specific algorithm
		\item by visualization reveals: CNN is able to learn \underline{distributed representation} \\
		$\Rightarrow$ recognize abstract attributes e.g. shape, texture, color, material etc.
		\item speed legged by region proposal
		\item performance legged by localization (instead of classification, thanks to CNN)
		\item jittered example: box proposal with $\text{IoU}^\text{region proposal}_\text{label box}\in(0.5, 1)$ but is not ground truth \\
		$\Rightarrow$ need more careful fine-tunning to avoid SVM (which is used for hard negative mining) \\
		$\Rightarrow$ problems arise from NOT being end-to-end trained
		\end{itemize}
	\end{itemize}

\item Fast R-CNN
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item based on r-cnn
		\item convolution implementation to predict all proposed regions \\
		(similar to fast sliding window)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item region proposal postponed to a shared feature map \\
		$\Rightarrow$ feature extraction once for all proposal
		\end{itemize}
	\end{itemize}

\item RPN: Faster R-CNN
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item based on fast r-cnn
		\item use CNN for segmentation \& region proposal (faster than selective search)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{One-stage Detection}
\begin{itemize}
\item You Only Look Once (YOLO)
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item grid the input image into disjoint $S \times S$ cells \\
		(apply usually fine grid, e.g. $S=19$)
		\item assign each object to a cell by its central point \\ 
		$\Rightarrow$ try ensuring maximal $\mathbf 1$ object in a grid
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $conf=p(\text{obj}) \times \text{IoU}^\text{label}_\text{pred}$ confidence of current box containing an object \\
		$\Rightarrow 0$ desired if none, IoU between pred and label desired if exist \\
		where, $p(\text{ob}j)$ the probability of object existence in the CELL
		\item $x, y \in [0,1]$ the box center in the cell, normalized by cell size \\
		$\Rightarrow$ easier to learn, as dense layer not confused by varying locations
		\item $h, w \in [0, 1]$ the box size normalized by image size \\
		$\Rightarrow$ easier to learn for dense layer
		\end{itemize}
	\item Classification Encoding
		\begin{itemize}
		\item $p_1,...,p_C$ the conditional probability $p(\text{class}_c|\text{obj})$ for object inside the CELL \\
		i.e. the class probability of object, given object existing \& assigned to the cell
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item for each cell, predict both $[p_1,...,p_C]$ and $B$ bounding boxes $[conf, x, y, w, h]$ \\
		$\Rightarrow$ final prediction as a $S\times S \times (C+5B)$ tensor
		\item $p_c\cdot conf = p(\text{class}_c|\text{obj})\cdot p(\text{obj})\cdot \text{IoU}^\text{label}_\text{pred} = p(\text{class}_c, \text{obj}) \cdot \text{IoU}^\text{label}_\text{pred}$ \\
		where $p_c$ from the cell, $conf$ from each box in the cell \\
		$\Rightarrow$ combine the decouple pred for various class-specific boxes prediction
		\item non-max suppression to fix multiple detection box (at test time)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item $24$ conv layer downsampling from $448\times448\times3$ to $S\times S\times 1024$
		\item followed by: $\rightarrow$dense with dropout$\rightarrow4096\rightarrow$dense$\rightarrow S\times S\times (C+5B)$ \\
		(dropout to prevent co-adaption, i.e. dependent units between 2 dense layers)
		\item leaky ReLu $a=\max(z,0.1z)$ used, except for linear activation in final layer
		\end{itemize}
	\item Optimization Goal
		\begin{itemize}
		\Item \begin{align*} \displaystyle \text{loss} = & \lambda_\text{obj}\sum_{i=0}^{S^2}\sum_{j=0}^{B} \mathbf{1}^{obj}_{ij} [(x_i-\hat x_i)^2 + (y_i-\hat y_i)^2] +\\
		& \lambda_\text{obj}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbf 1^{obj}_{ij} [(\sqrt{w_i}-\sqrt{\hat w_i})^2 + (\sqrt{h_i}-\sqrt{\hat h_i})^2] + \\
		& \sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbf 1^{obj}_{ij} (conf_{ij} - \hat {conf}_{ij})^2 + \lambda_\text{none}\cdot\mathbf 1^{none}_{ij} (conf_{ij} - \hat {conf}_{ij})^2 +\\
		& \sum_{i=0}^{S^2}\mathbf 1^{obj}_{i}\sum_{c=0}^C (p_{ci} - \hat p_{ci})^2 \end{align*}
		\item $\lambda_\text{obj} = 5$ to up-weight box with obj, $\lambda_\text{none}=0.5$ for empty box
			\begin{itemize}
			\item balance the dataset: avoid large num of empty box pushing $conf$ pred to $0$
			\item emphasize on predicting box with object: adjust the box pred $x,y,w,h$
			\end{itemize}
		\item $\mathbf{1}^{obj}_{ij}$ the indicator: $1$ if obj exists in $j$ box of $i$ cell, similar for $\mathbf 1^{none}_{ij}$
		\item minimize $\sqrt{w_i}, \sqrt{h_i}$: mitigate various box size into similar scale \\ 
		(as same error occurred in small box matter more than that in big box)
		\item $p_{ci}$ the classification prob for $i$ cell (only trained if it contains obj)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item optimize the sum-squared error (due to its simplicity)
		\item for each cell: correct its classification if it contains obj
		\item for each box:
			\begin{itemize}
			\item increase $conf$ \& adjust box pred if it overlaps the most with label of its cell
			\item decrease $conf$ for other box (not responsible for having obj)
			\end{itemize}
		\item pre-training: pretrained first 20 conv layers on classification dataset
		\item data augmentation: random scaling size, adjusting exposure and saturation
		\end{itemize}
	\item Limitation
		\begin{itemize}
		\item use only coarse feature to predict box \\ 
		(due to multiple downsampling \& produced by dense layer)
		\item NOT able to handle: multiple objects of the same type in a cell \\
		(NOR when same-type objects more than total pred boxes) \\
		$\Rightarrow$ suffer from small objects in group (e.g. birds)
		\item struggle in box localization
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item bounding box localization \& classification as regression problem \\
		(predicted totally by a dense layer)
		\item end-to-end optimizing directly the detection score \\
		(vs. separate modules: region proposal, bounding box localization, classification)
		\item global information for each bounding box localization \& classification \\
		(instead of taking only local info to classify boxes) \\
		$\Rightarrow$ less false alarm (mistake background as object)
		\item generalizable representation for detection \\
		(as feature extraction embedded in the network) \\
		$\Rightarrow$ able to generalized to art work
		\item much less predicted box (compared with ~2000 boxes in R-CNN framework)
		\item fast $\Rightarrow$ ensemble with (fast) R-CNN framework with no overhead \\
		$\Rightarrow$ mitigate the false alarm in R-CNN framework
		\end{itemize}
	\end{itemize}

\item YOLOv2, YOLO9000 \label{DL_CV_Objdet_YOLOv2}
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item no more explicit grid, implicitly set by downsampling factor \\
		(similar to spatial pyramid pooling)
		\item select $N_A$ anchor boxes: covering most of the interested objects \\
		(e.g. tall-thin box for pedestrian, low-wide box for car)
		\item assign the objects to a tuple (cell, anchor/prior box)
			\begin{itemize}
			\item assign to cell by its central point $\Rightarrow$ try ensuring maximal $N_A$ object in a cell
			\item assign to one from $N_A$ anchor boxes depending on its IoU with all $N_A$ boxes \\
			(NOT assigned, if all $\text{IoU}^\text{\tiny label}_\text{\tiny anchor} < 0.5$)
			\end{itemize}
		\end{itemize}
	\item Anchor Box Selection (Prior)
		\begin{itemize}
		\item run K-means on all label box in train set \\
		with distance $\text{dist}(\text{box}, \text{centroid}) = 1 - \text{IoU}(\text{box}, \text{centroid})$
		\item use cluster centroid as the anchor box \\
		$\Rightarrow$ for anchor box $a\in A, a = [w_a,h_a]$
		\item num of anchor box (centroid) $N_A$ reflect recall $\leftrightarrow$ model complexity trade-off
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $t_x,t_y \in (0,1)$ the box center in the cell, normalized by cell size \\
		$\Rightarrow$ true pred center $x=c_w\sigma(t_x) + c_x, y=c_h\sigma(t_y) + c_y$ where $c_x,c_y,c_w,c_h$ the cell \\
		$\Rightarrow$ easier to learn, as YOLO
		\item $t_w,t_h$ coefficient to adjust anchor box size \\
		$\Rightarrow$ true pred size $w=w_a{\rm e}^{t_w}, h=h_a{\rm e}^{t_h}$ where $w_a,h_w$ the anchor box size \\
		$\Rightarrow$ more robust, avoid influence from anchor prior
		\item $p_1,...,p_C$ classification pred $\Rightarrow$ decouple from cell, each box a classification
		\end{itemize}
	\item Classification with Hierarchical Encoding
		\begin{itemize}
		\item tree-structure for multi-label: \hyperref[DL_Layers_Hisoftmax]{a hierarchical softmax} \\ 
		(hence, NOT assuming mutual exclusion between class)
		\item lookup WordNet for class label \& its path to the selected root "physical object" \\
		(if multi-path, choose the one with least new word involved) \\
		$\Rightarrow$ build a WordTree
		\item multiple softmax: one for each set of direct children of a node (if not leaf) \\
		e.g. at node "dog": predict $p(\text{malamute dog}|\text{dog}), p(\text{terrier dog}|\text{dog}), ...$ \\
		$\Rightarrow$ pred a Bayesian network, according to WordTree
		\item $\Rightarrow t_o$ objectiveness by summing conditional prob accordingly \\
		$\Rightarrow$ true objectiveness: $conf = p(\text{obj})\cdot \text{IoU}^\text{label}_\text{pred} = \sigma (t_o)$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item for each cell, $N_A$ bounding boxes: $[\text{spatial info}[t_x, t_y, t_w, t_h], \text{ a flatten WordTree}[p_1,...,p_C]]$ \\
		$\Rightarrow$ final prediction as a $\frac I{32} \times \frac I{32} \times N_A(4+C)$ tensor (each box a WordTree!)
		\item objectiveness of each box: from WordTree of each box
		\item box classification: choose highest confidence path at every node \\ 
		(till a specified threshold or a leaf node reached)
		\item non-max suppression to fix multiple detection box (at test time)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item fully conv net: downsample by $32$, directly predict $\frac I{32}\times \frac I{32}\times (C+4N_A)$ tensor
		\item \hyperref[DL_Block_Res_Passthrough]{passthrough layer} with concat ($1\%\uparrow$) \\
		e.g. spatially stack $26\times26\times512$ into $13\times13\times1024$ before concat
		\item batch norm in ALL conv layer ($>2\%$ mAP$\uparrow$)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item pre-training on classification with data augmentation \\
		(e.g. random crop, rotation, hue, saturation and shifting shift)
		\item more time for model fine-tuning on detection dataset ($\sim4\%$ mAP$\uparrow$) \\ 
		as model takes time to adjust to resolution change
		\item multi-scale training s.t. model able to cope with varying resolution
		\item joint training: mix detection \& classification dataset ($\sim5\%\uparrow$) \\
		$\Rightarrow$ for detection label, full loss available \\
		$\Rightarrow$ for classification: only classification \& objectiveness loss considered
			\begin{itemize}
			\item select responsible pred box by the largest objectiveness
			\item objectiveness loss: considered if $t_o$ of responsible box $<$ threshold ($0.3$)
			\item classification loss: only nodes in its WordTree above current label considered \\
			with $p(\text{physical object})=1$ for label
			\end{itemize}
		\item balancing joint dataset: oversampling detection dataset \\
		$\Rightarrow$ s.t. detection data : classification data = 1:4 
		\end{itemize}
	\item Limitation
		\begin{itemize}
		\item each box a classification: too much output channel
		\item still, detectable objects number restricted by anchor box num in a cell \\
		(e.g. group of small objects)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item still, fast \& global context for box detection
		\item auto-select prior, better than handpicked \\
		(examined by using anchor box directly as prediction)
		\item adjustable speed$\leftrightarrow$accuracy trade-off via input resolution \\
		(enabled by being fully conv \& multi-scale trained) \\
		$\Rightarrow$ larger resolution, slower, more accurate
		\item joint training with WordTree: multi-task (detection + classification) learning \\
		$\Rightarrow$ enrich classes to detect \& more robust in class-detection \\
		$\Rightarrow$ graceful degrade on detecting new/unknown data
		\item decoupled localization \& classification \\
		$\Rightarrow$ able to handle some multiple same-type objects in same cell
		\end{itemize}
	\end{itemize}

\item YOLOv3
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item same as YOLOv2
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $t_x,t_y,t_w,t_h$: same as YOLOv2
		\item $t_o$ the objectiveness, modeling directly $p(\text{obj})$, instead of $p(\text{obj})\cdot\text{IoU}$
		\item $p_1,...,p_C$ the classification of obj: conditional prob $p(\text{class}_c|\text{obj})$, as YOLO
		\end{itemize}
	\item Classification Encoding
		\begin{itemize}
		\item independent logistic regression for each class (instead of hierarchical softmax) \\ 
		$\Rightarrow$ multiple binary classification (NOT assuming mutual exclusive classes) \\
		$\Rightarrow$ multi-label approach (box may contain more than one class)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item for each cell, $N_A$ bounding boxes $[t_x, t_y, t_w, t_h, t_o, p_1,...,p_C]$ \\
		($N_A$ the num of anchor box)
		\item $\Rightarrow$ final prediction as a $\frac I{N^l} \times \frac I{N^l} \times N_A(C+5)$ tensor \\
		where $N^l$ the size of downsampled feature map (at pred layer $l$)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item residual blocks \& batch norm as YOLOv2
		\item 2D encoder-decoder structure with concat connection, with $3$ pred branches \\
		$\Rightarrow$ final num of pred boxes $\displaystyle\sum_{l\in\text{pred layer}}(N^l\times N^l\times N_A)$
		\begin{figure}[ht]
		\includegraphics[width=0.9\linewidth, right]{"./Deep Learning/plot/topic-objdet onestage yolov3 architecture".png}
		\end{figure}
		\end{itemize}
	\item Optimization
		\begin{itemize}
		\item sum of the binary cross-entropy for classification loss
		\end{itemize}
	\item Training
		\begin{itemize}
		\item pretrain backbone (feature extractor) on classification
		\item multi-scale training \& data augmentation as YOLOv2
		\end{itemize}
	\item Failed Approach
		\begin{itemize}
		\item box $x,y$ as an offset to anchor box position $x_a,y_a$ (stability decreased)
		\item linear regression for $x,y$ (worse than bounded by logistic)
		\item focal loss (already solved by decoupled $p(\text{obj})$ \& $p(\text{class}_c|\text{obj})$)
		\item dual IoU threshold in assigning object (need more tunning for stabilized model)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item real-time applicable detector
		\item detection at multiple scale, with encoder-decoder structure
		\item decent localization + great box classification \\
		$\Rightarrow$ emphasize on box classification, since human insensible to IoU change
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Object Tracking}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Raw Data
		\begin{itemize}
		\item time-series observation from sensor $\Rightarrow$ need to embed detection
		\end{itemize}
	\item Object Representation
		\begin{itemize}
		\item bounding box: 2/3-D size, with pose, motion, velocity, etc.
		\item point model: centroid point with attributes denoting an object (including bbox)
		\item silhouette: for non-rigid object
		\item articulate: for articulated model, e.g. human skeletal model
		\end{itemize}
	\item Object Existence
		\begin{itemize}
		\item probability density for existence
		\item probability of existence as a feature in representation info \\
		(objectiveness in detection)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Trajectory
		\begin{itemize}
		\item recover the true trajectory of the object \\ 
		(including current location, potentially future prediction)
		\end{itemize}
	\end{itemize}
\item Overview
	\begin{itemize}
	\item Online Learning (Tracking)
		\begin{itemize}
		\item trade off speed - model complexity, as train on arriving frames
		\item adaptive, as accounting the history info of a track \\
		$\Rightarrow$ may provide more info (e.g. covariance matrix from Kalman filter)
		\end{itemize}
	\item Offline Tracking
		\begin{itemize}
		\item learn similarity functions between frames offline
		\item fast, as no online training needed $\Rightarrow$ but NO explicit adaptive
		\item batch method: generate the track after all frames examined
		\end{itemize}
	\item Tracking-by-Detection
		\begin{itemize}
		\item detect target(s) in each frame; link target into track \\
		$\Rightarrow$ as a two-stage problem
		\item $\Rightarrow$ viewed as meta learning / one-shot learning \\
		(train on first frame, detect on subsequent frames)
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Appearance Model
		\begin{itemize}
		\item robust target-specific model
		\item shape\&color can be trap: changing clothes etc.
		$\Rightarrow$ learning directly transformation by image pair (e.g. siamRPN)
		\end{itemize}
	\item Long Term Dependency
		\begin{itemize}
		\item re-identification: people leaving and re-entering the scene
		\item hot standby surveillance
		\end{itemize}
	\item Initialization \& Termination Criteria
		\begin{itemize}
		\item bad initialization: overlapped box due to overlapped object \\
		(e.g. two closely standing people)
		\item termination vs. occlusion \\
		$\Rightarrow$ need to tell leaving the view or temporally occluded
		\end{itemize}
	\item Fast Motion
		\begin{itemize}
		\item large search area, hence more background noise, hence similarity can fail
		\item motion blur (which can be modeled as appearance change)
		\end{itemize}
	\end{itemize}
\item Evaluation Metrics
	\begin{itemize}
	\item Success
		\begin{itemize}
		\item overlap success rate: average pred-label box IoU
		\item overlap success plot: the rate of pred-label box IoU $\ge$ a given threshold \\
		$\Rightarrow$ can have AUC
		\item orientation success: diff of pred-label yaw angle $\le$ threshold $=\ang{10}$
		\item other success... \\
		$\Rightarrow$ review the overall estimated quality
		\end{itemize}
	\item Normalized Cumulative Sum of Success vs Normalized Time
		\begin{itemize}
		\item a plot $\Rightarrow$ review tracking quality over time
		\end{itemize}
	\item Trajectory Difference
		\begin{itemize}
		\item compare the similarity of pred-label trajectory \\
		(may consider abrupt change, slowly drift, and etc.)
		\end{itemize}
	\item VOT Metrics
		\begin{itemize}
		\item robustness: failure times (frame cnt of lost track)
		\item accuracy: average overlap while tracking successfully
		\item expected average overlap (EAO): accounts for both accuracy \& robustness
		\end{itemize}
	\end{itemize}
\item Dataset
	\begin{itemize}
	\item Video (Single) Object Tracking
		\begin{itemize}
		\item Youtube-BB
			\begin{itemize}
			\item $>100,000$ videos, annotated once every $30$ frames
			\end{itemize}
		\item ILSVRC (ImageNet) VID
			\begin{itemize}
			\item $\sim4000$ videos annotated frame-by-frame
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item State-of-the-Art
	\begin{itemize}
	\item Data Balance
		\begin{itemize}
		\item avoid simple negative (foreground-background in siamese tracker) \\
		$\Rightarrow$ use negative box with semantic instance (discussed in DaSiam) \\
		$\Rightarrow$ cascaded RPN to filter out hard negative
		\item triplet loss: mine the relation of (template, positive instance, negative instance) \\
		$\Rightarrow$ focus on making correction decision on hard neg
		\item scale balance: having enough detail for semantic similar distractor \\
		cascaded RPN: feature-transform-block for multi-scale feature \\
		siamgrpn++: multi-level feature (enough diversity from deepnet) \\
		(yet, in siamRPN, model overfits to object scale)
		\end{itemize}
	\item Deep Power
		\begin{itemize}
		\item revealed to be central bias in deepnet, due to padding \\
		(network can realize input patch location by if feature is padding influenced) \\
		(padding on template prevent xcorr from effectively measuring similarity) \\
		$\Rightarrow$ siamrpn++: remove location bias in training (data augmentation) \\
		$\Rightarrow$ siamDW: new network architecture (crop inside the net)
		\item worse localization due to large receptive field \& accumulated stride
		\end{itemize}
	\item $\Rightarrow$ Discriminative Ability
		\begin{itemize}
		\item deep feature via deepnet
		\item mining pos-neg for better discriminative ability
		\item use background info in tracking stage, instead of only in training phase \\
		(introducing optimization approach?)
		\end{itemize}
	\item Finer Detail
		\begin{itemize}
		\item instance tracking via multi-tasking with video segmentation
		\item $\Rightarrow$ attention based cropping?
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Detection in Tracking}
\begin{itemize}
\item Single-frame Detection
	\begin{itemize}
	\item \hyperref[DL_CV_Objdet]{Bounding Box Detection}
		\begin{itemize}
		\item YOLO, R-CNN, etc.
		\end{itemize}
	\item Point Detection
		\begin{itemize}
		\item detect landmarks
		\end{itemize}
	\item Background Modeling
		\begin{itemize}
		\item segmentation, ...
		\end{itemize}
	\end{itemize}
\item Temporal Detection
	\begin{itemize}
	\item Optic Flow
		\begin{itemize}
		\item able to represent non-rigid, deformable object
		\item yet, may failed in moving foreground (e.g. birds, fog, smoke...)
		\end{itemize}
	\item Motion Detection
	\item Orientation
		\begin{itemize}
		\item frame differencing of location, ...
		\end{itemize}
	\item Background Modeling
		\begin{itemize}
		\item adaptive background, ...
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Single-Object Tracking}
\begin{itemize}
\item Kalman Filter
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item predict covariance $\Rightarrow$ better gating in association
		\item cooperate with noise \\
		$\Rightarrow$ able to modified to account association uncertainty
		\end{itemize}
	\end{itemize}

\item Particle Filter

\item Correlation Filter
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item online-training to learn the object appearance at previous frame \\
		$\Rightarrow$ conv over test (current) frame for similarity response
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item no explicitly modeled, though can augment online-training data with past images
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item $\mathbf w$ the weight to learn $\mathbf x$ input image of current frame, $y$ the desire response
		\item $\mathbf x[\tau]$ the circular shift of $\mathbf x$ by $\tau=[\tau_x, \tau_y]$ pixel
		\item formulated as regression problem \\ 
		$\Rightarrow \displaystyle \arg_{\mathbf w}\min\frac 1 2\sum_{n\in N}\norm{\mathbf x[\tau_n]^T\star \mathbf w - y_{\tau_n}}^2 + \frac \lambda 2 \norm{ \mathbf w}^2$ \\
		(mosse: minimum output sum of squared error, with regularization)
		\item original solution for regression $(X^T X + \lambda I)^{-1}$, where $X=\{\mathbf x[\tau_n]\mid n = 1,...,N\}$ \\
		(corresponding $\mathbf y = \{y_{\tau_n}\mid n =1,...,N\}$) \\
		$\Rightarrow \mathcal O(N^3)$, where $N$ the num of training example in $X$
		\item since $\mathbf x[\tau_n]$ a circulant toeplitz matrix $\Rightarrow$ fourier transform into diagonal matrix
		\end{itemize}
	\item 
	\end{itemize}

\item GOTURN: 
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item CNN
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item no explicit tracker (not modeling temporal info)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop $t^\text{th}$ image: centered at current bounding box with context
		\item crop $t+1^\text{th}$ image: centered at bbox in $t$, doubled size (gating)
		\item encode both image via CNN detector; concat encoding \& feed to dense layer
		\item regress the bbox in $t+1^\text{th}$ crop
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item arbitrary target object selected for tracking (treated as ground truth)
		\item $1$-step prediction of location, then predict further on...
		\end{itemize}
	\item Training
		\begin{itemize}
		\item train with consecutive image pair froxm sequences
		\item $L1$ loss for exact match, as it does NOT smooth out in $(0^-, 0^+)$ as $L2$ loss \\ 
		$\Rightarrow$ mitigate the introduced motion noise
		\item data augment: motion noise applied on the $2^\text{th}$ image; random crop on the $1^\text{st}$\\ 
		(motion noise $\sim$ Laplace distribution and prefer smooth \& small motion)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $2$-frame model as a simple local generic detector, no explicit temporal tracking \\
		$\Rightarrow$ learn to locate the nearest similar object by comparing $t-1,t$ frame \\
		$\Rightarrow$ learn to discover relationship between object appearance \\ 
		(by the Siamese setting) \\
		$\Rightarrow$ refine bbox for nearest similar object with one proposal ($t-1$ bbox)
		\item $\Rightarrow$ able to specialize tracker for specific object tracking \\
		e.g. fine-tune on car video for car tracker
		\item can NOT possess intrinsic invariance of object movement at various direction \\
		$\Rightarrow$ detector needs to be trained by data augmentation for object at all positions \\
		(due to dense layer)
		\item can NOT handle large drastic target change: preferring local \& smooth change 
		\item Not good at exact frozen scene: training augmented with noisy motion \\
		(could be worse if NOT using $L1$ loss)
		\item real-time timing ($100$fps), included in opencv $\Rightarrow$ stable performance
		\end{itemize}
	\end{itemize}

\item SiamFC: Fully-Conv Siamese Network for Object Tracking
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item siamese CNN to extract feature from exemplar \& search image
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item no explicit tracker, yet cross-correlation to measure spatial similarity
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item exemplar and search image cropped with target in the center \& feature extracted into $z, x$ \\
		(at most $T$ frames apart)
		\item conv exemplar $z$ on search image $x$ for response map of similarity
		\item maximum score in response map as object in cur frame \& fit a box accordingly
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item AlexNet as CNN
		\end{itemize}
	\item Training
		\begin{itemize}
		\item logistic loss for final response map from final cross-correlation \\
		(cosine window applied to penalize large displacements)
		\item weighted for pos-neg balance
		\item bbox encoded as a mask with a square area of $1s$
		\item image cropped with padding \& scaled to $127\times127$ \& $255\times255$ for convenience
		\item $50$ epochs ($50000$ examples per epoch), batch size $8$
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item crop with padding on box of $1^\text{st}$ frame, crop with $t-1^\text{th}$ box padded to $4\times$ size on frame $t$
		\item tracking with scale: processing multi-scaled version of search image ($5$ scale $1.025^{\{-2,-1,0,1,2\}}$)
		\item produce response map of similarity \& upsampled to $272\times272$ for better localization
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $2$-frame model: measuring spatial similarity by correlation for localization \\
		$\Rightarrow$ an unrolled RNN trained with length $2$ \\ 
		$\Rightarrow$ strong initialization for RNN-based tracker
		\item learning strong offline embedding for exemplar-search similarity measurement \\
		$\Rightarrow$ complementary to online tracker
		\item updating exemplar $z$ in time series NOT gaining much (as initial box assumed to be a great one)
		\end{itemize}
	\end{itemize}

\item Re$^3$: Real-time Recurrent Regression Network
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item CNN to extracts multi-scale representation from image \\
		(more descriptive info e.g. human in red/blue shirt) \\
		$\Rightarrow$ a siamese net as encoding
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item two-layer, factored LSTM, taking tracker input at both layer \\
		$\Rightarrow$ longer dependency \& more complex object transformation with 2 layers
		\item hidden state as tracker state \\
		$\Rightarrow$ forward-prop to update (no training)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop $t^\text{th}$ image: centered at current bounding box, extended to twice of box size
		\item crop $t+1^\text{th}$ image: centered at box of $t$, doubled size (gating)
		\item late fusion: concat CNN output from $t, t+1$ \& fed into dense layer for fusion
		\item LSTM tracker updates on fused info
		\item dense layer regress the tracker (LSTM) output for bounding box at $t+1$
		\end{itemize}
	\item Training
		\begin{itemize}
		\item bounding box defined by up-left, right-bottom coordinates in the crop \\
		(hence as a ratio of bounding box size)
		\item $L1$ loss to encourage exact match
		\item short sequence ($2$ unrolls) \& multi-batch ($64$) to overcome plateaus \\
		$\Rightarrow$ slowly increase to $32$ unrolls \& batch size $4$
		\item use ground-truth crop when training with short sequence \\ 
		$\Rightarrow$ slowly increase probability to use predicted crop, when increasing unrolls \\
		$\Rightarrow$ to prevent from accumulating drifts
		\end{itemize}
	\item Data Augmentation
		\begin{itemize}
		\item utilize detection dataset: crop the object with bounding box
		\item random crop a patch from the same image as background
		\item occluders randomly taken from the same image
		\item object with box initialized with velocity with Gaussian noise
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item an initial bounding box over arbitrary object given at the start
		\item crops fed into net at each time step
		\item LSTM state reset after each $32$ frames (as maximally trained with $32$ unrolls) \\
		$\Rightarrow$ hot-start by using the state in $1^\text{st}$ forward pass (instead of $\mathbf 0$) \\
		$\Rightarrow$ preserve the initial encoding \& recover from drift
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item end-to-end training for both detector and tracker
		\item model regress changes to the box ratio $\Rightarrow$ easier as bbox refinement
		\item LSTM tracker maintain track state $\Rightarrow$ temporal fusion \\
		$\Rightarrow$ overcome occlusion, update for variance/shape change \\
		(observation usefulness modeled by LSTM tracker)
		\item LSTM tracker needs specialized training, and single-layer LSTM NOT enough \\
		(or, can hurt performance due to instability)
		\item LSTM state reset: prevent drift, yet can fail if initial box overlaps other object
		\item may still, drifted to similar nearby object, known object (e.g. face), large motion \\
		(comparison \& failure: \url{https://youtu.be/RByCiOLlxug?t=214})
		\end{itemize}
	\end{itemize}

\item CFNet: Correlation Filter based Tracking (SiamFC v2)
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item siamese CNN 
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item correlation filter embedded in forward prop $\Rightarrow$ online-learning $1$-step tracker
		\end{itemize}
	\item Correlation Filter Block
		\begin{itemize}
		\item input training img $x\in \mathbb R^{m\times m}$, test img $z$
		\item $w\star x$ construct 
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop $t$ frame at box center of $t-1$ frame, with $4\times$ larger \& siamese CNN extracts feature from \\
		$\Rightarrow$ extracted last frame feature (train) \& cur frame feature (test) into $x, z$
		\item solving correlation filter optimization: get circular shift of $x$ by circular linear matrix $k$ \\
		$\Rightarrow$ via Lagrangian dual, turn optimization into linear equations system \\
		$\Rightarrow$ solve for equations system \& construct back prop map \\
		(can even learn the desired response $y$ instead of assuming it to be Gaussian)
		\item crop CF output \& convolve with test image $z$ to regress box
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item 
		\end{itemize}
	\item Training
	\item Understanding
		\begin{itemize}
		\item correlation filter embedded in forward prop \\ 
		$\Rightarrow$ utilize online-learning to acquire prior for target (while tracking generic object)
		\item enable ultra-lightweight network comparative with deep net \\
		(though correlation filter layer does NOT improve the ceiling performance)
		\item updating template on each frame (v.s. using always $1^\text{st}$ frame)
		\end{itemize}
	\end{itemize}

\item \textbf{SiamRPN: Tracking with Siamese Region Proposal Network}
	\begin{itemize}
	\item Tracking Initialization
		\begin{itemize}
		\item take initial frame \& given bbox as template $z$
		\item CNN (modified AlexNet) obtains a feature map $\varphi(z)$ \\
		(crop $(w+p)\times (h+p)$ with padding $p=\frac {w+h}2$ \& resized to $227\times227$)
		\end{itemize}
	\item Detector
		\begin{itemize}
		\item in $t>1$ frames, each frame as detection frame $x$
		\item same CNN (modified AlexNet) extracts feature maps into $\varphi(x)$\\
		(crop $2(w+p)\times2(h+p)$ \& resized to $255\times255$)
		\item $2$ independent \& identical branch in RPN: classification/regression branch \\
		$\Rightarrow$ conv to get $\zeta[\varphi(z)], \zeta[\varphi(x)]$ \& conv with $\zeta[\varphi(z)]$ as kernel over $\zeta[\varphi(x)]$ \\
		(with different output branch)
		\item use kernel from template branch to convolute on feature map of detection branch \\
		(same channel num) \\
		$\Rightarrow$ regress bbox based on response map
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item tracking as detection by similarity, no explicit tracker
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item anchor box $x^\text{an},y^\text{an},w^\text{an},h^\text{an}$, ground truth box $x^\text{gt},y^\text{gt},w^\text{gt},h^\text{gt}$
		\item perfect prediction as $\delta x = \frac {x^\text{gt}-x^\text{an}}{w^\text{an}}, \delta y = \frac{y^\text{gt}-y^\text{an}}{y^\text{an}}, \delta w = \ln \frac {w^\text{gt}}{w^\text{an}}, \delta h = \frac{h^\text{gt}}{h^\text{an}}$ \\
		$\Rightarrow$ predict box as $x^\text{p}=\delta xw^\text{an} + x^\text{an}, y^\text{p}=\delta yh^\text{an} +y^\text{an}, w^\text{p}=e^{\delta w}w^\text{an}, h^\text{p}=e^{\delta h}h^\text{an}$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item crop on template \& search img \& extract feature volume
		\item take template volume as kernel, convolute on feature map of current frame \\
		$\Rightarrow$ RPN takes resulted response map as proposal \& regress bboxes
		\item select top $K$ response in classification branch as set of $\{(i_k,j_k,c_k)\mid k\in(0,K)\}$ \\
		$\Rightarrow$ use the location $\{(i_k,j_k,c_k)\}$ to obtain corresponding anchor \& pred \\
		$\Rightarrow$ generate $K$ proposal as $\{x_k^\text{pro}, y_k^\text{pro}, w_k^\text{pro}, h_k^\text{pro}\}$
		\item box refinement: only central $g\times g$ anchors remain considered \\ 
		(use the center size of search region)
		\item size change penalty $\displaystyle e^{k*\max(\frac {r_t}{r_{t-1}}, \frac {r_{t-1}}{r_{t}})*\max(\frac {s_t}{s_{t-1}}, \frac {s_t}{s_{t-1}})}$, \\
		where $r_t$ the bbox $\frac w h$ ratio of frame $t$, $s^2=(w+p)(h+p)$ with padding $p=\frac {w+h} 2$ \\
		$\Rightarrow$ bbox score (objectness/classification response) multiplied by penalty
		\item non-max suppression on re-ranked boxes for final bbox \\ 
		(target size updated by linear interpolation for smooth translation)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item arbitrary target object selected for tracking (treated as ground truth) \\
		$\Rightarrow$ extract the feature template $\varphi(z) \& \zeta[\varphi(z)]$
		\item each $t>1$ frame, perform feature extraction, detection with kernel $\zeta[\varphi()z]$ \& bbox regression \\
		$\Rightarrow$ formulate as task of a one-shot learning for detection
		\item gating on frame $t$ based on $t-1$ result
		\item detect target within gated region of frame $t$ \\
		(the best remained \& adjusted anchor as the target to track)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item crop input by $\sqrt{(w+p)*(h+p)}$, where padding $p=\frac{w+h}2$ \& then resize to fixed size \\
		$\Rightarrow$ trofeat box as square \& has always the same percentage  target in the input \\
		(crop based on each label box: NOT using previous label box)
		\item cross-entropy loss for classification \& smooth $L1$ loss for regression
		\item pos-neg example: dual IoU threshold $(0,0.3,0.6)$
		\item batch: size $64$, with at most $16$ positive examples
		\item fine-tune only last $2$ conv layers in AlexNet
		\item dataset: image pair from imagenet VID \& youtube-BB
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $2$-frame model: tracking as one-shot detection (with response map of similarity) \\
		$\Rightarrow 1^\text{st}$ bbox as only examplar to learn the conv kernel in RPN \\
		(which is used in following frames for bbox proposal) \\
		$\Rightarrow$ Siamese net learns to efficiently map $z$ to weights $[\varphi (z)]$ (learning to learn) \\
		$\Rightarrow$ has learned transformation of target, hence update template has mere improvement
		\item template branch to extract feature discriminate fore-/back-ground \\
		$\Rightarrow$ predict detection kernel by input $z$ and its weight in $[\varphi(\cdot)]$ as a meta learner
		\item detection branch (RPN) to propose \& refine a compact bbox (instead of online fine-tuning) \\ 
		$\Rightarrow$ no need for online learning, nor multi-scale test
		\item crop the kernel-branch: crop out edge case to avoid noise \\
		$\Rightarrow$ NOT able to handle large search region \\ 
		(use only limited center size in refinement)
		\item yet, using both VID \& youtube-bb dataset \\
		$\Rightarrow$ youtube-bb contain larger transform between frames \\
		(due to one labeled frame in every 30 frames) \\
		$\Rightarrow$ needs larger center-size \& context to overcome large transformation
		\item tracking based on \underline{similarity \& transformation \& semantic}
		\item similarity: from correlation
		\item transformation: from using $z$-$x$ pair from a large frame range ($\sim100$)
		(as encoding bbox through crop \& both template-search img are cropped by label box) \\
		(instead of box from previous frame as in Re$^3$) \\
		$\Rightarrow$ learning appearance transformation, instead of object motion
		\item semantic: from using detection net (RPN) \\
		$\Rightarrow$ anchor for obejct different $w$-$h$ ratio \\
		\item fail in large \& fast moving object, potentail reason: \\
		large search area contains too much noise \& similarity get more response
		\item fail with different image crop: training with only a specific crop strategy \\
		(model trained with a fix 255x255 crop, with a fixed ratio of region being target object) \\
		$\Rightarrow$ overfit the chosen scale \& NOT able to handle different scale \\
		$\Rightarrow$ not fully explore the discriminative ability from the semantic
		\item may fall back to simple detector if backbone can not extract meaningful template \\
		(potential reason: not fully trained, training set do not have hard negative, ...,) \\ 
		by meaningful: discriminative enough for two similar object - need to able to recognize tiny feature \\
		(similar to requirement in re-ID)
		\item offline transformation + semantic learning to take the place of online learning \\
		$\Rightarrow$ NOT using background info of each video \\
		(use only foreground info via similarity)
		\end{itemize}
	\end{itemize}

\item DaSiamRPN
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item using negative pair with semantic object $\Rightarrow$ more discriminative \\
		$\Rightarrow$ model realize difference between similar object \\
		(instead of only foreground-background)
		\item $\Rightarrow$ hence meaningful score \\ 
		$\Rightarrow$ able to use score to denote lost-recovery in noisy \& big search region \\
		(otherwise, may pick up random object to track with high score)
		\end{itemize}
	\end{itemize}

\item C-RPN: Siamese Cascaded Region Proposal Network
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item modified AlexNet for encoding (multi-scale info utilized) \\
		$\Rightarrow$ extracts info from search region $x$ with gating \\ 
		(target template $z$ already prepared) \\
		$\Rightarrow \phi_n(\cdot)$ for features at $n^\text{th}$ layer, backwards
		\item feature fusion $\Phi_l(\cdot) = f \left( \Phi_{l-1}, \phi_l \right)$, with $\Phi_1 = \phi_1$ \\
		$\Rightarrow$ recursively fuse semantic info with lower-level spatial infoz
		\item $f(\Phi_{l-1}, \phi_l)$: feature transfer block for fusion \\
		$\Rightarrow$ upsample $\Phi_{l-1}$ by deconv, further 2 convs on $\phi_l$ for channel matching \\
		$\Rightarrow$ element-wise sum, then interpolation as downsampling
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item 2-frame siamese net as tracker
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item same CNN extracts features from search region
		\item $l^\text{th}$ RPN convolute $\Phi_{l}(z)$ on $\Phi_{l}(x)$ \\
		$\Rightarrow$ regresses boxes based on response map \& anchor boxes $A_{l}$
		\item discard any boxes $\in A_l$ with confidence/objectness lower than a \underline{preset} threshold \\
		$\Rightarrow$ produce anchor boxes $A_{l+1}$
		\item fuse semantic info with lower-level info, for both branch for $x, z$
		\item $l+1^\text{th}$ RPN further refine anchors $A_{l+1}$
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item loss for $l^\text{th}$ RPN $\displaystyle L_l = \sum_{a\in A_{l}}L_\text{cls}(c^l_a, \hat c^l_a) + \lambda \sum_{a\in A_l} \hat c^l_a \cdot L_\text{loc}(r_a^l, \hat r_a^l)$, \\
		where $c^l_a$/$\hat c^l_a$ the predict/label objectness for anchor $a$; \\
		with  $r_a^l$/$\hat r_a^l$ the predict/label location for anchor $a$ (encoded as YOLOv2) \\
		note: label based on current anchor $a\in A_l$ and ground truth $\hat r_a$
		\item $\Rightarrow$ total loss $\displaystyle L = \sum_{l} L_l$
		\end{itemize}	
	\item Training
		\begin{itemize}
		\item random sample image from a video \\
		$\Rightarrow$ forming image pair (target template image, image with/without target)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multi-stage tracking: each RPN sequentially refine bbox (size \& location)
		\item hard negative mining by filtering out box proposal at each RPN stage \\
		$\Rightarrow$ training samples sequentially more balanced \\
		$\Rightarrow$ RPNs sequentially more discriminative
		\item $\Rightarrow$ hence, more discriminative between similar nearby object \\
		(compare with Re$^3$ \& SiamRPN)
		\item fusion of multi-level feature (spatial + semantic info) for RPN \\
		$\Rightarrow$ provide detail for semantic similar distractor
		\end{itemize}
	\end{itemize}

\item Triplet Loss in Siamese Tracker
	\begin{itemize}
	\item Tracker
		\begin{itemize}
		\item two-frame siamese net as tracker
		\item inference as siamese tracker: init \& track
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item triplet: exemplar (template) $z$, positive instance $x_i$, negative instance $x_j$,
		where positive/negative instance the search image with/without target
		\item matching prob: $\displaystyle prob(vp_i, vn_j)=\frac{e^{vp_i}}{e^{vp_i} + e^{vn_j}}$, \\
		where $vp_i / vn_j$ the predicted prob of $x_i/x_j$ to be pos-/neg-ative \\
		$\Rightarrow$ a softmax over prediction on pos-neg instance (with different search img)
		\item joint prob: $\displaystyle \frac {1}{MN} \prod_{i}^M\prod_j^N prob(vp_i, vn_j)$, where \\
		$M/N$ the number of instance in the positive/negative set
		\begin{align*} \Rightarrow \text{triplet loss } \displaystyle L_t &= - \frac 1 {MN} \sum_{i}^M\sum_{j}^N\log prob(vp_i, vn_j) \\
		&= \frac 1 {MN} \sum_{i}^M\sum_{j}^N \log(1+e^{vn_j-vp_i}) 
		\end{align*}
		\end{itemize}
	\item Training
		\begin{itemize}
		\item train another $10$ epoch with the triplet loss after the original training
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item scale the normal logistic loss (used due to binary classification) accordingly \\
		\begin{align*}
		\Rightarrow \displaystyle L_l &= \sum_i^M \frac 1 {2M}\log(1+e^{-vp_i}) + \sum_{j}^N \frac 1 {2N}\log(1+vn_j) \\
		&= \frac 1 {MN} \sum_i^M\sum_j^N \frac 1 2 (\log(1+e^{-vp_i}) + \log(1+e^{vn_j}))
		\end{align*}
		i.e. repeat $N$ times for $M$ positive instance; $M$ times for $N$ negative instance
		\item thus, logistic v.s. triplet lies in their term inside sum \\ 
		i.e. $\underbrace{\frac 1 2 (\log(1+e^{-vp_i}) + \log(1+e^{vn_j}))}_{T_l} \text{ v.s. } \underbrace{\log(1+e^{vn_j-vp_i})}_{T_t}$
		\item for logistic: $\frac {\partial T_l}{\partial vp} = \frac {-1}{2(1+e^{vp})}, \frac {\partial T_l}{\partial vn} = \frac {1}{2(1+e^{-vn})}$
		\item for triplet: $\frac {\partial T_t}{\partial vp} = \frac {-1}{1+e^{vp-vn}}, \frac {\partial T_t}{\partial vn} = \frac {1}{1+e^{vp-vn}}$
		\item $\Rightarrow$ gradient value: \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-obj track triplet loss vs logistic loss".png} \\
		$\Rightarrow$ triplet loss offers larger absolute gradient when $vp < vn$ (up-left triangular) \\
		$\Rightarrow$ logistic loss: focus on making $vp$ larger \& $vn$ smaller, separately \\
		($\frac {\partial T_l}{\partial vp} \rightarrow 0$ when $vp \rightarrow +\infty$, similar for $vn$) \\
		$\Rightarrow$ while, triplet loss: focus on having $vp > vn$ (to ensure the correct decision)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item mine the relationship between exemplar-positive-negative \\
		$\Rightarrow$ larger absolute gradient under wrong classification (predict $vp \le vn$) \\
		$\Rightarrow$ focus on correct prediction, instead of only high score
		\item form more training examples: num of examples increases from $M+N$ to $MN$ \\
		$\Rightarrow$ more importantly, form a more diverse dataset
		\item all model trained with triplet loss outperform its original logistic loss version \\
		(compared with training extra $10$ epoch with logistic loss) \\
		(though not outperforming in every scenario, e.g. low resolution)
		\end{itemize}
	\end{itemize}

\item Learning Discriminative Model Prediction for Tracking
	\begin{itemize}
	\item 
	\end{itemize}

\item SiamRPN++
	\begin{itemize}
	\item Detector
		\begin{itemize}
		\item deep backbone $\Rightarrow$ multi-level feature with enough detail-semantic diversity
		\item depth-wise separable correlation: xcorr on each corresponding channel-channel \\
		(similar to depthwise separable conv: 1x1 conv to replace summation) \\
		$\Rightarrow$ each semantic info measure similarity separately (more semantic) \\
		$\Rightarrow$ meanwhile, reduce parameters and thus faster convergence
		\item neck: conv-bn block to adjust \& align channel before xcorr \\ 
		(separate neck for classification - regression)		
		\item multi-RPN: corresponding to multi-level feature \\
		$\Rightarrow$ depthwise xcorr applied on each level feature \& each followed by an RPN \\
		$\Rightarrow$ (trainable weight) weighted average directly on RPN result
		\end{itemize}
	\item Tracker
		\begin{itemize}
		\item siamese RPN net tracker, with deeper backbone (e.g. resnet, rather than alexnet)
		\end{itemize}
	\item Analysis
		\begin{itemize}
		\item siamese tracker: correlation to measure similarity \\
		$\Rightarrow f(\mathbf z, \mathbf x) = \phi(\mathbf z) * \phi(\mathbf x) + b$, where $\phi(\cdot)$ map into embedded space, $b$ the bias
		\item $\Rightarrow$ need to have strict translation INvariance: $f(\mathbf z, \mathbf x[\Delta\tau_j]) = f(\mathbf z, \mathbf x)[\Delta\tau_j]$, where \\ $[\Delta\tau_j]$ the translation, i.e. shifting sub-window \\
		(i.e. conv kernel canNOT infer the input location by inspecting its feature)
		\item $\Rightarrow$ need to have structure symmetry: $f(\mathbf z, \mathbf x) = f(\mathbf x, \mathbf z)$
		\item YET, deep net involves too much $0$-padding, which creates unique response \\
		$\Rightarrow$ conv kernel can recognize if its input is from center-border-corner \\
		$\Rightarrow$ destroy the translation invariance \\
		(as network can now realize the input location from its content) \\
		(e.g. if/how the content is influenced by $0$-padding) \\
		$\Rightarrow$ introduce central bias (\underline{from in-model padding}) \\
		(as most targets labeled in center - a wrong evidence learned to localize target) \\
		$\Rightarrow$ trained with shift augmentation \\ 
		(random shifting the crop region s.t. target not right in the center) \\
		$\Rightarrow$ let network realize $0$-padding influenced feature NOT related to target \\
		$\Rightarrow$ destroy the central bias
		\item $0$-padding in template \& only border of search img get $0$ padded \\
		$\Rightarrow$ template with $0$-padding hard to match the center of search img \\
		(as only the border get padded) \\
		$\Rightarrow$ strong signal for network to realize location \\
		$\Rightarrow$ central bias again! (\underline{introduced by xcorr}) \\
		$\Rightarrow$ crop out padding-influenced part from template \\ 
		(e.g. use only the central 7x7 of template branch as template)
		\item $0$-padding in preparing network input (input image patch) \\
		$\Rightarrow$ may get network realize location again (\underline{from outside-model op}) \\
		$\Rightarrow$ use the image average (channel-wise) instead of $0$ to pad
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item depthwise xcorr with deepnet: tracking with semantic - similarity \\
		$\Rightarrow$ extract useful semantic for similarity \\
		$\Rightarrow$ each channel captures some specific semantic (enforced by depthwise xcorr) \\
		\includegraphics[width=0.6\linewidth, left]{"./Deep Learning/plot/topic-obj track rpn++ semantic in depthwise xcorr".png} \\
		(response of $xcorr(\phi(\mathbf z), \phi(\mathbf x))$ at different channel, only search image shown) \\
		$\Rightarrow$ better tracking result in \underline{hard case} (e.g. under high IoU threshold)
		\item \underline{deep tracker can learn central bias, due to padding \& target-centered cropping} \\
		$\Rightarrow$ network learn to locate target by wrong evidence (input patch location) \\ 
		(as padding introduces unique response on the border \& corner) \\
		$\Rightarrow$ shift augmentation \& template central crop \& average padding \\
		$\Rightarrow$ force network to be location invariance \& ignore response from padding
		\item RPN use supervision more than similarity \\ 
		$\Rightarrow$ need asymmetric part to map similarity to RPN regression-classification task \\
		$\Rightarrow$ separate xcorr for different class
		\item multi-level feature may be meaningful only given enough detail-semantic diversity \\
		$\Rightarrow$ only meaningful with deep-enough net
		\end{itemize}
	\end{itemize}

\item SiamDW: Deeper and Wider Siam
	\begin{itemize}
	\item Analysis
		\begin{itemize}
		\item padding destroy the translation invariance \\
		(can know if a feature comes from corner-border-center by analyzing its content) \\
		$\Rightarrow$ as all target labeled in center, thus central bias
		\item padding results in INconsistent template-search feature map
			\begin{itemize}
			\item template feature extracted with padding
			\item search feature map has no such padding in non-border area
			\end{itemize}
		$\Rightarrow$ xcorr canNOT effectively measure similarity
		\item $\Rightarrow$ remove padding by "cropping-inside residual" (CIR) unit \\
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW central bias with-without crop".png}		
		\end{itemize}
	\item $\Rightarrow$ CIR Unit (to remove padding influence)
		\begin{itemize}
		\item add cropping after addition $\Rightarrow$ remove padding influenced feature \\
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW cir unit".png}
		\item change the position of downsampling ops \\
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW cir downsample".png} \\
		$\Rightarrow$ use conv with stride $1$ (instead of $2$) \& add 2x2 max pool after addition-crop \\
		$\Rightarrow$ remove padding influence \& considering info at the border (by max pool) \\
		(directly crop after stride-2 conv: miss out potential strong response at the edge) \\
		(has been also empirically proven direct-cropping has worse result)
		\item increase the width of each unit (as inception, resXnet) \\
		\includegraphics[width=.5\linewidth, left]{"./Deep Learning/plot/topic-obj track siamDW cir wider".png}
		\end{itemize}
	\item Backbone
		\begin{itemize}
		\item modified Resnet with stacked CIR unit \& downsampled by CIR-D unit \\
		$\Rightarrow$ with CIResNet-22 being the best-performance net \\
		(CIResNet-43 perform almost the worst, due to in less weight CIResNet-43) \\ 
		really? less wright, yes, but being deeper not help?
		\end{itemize}
	\item Training
		\begin{itemize}
		\item gradually finetune the network from back to front \\ 
		(e.g. unfreeze a unit every $5$ epoch)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item siam net as tracker \& tracking by detection
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item padding leads to \underline{central bias \& degraded similarity measurement} \\
		$\Rightarrow$ cropping to remove padding influence \\
		(why not, crop only before xcorr \& logit feature map to speedup?)
		\item ensure receptive field to be $60\% \sim 80\%$ size of the template image \\ 
		(empirical setting to have best performance for siam tracker) \\
		$\Rightarrow$ as larger RF includes more context \& thus insensitive to spatial location
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Multi-Object Tracking \& Data Association}
\begin{itemize}
\item Gated Association
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item given prediction, its variance and detection noise, filter out an interested area
		\item consider only detection inside the interested area (satisfying requirements)
		\item score each detection \& associate detection (detection result) with tracker \\
		$\Rightarrow$ which detection belongs to which trajectory
		\end{itemize}
	\item Global Nearest Neighbor
		\begin{itemize}
		\item choose the best / most probable / nearest \\
		(under the constraint that an detection can associate with at most one track)
		$\Rightarrow$ assume one detection is produced by single object
		\item require accurate and sparse detection, with few false alarm
		$\Rightarrow$ sensible to noise (easily fail in crowded scene)
		\end{itemize}
	\item Nearest Neighbor
		\begin{itemize}
		\item choose the best / most probable / nearest
		(thought one detection may be used by multiple tracks)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\item Joint Probability Data Association (JPDA)
	\begin{itemize}
	\item Procedure
	\item Understanding
		\begin{itemize}
		\item probabilistic perspective for prediction-detection relation \\ 
		$\Rightarrow$ cooperate with uncertain association: weight all detections by probability
		\item hence, crowded detections tends to pull multiple tracks together \\ 
		$\Rightarrow$ coalescence problem
		\end{itemize}
	\end{itemize}
\item Multiple Hypothesis Tracking
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item Gaussian models for target dynamics and noise in detection
		\item uniform distribution for false alarm (false-positive detection)
		\end{itemize}
	\item Track Hypothesis
		\begin{itemize}
		\item association result given track initiation, prediction and detection \\
		$\Rightarrow$ a sequence of selected detection
		\item compatibility: tracks are compatible, if they do NOT share any detection \\
		$\Rightarrow$ any track update using the same detection are INcompatible
		\end{itemize}
	\item Track Tree (Clustering)
		\begin{itemize}
		\item use incompatibility as edge, track as vertice, sort tracks by time \\
		$\Rightarrow$ each connected tree becomes a cluster (track family) \\
		(the tree level denotes time sequence)
		\item each tree shares a common root node (the initial detection)
		\item growing: whenever a new detection can be accounted for a track hypothesis (node)  \\ 
		$\Rightarrow$ the node generates 2 children nodes (tracks): update / not update
		\end{itemize}
	\item Global Hypothesis
		\begin{itemize}
		\item a global hypothesis contains only compatible track(s) \\ 
		$\Rightarrow$ the collection of track, with $\le1$ track from each tree/family
		\end{itemize}
	\item Track Score
		\begin{itemize}
		\item posterior ratio $r=\frac{p(D|T)p(T)}{p(D|F)p(F)}\triangleq \frac{p_T}{p_F}$, \\
		where $p(D|T), p(D|F)$ the likelihood given detection is true, false alarm \\
		with $D$ the detections in current track \\
		$\Rightarrow$ log ratio $lr = \ln \frac{p_T}{p_F}$
		\item use log ratio as score, at time $t, L(t) = L(t-1) + \Delta L(t)$, \\
		where $\Delta L(t) = \begin{cases} \ln(1-\hat{P}(D)) & \text{no update} \\ \Delta L_u(t) & \text{update} \end{cases}$, \\
		with $\hat{P}(D)$ the expected probability of detection; \\ 
		and $\Delta L_u(t)$ the residual error between prediction and detection \\
		($\Delta L_u(t)$ may include covariance, density, $\hat P_D$, etc.)
		\end{itemize}
	\item Global Hypothesis Score
		\begin{itemize}
		\item $s_H = \sum_{k\in K} L_k(t)$, \\ 
		where $K_H$ all (compatible) tracks in hypothesis $H$, $L_k$ the score for track $k$
		\end{itemize}
	\item Global Hypothesis Probability
		\begin{itemize}
		\item computed from hypothesis score (a maximum weighted independent set problem)
		\end{itemize}
	\item Track Probability
		\begin{itemize}
		\item the sum of probability of all hypothesis that contains the track
		\end{itemize}
	\item $N$-scan Pruning
		\begin{itemize}
		\item given detection at time $t$, eliminate IMplausible tracks originated at $t-N$ \\
		$\Rightarrow$ suppress tree from exponentially growing;
		\item $N$ the time step buffer before decision (scan = time), usually $N\ge 5$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item probabilistic perspective towards the result of decisions for data association \\
		(a larger scope than JPDA)
		\item defer critical decision into the future \\
		$\Rightarrow$ make decisions for the past after their observation available
		\item model track alternatives, each with a probability, by track tree and hypothesis \\ 
		(for all possible tracks, model joint prob over all detections in a track)
		\item model global joint probability of all tracks, by global hypothesis
		\item similar to DP-longest substring: maintain a set of candidates
		\item essentially, a bread-first search $\Rightarrow$ real-time ability constrained by tree size 
		\end{itemize}
	\end{itemize}

\item Maximum Net Flow
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item detection
		\end{itemize}
	\item Output
		\begin{itemize}
		\item association result
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item detection as node, possible association as edge \\
		$\Rightarrow$ construct a graph, with detection time as layer
		\item $\Rightarrow$ solve as maximum flow / minimal cost
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item another global optimization, vs. probabilistic perspective in MHT
		\end{itemize}
	\end{itemize}

\item Inversed Reinforced Learning for Data Association with Markov Decision Process
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item current detected bounding boxes, with objectness
		\item previously predicted bounding boxes
		\end{itemize}
	\item Output
		\begin{itemize}
		\item decision of data association between track \& detection
		\end{itemize}
	\item Markov Decision Process (MDP) for Track Management
		\begin{itemize}
		\item states: active, tracked, lost, inactive \\
		$\Rightarrow$ model the state of a track
		\item probability at each state given by trained model for each state
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item binary classification for track start (given a detection)
		\item optic flow to track \& association \\
		(rule model to decide if lost)
		\item regression model to associate lost track \& current detection \\
		(measure similarity)
		\item rule for track death: lost for consecutive $6$ frames
		\end{itemize}
	\item Training: Inversed Reinforcement Learning for Lost Recovery
		\begin{itemize}
		\item classifier for track start: trained offline
		\item regressor for lost track association: trained only when MDP make wrong decision \\
		(similar to hard-example mining?)
		\end{itemize}
	\item Tracking
	`	\begin{itemize}
		\item one MDP for a track in multi-object tracking \\
		$\Rightarrow$ multiple tracks may update with same detection (need to tune optic flow)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item explicit expression for track state \\
		$\Rightarrow$ can design state for hard scenario \\
		$\Rightarrow$ enable explicit control over optimization
		\item ugly crashed model for each state $\Rightarrow$ can be all unified to NN(s)
		\end{itemize}
	\end{itemize}

\item Siamese CNN for Association
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image pair $(I_1, I_2)$, with optic flow $I_1\rightarrow I_2$ as $(O_1,O_2)$ \\
		$\Rightarrow D = [I_1,I_2,O_1,O_2]$ (resized \& channel concat)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item probability of data association between two detections
		\end{itemize}
	\item Inference \& Structure
		\begin{itemize}
		\item 3 conv layers, max pooling, 4 dense layers
		\item examine spatio-temporal info: position change \& relative velocity by difference
		\item NN feature vector concat with handcraft feature
		\item feed into gradient boosting classifier (with $400$ trees) \\ 
		$\Rightarrow$ output as binary classification of (match, no match)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item true positive: associated ground truth detection in $2$ frames \\
		(time gap $\le15$ frames)
		\item negative: wrong association to true detection of other track / false detection
		\item data augmentation: false alarm, distortion on image
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item given current data association probability for all track-detection pair \\
		$\Rightarrow$ construct a linear program problem (with constraints)
		$\Rightarrow$ a global optimization for association given probability
		\item online tracker (e.g. kalman filter)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item NN approach for association probability \\
		(with fusion of NN \& rule-model via GB classifier) \\
		$\Rightarrow$ fusion much better than pure NN $\Rightarrow$ spatio-temporal info important
		\end{itemize}
	\end{itemize}

\item Online Multi-Target Tracking Using RNN
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item $D$ the dimension of bbox encoding (e.g. x, y, w, h, objectiveness, etc.)
		\item $x_t\in \mathbb{R}^{ND}$ all the $N$ track state (bbox) at time $t$
		\item $x^\star_{t} \in\mathbb{R}^{ND}$ all the $N$ predicted bboxes for time $t$, from time $t-1$
		\item $z_t\in \mathbb{R}^{MD}$ all the $M-1$ detected bbox at time $t$, with an empty detection
		\item $\varepsilon_t\in(0,1)^N  \in\mathbb R^N$ the existence probability (liveness) for all tracks
		\item $A_t \in \mathbb{R}_{N\times M}$ the probability matrix for data association between track-detection
		\item $h_t$ hidden state of track RNN at time $t$
		\item $C_t \in \mathbb{R}_{N\times M}$ the distance matrix between $x^\star_{t}$ and $z_t$ \\ 
		i.e. $C_t[i,j] = dist(x^\star_{t}[i] - z_t[j])$
		\item available ground truth: $\widetilde x_t, \widetilde A_t, \widetilde \varepsilon_t$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item given $x_t,h_t$, track RNN outputs state prediction $x^\star_{t+1}$, compute $h_{t+1}$
		\item based on pred $x^\star_{t+1}$ and detection $z_{t+1}$, compute $C_{t+1}$
		\item given $C_{t+1}$ and hidden state for $i^\text{th}$ track $h_{t+1}[i]$ \\ 
		$\Rightarrow$ association LSTM scans over all detection $z_{t+1}$ \\
		$\Rightarrow$ regress $A_{t+1}[i, :]$, the association prob for $i^\text{th}$ track and each bbox in $z_{t+1}$ \\
		(as part of track RNN process)
		\item given detection $z_{t+1}$, association prob $A_{t+1}$, latest liveness $\varepsilon_t$, with $h_{t+1}$ \\ 
		$\Rightarrow$ update state to be $x_{t+1}$, estimate liveness $\varepsilon_{t+1}$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item track RNN consists of a $2$-layer association LSTM
		\end{itemize}
	\item Loss
		\begin{itemize}
		\item prediction $L_\text{pred} = \frac \lambda {ND} \sum (x^\star_{t+1} - \widetilde x_{t+1})^2$
		\item updated state $L_\text{update} = \frac \kappa {ND} \sum (x_{t} - \widetilde x_{t+1})^2$
		\item liveness $L_\varepsilon = \widetilde \varepsilon_t\log\varepsilon_t + (1-\widetilde \varepsilon_t)\log(1-\varepsilon_t) + \abs{\varepsilon_t - \varepsilon_{t-1}}$ \\
		$\Rightarrow$ minimize the diff between consecutive liveness estimation $\Rightarrow$ smoothness \\
		(prevent track from termination for only a single detection lost)
		\item association $L_a = -\log(A_{t+1}[i,\widetilde{j}])$, where $\widetilde{j}$ the true association for $i^\text{th}$ track
		\end{itemize}
	\item Training
		\begin{itemize}
		\item data augmentation: sample synthetic trajectories from each labeled video \\
		(Gaussian distribution)
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item forward unroll track RNN, if liveness $\le0.6$, corresponding track ignored
		\item liveness $\ge0.6$ again, a new track initiated
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item specialized RNN cell accounting for prediction, update, birth-death of all tracks
		\item another LSTM cell designed for data association \\
		$\Rightarrow$ able to learn $1-1$ association by scanning \\
		(yet, unnecessary, since $N,M$ fixed i.e. a fixed size mapping)
		\item utilize given detector $\Rightarrow$ no appearance model (but only location \& size)
		\item able to maintain at most $N$ track with maximally $M$ detection at a time
		\end{itemize}
	\end{itemize}

\item Collaborative Deep Reinforcement Learning for MOT
	\begin{itemize}
	\item Notation
		\begin{itemize}
		\item $I_t$ the $t^\text{th}$ image frame
		\item $b^\star_{i,t}$ a bbox for $i^\text{th}$ ground truth object $p_i$ at frame $t$
		\item $B^\star_{i,t}$ a set of bboxes sampled around $b^\star_{t,i}$
		\item $g(a,b)$ cal the IoU between bbox $a,b$
		\item $p_{i,t} = \{ b,f \}$, the $i^\text{th}$ detected object at $t$, \\
		where $b=\{x,y,w,h\}$ the bbox; $f$ the appearance model \\
		$\Rightarrow$ distance $d(p_1,p_2) = \alpha (1-g(b_1,b_2)) + (1- \underbrace{\frac{f_1^Tf_2}{\norm{f_1}\norm{f_2}}}_\text{(cos dist)})$
		\item $H=\{ b_1, ..., b_t \}$ the history trajectory of an object \\
		$\Rightarrow H^K = \{b_{t-K+1}, ..., b_t\}$ the history of past $K$ frames
		\item an agent for each object $g = \{ H, p \}$
		\item detections as environment $\hat P_t = \{\hat p_1,...,\hat p_{n_t}\}$
		\item state at frame $t, s_t=\{G_t, \hat P_t \}$, where $A_t = \{ g_1,...,g_m \}$
		\item set of actions $\mathcal A=\{\text{update}, \text{ignore}, \text{block}, \text{delete} \}$
		\end{itemize}
	\item Prediction Net Inference
		\begin{itemize}
		\item crop the frame $t+1$ at the location of estimated bbox $b_{i,t}$ (of frame $t$)
		\item 3 conv layers, then dense, then concat with $H^{K=10}$ (fuse with temporal info)
		\item 2 dense layers to regress $b_{i,t+1}$, the bbox for object $i$ of frame $t+1$
		\end{itemize}
	\item Prediction Net Training
		\begin{itemize}
		\item regression loss $\displaystyle L = \sum_{i,t}\sum_{b\in B_{i,t}} g ( b^\star_{i,\mathbf{t+1}}, \phi(I_t, b, H^{K=10}_i))$, \\
		where $I_t$ the image frame at time $t$, $\phi$ the mapping of pred net
		\end{itemize}
	\item Action
		\begin{itemize}
		\item update: $f_{t+1}= (1-\rho_f) \cdot f_t + \rho_f \cdot \hat f_{t+1}$, where $\hat f_{t+1} \in$ selected detection $\hat p_{t+1}$; \\
		$b_{t+1} = (1-\rho_b) \cdot b_{t} + \rho_b \cdot b'_{t+1}$, where $b'_{t+1}$ predicted position; \\
		($\rho_f, \rho_b$ pre-selected)
		\item ignore: no detection suitable, use only prediction for update ($\rho_f=0, \rho_b=1$)
		\item block: same as ignore, no detection due to occlusion
		\item delete: remove the agent
		\end{itemize}
	\item Reward
		\begin{itemize}
		\item for agent $g$ at time $t$, with pred and ground-truth box $b'_{t+1}, b^\star_{t+1}$
		\item reward for agent $g$ at time $t: r^\star_{t} = r_t + \beta r_{j,t}$, where \\ 
		$r_t$ for itself; $r_{j,t}$ for its nearest neighbor, $\beta$ a balance factor \\ 
		($r_t,r_{j,t}$ calculated in the same manner) \\
		$\Rightarrow$ agents need to collaborate for better reward
		\item for action $a\in\{\text{update}, \text{ignore}, \text{block}\}$ \\
		$\Rightarrow r_t = \begin{cases}1 & \text{if }IoU \ge 0.7 \\ 0 & \text{if } 0.5 \le IoU \le 0.7 \\ -1 & \text{otherwise} \end{cases}$ ($IoU$ calculated between $b'_{t+1}$ and $b^\star_{t+1}$) \\
		for action $a=$ delete $\Rightarrow r_t = 1 \text{ if object disappear; else } -1$
		\item $\Rightarrow Q(s_t, a_t) = r^\star_{t} + \gamma r^\star_{t+1} + \gamma^2 r^\star_{t+2} + \dotsb$, where $\gamma$ decaying param
		\end{itemize}
	\item Decision Net Inference
		\begin{itemize}
		\item for each $g\in G_t$ with its current \& predicted location $b_t, b'_{t+1}$
		\item select a neighbor agent $g_{j} \in G_t-\{g\}$, that is nearest to $b_{t}$
		\item select the detection $\hat p \in \hat P_{t+1}$ that is nearest to $b'_{t+1}$
		\item 3 feature maps $(p\in g, p_j\in g_j, \hat p)$, flatten as $1$-D vector input \\
		$\Rightarrow 3\times$ dense layer to output prob over actions $\pi(a|s,\theta)$, where $\theta$ the weights
		\end{itemize}
	\item Decision Net Training
		\begin{itemize}
		\item goal $\displaystyle \arg \max_{\theta} L(\theta) = \mathbb E_{s,a} \log (\pi (a|s,\theta)) \cdot Q(s,a)$ \\
		\begin{align*} \displaystyle \Rightarrow \frac \partial {\partial \theta} L  &= \mathbb E_{s,a} \frac \partial {\partial \theta}[ \log (\pi (a|s,\theta)) \cdot Q(s,a) ] \\ &= \mathbb E_{s,a} [\frac{Q(s,a)}{\pi(a|s,\theta)} \cdot \frac {\partial }{\partial \theta} \pi(a|s,\theta)] \end{align*} \\
		$\Rightarrow$ increase probability for actions with $Q>0$; decrease for those with $Q<0$
		\item to speed up converge: value for state $s, \displaystyle V(s) = \frac{\sum_{a}p(a|s)Q(s,a)} {\sum_{a} p(a|s)}$ \\
		$\Rightarrow$ advantage value $A(s,a) = Q(s,a) - V(s)$ \\
		(in case all $Q>0$, or all $Q<0$ at the beginning: use expectation as zero-line)
		\item policy gradient as $L(\theta) = \mathbb E_{s,a} \log(\pi(a|s.\theta)) A(s,a)$
		\item pre-training: set $\gamma, \beta = 0$ before searching hyper-parameter $\beta,\gamma$
		\item training set: detection bbox $=\{ \hat b \in \text{detection } \hat P | \text{ IoU}^{\hat b}_\text{label} > 0.5 \} + \{\text{label } b^\star \}$
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item initiate a track for each initial detection
		\item for each agent, predict its location at $t+1$, by pred net
		\item for each agent $g$, select its closest detection $\hat p \in \hat P_{t+1}$, $p_j\in$ closest agent $g_j$\\
		$\Rightarrow$ decision net: $(p\in g, \hat p, p_j) \rightarrow$ action $\mathcal A$ 
		\item track terminates when decision net decides to "delete"
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item prediction net as a detector to give more precise location \\
		(an implicit self-trained detector)
		\item decision net to eliminate false alarm \& combine prediction \\ 
		$\Rightarrow$ robust to different detector/predictor
		\item decision net trained to collaboratively maximize utility \\ 
		$\Rightarrow$ mitigate the false negative \\
		$\Rightarrow$ introduce distractor in training for better discriminative ability
		\item may trapped in false appearance feature $\Rightarrow$ ID switch \\ 
		(e.g. blue box handed from one person to another person)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Instance Tracking}
\begin{itemize}
\item SiamMask: Fast Online Object Tracking and Segmentation : A Unifying Approach
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item initial bbox $z$ as target template at first frame (exemplar)
		\item cropped search region $x_t$, centered at target location of $t-1$
		\end{itemize}
	\item Output
		\begin{itemize}
		\item response map for object localization
		\item bbox regression for target (with resizing \& \underline{rotation})
		\item binary segmentation mask i.e. pixel $\in$ (target, not target)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item given $z, x_t$ backbone CNN extract feature as $f(z), f(x_t)$
		\item cross-correlation response map $g(z,x_t) = f(z)\star f(x_t)$, \\
		where $f(z)$ used as kernel \\
		$\Rightarrow g$ as response of candidate window (RoW) \\
		$\Rightarrow$ encode similarity between $z$ and each candidate window/bbox in $f(x_t)$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item backbone CNN ResNet-50 as feature extractor, with adjustment \\
		(dilated conv for better resolution)
		\item cross-relation between two extracted feature maps \\ 
		$\Rightarrow$ response map, each location an RoW
		\item bbox regression: 2$\times$(1-by-1 conv) for $k$ anchors at each RoW
		\item bbox classification: 2$\times$(1-by-1 conv) for score of $k$ anchors at each RoW
		\item segmentation: 2$\times$(1-by-1 conv) + upsampling with skip + per-pixel sigmoid \\
		$\Rightarrow$ upsample into a mask for each RoW location
		\end{itemize}
	\item Training
		\begin{itemize}
		\item loss $L = \lambda_1 L_\text{mask} +\lambda_2 L_\text{score} +\lambda_3 L_\text{box}$ \\
		($L_\text{mask}, L_{box}$ considered only at location for ground truth)
		\item trained different branch using corresponding dataset
		\item data augmentation: random jitter, translation, rescaling
		\end{itemize}
	\item Tracking
		\begin{itemize}
		\item one-step update, may use mask to produce 
		\item multi-object: multiple initialization, each with a net as tracker
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multi-task training improves all branches \\
		(semi-supervised video object segmentation, bbox tracking)
		\item mask branch: output the mask in the context of $x$ for that obj in $z$
		\item simple starting point \& fast speed for video object segmentation (compared to $0.1$FPS)
		\item more descriptive representation for tracked object $\Rightarrow$ more detail in mask
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Face Recognition}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item image from camera
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Identity Recognition
		\begin{itemize}
		\item recognize the identity of the face in image
		\item refuse to recognize if the face does NOT belongs to any stored identity
		\end{itemize}
	\item Liveness Detection
		\begin{itemize}
		\item make sure the face in image are from a live human \\
		(instead of picture etc.)
		\end{itemize}
	\end{itemize}
\item Face Verification
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image from camera \& identity
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item true/false, regarding whether the image content belongs to the identity
		\end{itemize}
	\end{itemize}
	
\item Challenge
	\begin{itemize}
	\item One-shot Learning
		\begin{itemize}
		\item given only single (at most, few) face-identity pair for each identity
		\item still, need to build a robust system for recognition task
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Siamese Network as Encoder
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item CNN + dense layer to encode the input image $x^i$ as a vector $f(x^i)$
		\end{itemize}
	\item Learning Goal
		\begin{itemize}
		\item minimize $\norm{f(x^i)-f(x^j)}^2$ if $x^i, x^j$ from same identity
		\item maximize $\norm{f(x^i)-f(x^j)}^2$ if $x^i, x^j$ from different identity
		\item $\Rightarrow$ learning encoding given a fixed distance function $d(x_1, x_2) \ge 0$\\
		(here, $d(x_1, x_2)=\norm{x_1-x_2}^2$)
		\end{itemize}
	\item Triplet Loss
		\begin{itemize}
		\item given an anchor image $A$ representing the identity $I$
		\item take a positive image $P\in$ identity $I$; an negative image $N\not\in$ identity $I$
		\item $\Rightarrow L(A,P,N) = \max\left( d(f(A), f(P)) - d(f(A), f(N)) + \alpha, 0 \right)$, where \\
		$\alpha$ an hyperparamter to make sure the net differentiate them by a margin; \\
		$\max()$ to make the loss $=0$ as long as the requirement satisfied
		\end{itemize}
	\item Training: Hard Negative Mining
		\begin{itemize}
		\item due to large variance in the dataset $\Rightarrow$ $d(A,P) << d(A,N)$ in most case
		\item due to large number of identities $\Rightarrow$ permutation explosion
		\item $\Rightarrow$ evaluate current net on dataset, use mistakes for the next epoch
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn a encoder towards a selected distance function \\
		$\Rightarrow$ use permutation to have more training examples
		\item able to precomputing the encoded vector for fast recognition
		\end{itemize}
	\end{itemize}

\item Encoding + Binary Classification
	\begin{itemize}
	\item Structure
		\begin{itemize}
		\item still, CNN + dense layer to encode input image
		\end{itemize}
	\item Learning Goal
		\begin{itemize}
		\item given two encoded vectors, another net (or logistic regression) to perform binary classification \\
		$\Rightarrow$ $1$ for two image has same identity; $0$ for different identities
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item still, utilize permutation for larger training set \\ 
		(use pair, instead of triplet)
		\item learn the similarity function as well: output directly the result of comparison
		\item pre-compute the encoding of Siamese net \\
		$\Rightarrow$ enable flexible deployment (device performs only bi-classification)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Stereo Vision}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Visual Perception
		\begin{itemize}
		\item image from mono-camera
		\item images pair from stereo-/multi-cameras
		\item video sequence from mono-/stereo-cameras
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Depth Perception
		\begin{itemize}
		\item directly output a depth map
		\item output a disparity map as indirect depth perception
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Unsupervised Learning}
\begin{itemize}
\item Unsupervised Monocular Depth Estimation with Left-Right Consistency
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item epipolar constraint with image reconstruction loss \\
		$\Rightarrow$ unsupervised depth from mono-camera \\
		$\Rightarrow$ avoid stereo-/multi-camera in practice use \& avoid pixel-level labeling
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Recognition at a Distance}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Video
		\begin{itemize}
		\item hot standby camera with stationary \& active vision
		\item stationary vision: wide field of view with low resolution for detection
		\item active vision: narrow field of view with high resolution for detail analysis
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Recognition in the Wild
		\begin{itemize}
		\item large coverage areas: $>100$m range \\
		$\Rightarrow$ scale beyond the theoretical analysis and practical design advice
		\item with NO subject cooperation
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item None Cooperative Subject
		\begin{itemize}
		\item not cooperating, may even be evading the system
		\end{itemize}
	\item Resolution
		\begin{itemize}
		\item low resolution due to the very long object distance
		\item restricted by lens resolution, which is then restricted by price
		\item trade-off between wideness (coverage) \& depth (zooming)
		\end{itemize}
	\item Illumination
		\begin{itemize}
		\item dynamic illumination in the wild scene
		\item maximum light intensity restricted by aperture size \\
		(given a fixed exposure time)
		\end{itemize}
	\item Distortion and Blur
		\begin{itemize}
		\item amplified noise, due to: \\
		low brightness $\Rightarrow$ low signal-to-noise ratio $\Rightarrow$ ISO amplification
		\item motion blur (if trade-off between ISO \& exposure time)
		\item blur from sensor tilt, as a hot standby system
		\item fog, haze \& atmosphere blur for very long distance recognition
		\end{itemize}
	\item Pose
		\begin{itemize}
		\item the view angle due the camera position \\ 
		(usually overhead for less occlusion $\Rightarrow$ downward tilt)
		\end{itemize}
	\item Multi-Object
		\begin{itemize}
		\item schedule the limited high-resolution vision resource for multiple candidates \\
		$\Rightarrow$ time window prediction, scheduling \& resource allocation
		\end{itemize}
	\item Physical Coupling
		\begin{itemize}
		\item expensive field test (mitigated by virtual environment)
		\end{itemize}
	\end{itemize}
\item Application
	\begin{itemize}
	\item Watch-list Recognition
		\begin{itemize}
		\item an alert when person of interest appear/approach
		\end{itemize}
	\item Re-recognition
		\begin{itemize}
		\item cross-camera tracking, long-range persistent tracking
		\end{itemize}
	\item Logging
		\begin{itemize}
		\item catalog best recognized feature (e.g. face) for each person entering a region
		\item marketing: understand customer activities and behavior
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Stationary Vision}
\begin{itemize}
\item 
\end{itemize}

\subsubsection{3D Imaging}
\begin{itemize}
\item 
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsection{Re-Identification}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Images
		\begin{itemize}
		\item captured by camera networks across multiple areas
		\item may assume to be a crop over interested target \\
		(e.g. a bounding box crop, instead of the whole image)
		\item a single image or a images sequence of target
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Identification
		\begin{itemize}
		\item retrieve images from gallery/database that has same identification as input
		\end{itemize}
	\item Ranking
		\begin{itemize}
		\item list out the most probable ID, to ensure recall \\
		(e.g. in security scenario)
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Varying Appearance
		\begin{itemize}
		\item due to different view point, pose, background change (indoor v.s. outdoor), etc. \\
		$\Rightarrow$ hard to realize inter-class similarity \& intra-class difference
		\end{itemize}
	\item Limited Data
		\begin{itemize}
		\item small dataset, hard to learn to use rare feature \\
		(though rare feature should be a strong identification sign)
		\end{itemize}
	\end{itemize}
\item Metrics
	\begin{itemize}
	\item Rank-1 Accuracy
		\begin{itemize}
		\item measure the match of ID with highest predict prob
		\end{itemize}
	\item mean Average Precision
		\begin{itemize}
		\item measure the accuracy of match of whole ranking \\
		$\Rightarrow$ account for the hard case (same ID can be multi-viewed in camera net)
		\end{itemize}
	\end{itemize}
\item Trends
	\begin{itemize}
	\item Local \& Distributed Representation
		\begin{itemize}
		\item feature on human body with human body model (e.g. skeleton model) as a prior
		\item region proposal for different local area on human body
		\item salient partitions with attention
		\end{itemize}
	\item Multi-scale Representation
		\begin{itemize}
		\item concat local feature, features at multi-scales
		\item spatial attention
		\end{itemize}
	\item Surpassing Human Performance
	\end{itemize}
\end{itemize}
\subsubsection{People Re-ID}
\begin{itemize}
\item PCB-RPP: Person Retrieval with Refined Part Pooling
	\begin{itemize}
	\item Inference 
		\begin{itemize}
		\item CNN extracts feature map, vertically "soft" partitioned into $p$ parts
		\item each of $p$ branch independently performs average pooling, processing \& output 
		\item $p$ output concat into together to measure overall similarity
		\end{itemize}
	\item PCB (Part-based Convolution Baseline) Structure
		\begin{itemize}
		\item ResNet-50 as backbone, before the global average pooling \\
		(last downsampling removed $\Rightarrow$ richer feature granularity)
		\item vertically partitioned into $p$ parts, average pooling on each part \\
		$\Rightarrow$ part average pooling (centroid representation as part-level feature)
		\item 1x1 conv over $p$ vec to extract channel-wise info \& dimension reduction \\
		(sharing weights between parts here)
		\item each vec a separate dense layer for output \\
		$\Rightarrow p$ vector, each able to independently measure similarity / predict ID
		\end{itemize}
	\item RPP (Refined Part Pooling) Structure
		\begin{itemize}
		\item observe within-part INconsistency in hard-uniform partition \\
		$\Rightarrow$ violate part-level feature assumption \\
		(violate: feature vec in same part are similar \& dissimilar to vec in other parts)
		\item $\Rightarrow$ a softmax classifier to predict the part a feature vec should belong to \\
		$\Rightarrow$ model $P(P_i\mid f)$, where $P_i$ $i^\text{th}$ part, $f$ the feature vec
		\item feature representation for $i^\text{th}$ part $\displaystyle = \sum_{f\in\{P_i\}} P(P_i\mid f)\cdot f$ \\
		(weighted average, instead of simple part average pooling) \\
		$\Rightarrow$ essentially, use attention for a "soft-adaptive" partition
		\end{itemize}
	\item Training
		\begin{itemize}
		\item train initial PCB net with all $p$ cross-entropy loss to convergence
		\item replace $p$-part average pooling with $p$ part classifiers (attention generator)
		\item with PCB net fixed, train only part classifiers to convergence \\
		$\Rightarrow$ ensure part-based attention is learned \\
		(since there lacks direct supervision for attention to be part-focused) \\
		$\Rightarrow$ force to collect part-consistent feature (close to original part feature)
		\item train whole net to convergence for fine-tunning \\ 
		$\Rightarrow$ further jointly refine consistency \& performance
		\item augmented by horizontal flip, 60+10+10 epochs
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item model prior on people: more diverse in vertical, more similar in horizontal (as symmetric) \\
		(can be visualized by activation testing on their corresponding final classifier - dense layer)
		\item separate supervision \& independent classifier on each branch is superior \\
		$\Rightarrow$ vital to learn\&use discriminative \underline{part-level features}
		\item waive the need of learning part partitioning algorithm \\
		$\Rightarrow$ less noise source \& NOT depends on other realms e.g. human pose estimation \\
		(as there are gaps between pose estimation \& re-ID)
		\item setting $p$ needs validation $\Rightarrow p=6$ empirically \\
		(small $p$: just global feature, large $p$: some redundant parts being repeated/empty)
		\item within-part INconsistency observed in PCB: \\ 
		by clustering vec \& compute similarity with the average pooling of each part
		\begin{minipage}[r]{0.5\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/topic-reid pcb-rpp within-part inconsistency".png}
		\end{minipage}
		\begin{minipage}[l]{\linewidth}
		where, \\
		partitioned to $p=6$ parts, \\ 
		vec in feature map colored to its closest part \\
		(by similarity between vec \& per-part average pooling) \\
		$\Rightarrow$ overall consistent, yet with some outliers
		\end{minipage}
		\item RPP emphasize \text{within-part consistency}, by refining pre-partitioned parts \\
		$\Rightarrow$ protect part-level feature assumption \\
		(s.t. feature vec within same part are similar \& dissimilar to vec in other parts)
		\item attention as "soft-adaptive" partition \\ 
		$\Rightarrow$ over whole feature maps to aggregate feature for each part \\ 
		$\Rightarrow$ avoid outliers in "hard-uniform" partition \\
		$\Rightarrow$ hence better partition for deep part feature
		\item: \underline{EM-like iterative training for desired effect} (better than only joint training) \\
		(more supervision)
		\end{itemize}
	\end{itemize}
\item MGNet: Discriminative Features with Multiple Granularities
	\begin{itemize}
	\item Inference
		\begin{itemize}
		\item CNN extracts feature map, each branch vertically partitioned with various granularity
		\item each branch output part/global-level representation accordingly
		\item all (global + multi-granularity) representation concat as final representation
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item ResNet-50 as backbone (before and including res\_conv4\_1)
		\item global branch: res\_conv5\_1 block + global max pooling + 1x1 conv
		\item part-$n$ branch: split into $n$ strips, each strip a max pooling + 1x1 conv \\
		(max pooling \& 1x1 conv before split to generate branch-level global feature $f^{P_n}_g$) \\
		$\Rightarrow$ employ part-$2$ \& part-$3$ branch \\
		\begin{minipage}[r]{0.5\linewidth}
		\includegraphics[width=\linewidth, left]{"./Deep Learning/plot/topic-reid PGN".png}
		\end{minipage}
		\begin{minipage}[l]{\linewidth}
		res\_conv5\_1 block + max pooling + 1x1 conv \\ \\
		$n$ strips, each a max pooling + 1x1 conv \\
		\phantom{x}\hspace{1cm} + \\
		max pooling \& 1x1 conv before split into strips \\ 
		($\Rightarrow$ branch-level global feature $f^{P_n}_g$) \\
		\end{minipage}
		\end{itemize}
	\item Training
		\begin{itemize}
		\item pre-trained on ImageNet \& horizontal flip for data augmentation
		\item each branch a softmax loss \& triplet loss \\
		\item softmax: learning basic discrimination in ReID as multi-classification problem \\ 
		(classification to all number of class in dataset) \\
		$\Rightarrow$ no bias/activation for better discrimination
		\item batch-hard triplet: metric learning \& better ranking performance \\
		(embed on final feature after 1x1 conv on each branch)
		\item classfication-before-metric, as shown \\
		(softmax loss on part feature \& before each global feature) \\
		(triplet loss after softmax \& only on each global feature) \\
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item multi-scale feature matters $\Rightarrow$ global + multi-granularity local feature \\
		$\Rightarrow$ global branch with downsampling: \underline{global \& coarse feature} \\
		$\Rightarrow$ local branch with no strided conv: \underline{local \& fine feature} \\
		(better than PCB-RPP by a large margin, especially mAP and in hard scene)
		\item able to learn to focus on various part based on its split region \\
		e.g. global branch: main body, part-$3$: small salient feature \& limbs \\
		$\Rightarrow$ enhanced ability to notice \underline{infrequent yet discriminative} feature \\
		(by fine-local feature)
		\item NOT applying triplet loss on local feature of any branch \\
		(local feature not enough for identification, hence not bother to confuse model)
		\item NOT using single feature map for different split of granularity \\
		$\Rightarrow$ s.t. each branch can further mining better feature for its split setting
		\item extra weights are NOT main contributor (experimented)
		\item ensembling helps, yet still better if multi-branch jointly trained \\
		(sharing backbone for joint goal: branches mutually complement others)
		\item $\Rightarrow$ overlap in split matter: branches part-$2/4$ worse than branches part-$3/4$
		\item triplet loss further help network to capture fine-local feature
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Image Style Transfer}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Content Image $C$
		\begin{itemize}
		\item the image containing the spatial info (content)
		\end{itemize}
	\item Style Image $S$
		\begin{itemize}
		\item the image containing the style of presenting the content
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Generated Image $G$
		\begin{itemize}
		\item a image with content from $C$ drawn in style of $S$
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/topic-image style transfer nst overview".png}
		\end{figure}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Neural Style Transfer
	\begin{itemize}
	\item Learning Goal
		\begin{itemize}
		\item given input $C,S$ with output $G$, loss $L = \sum_{l} \left[\alpha L_\text{content}(C,G) + \beta L_\text{style}(C,S)\right]$, \\
		where $l$ is sum over chosen hidden layers of the CNN
		\item $\Rightarrow$ minimize content \& style difference
		\end{itemize}
	\item Content
		\begin{itemize}
		\item given input, the activations from a set of (hidden) layers of the net
		\item $\Rightarrow$ similarity of $C, G$ measured as $\sum_l d(a^{l(C)}, a^{l(G)})$, \\ 
		where $a^{l(\cdot)}$ the feature maps at layer $l$ given the input, $d(\cdot)$ a distance function \\
		(e.g. $d(x_1,x_2) = \norm{x_1 - x_2}^2$)
		\end{itemize}
	\item Style
		\begin{itemize}
		\item given input, the correlation between activations across channels, for chosen layers \\
		$\Rightarrow$ correlation matrix across feature map at each channel as style matrix \\
		(actually, gram matrix)
		\item let $a_{i,j,k}^l$ the activation at a $h\times w\times c$ conv kernel location $i,j,k$ in layer $l$ \\ 
		$\Rightarrow$ style (gram) matrix $M^l_{k,k'} = \sum_{i,j}a^l_{ijk}\cdot a^l_{ijk'}$, for all $k,k'\in\{1,...,c\}$
		\item $\Rightarrow$ similarity of $S, G$ measured as $\sum_l \left[ \frac 1 {(2 h^l w^l c^l)^2} d(M^{l(S)}, M^{l(G)}) \right] $ \\
		where $M^{l(\cdot)}$ the gram matrix at layer $l$ given the input, $d(\cdot)$ a distance function, with normalization term $\frac 1 {(2 h^l w^l c^l)^2}$ \\
		(e.g. $d(x_1, x_2) = \norm{x_1-x_2}^2_F$, the euclidean norm between matrices)
		\begin{figure}[ht]
		\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/plot/topic-image style transfer nst style matrix".png}
		\end{figure}	
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Video Generation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image
		\begin{itemize}
		\item as the first frame of the video
		\end{itemize}
	\item Caption
		\begin{itemize}
		\item specifying the video content
		\end{itemize}
	\end{itemize}
\item Output
	\begin{itemize}
	\item a video sequence
	\end{itemize}
\item Metrics
	\begin{itemize}
	\item Similarity / Distance
		\begin{itemize}
		\item between generated \& original video \\
		(original video usually uniformly sampled \& with its image downsampled)
		\end{itemize}
	\item Use Study
		\begin{itemize}
		\item unique human answering question \\
		e.g. which video more realistic / more suitable for the given caption? etc.
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Consistency
		\begin{itemize}
		\item consistent scenes between frames
		\end{itemize}
	\item Realistic Motion
		\begin{itemize}
		\item especially human/animal motion due to their high complexity
		\end{itemize}
	\item Conditioning Video Generation
		\begin{itemize}
		\item control the content, style, etc. \\
		(given initial frame: overlap with video prediction realm) \\
		(while caption provides more control)
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Generation from Caption}
\begin{itemize}
\item CFT-GAN: Conditional Video Generation Using Action-Appearance Captions
	\begin{itemize}
	\item Inference (Generation)
		\begin{itemize}
		\item encode caption with randomness: encode the description of subject, action \& background, etc.
		\item generate optical flow to represent action given $\psi, z_\text{flow}\sim p_z$
		\end{itemize}
	\item Caption Encoding
		\begin{itemize}
		\item caption feature: $\psi$ by Fisher Vectors with HGLMM
		\item conditioning augmentation: $\mathbf c = \psi*(1+\epsilon)$, where $*$ element-wise product \\
		$\Rightarrow$ avoid coarse distribution of $\psi$ by adding noise $\epsilon\sim\mathcal N(0,1)$ \\
		$\Rightarrow \mathbf c \sim \mathcal N(\psi, 1)$
		\end{itemize}
	\item Latent Variables
		\begin{itemize}
		\item random variable (vector) $z\sim\mathcal N(0,1)$
		\end{itemize}
	\item Motion Generator FlowGAN
		\begin{itemize}
		\item input: concat [sampled $\mathbf c=\mathbf c_\text{flow}$, sampled $z=z_\text{flow}$]
		\item based on VGAN: generate optical flow $\mathbf f = m(z)*f(z)$, ($*$ element-wise product)\\ 
		where $m(z)$ a mask to fuse foreground $f(z)$ \& background $b(z)=0$ \\
		($b(z)=0$ due to static camera, as background NOT generating optic flow)
		\item for each time step, upsampled to $64\times 64$ feature map \& mask \\
		$\Rightarrow$ 3D conv for volume of $64\times64\times t$
		\end{itemize}
	\item Motion Discriminator
		\begin{itemize}
		\item concat an input flow \& a corresponding real flow, downsample
		\item dense layer to compress original caption feature $\psi$ as $\psi'$
		\item tile $\psi'$ into downsampled feature map \& discriminate if input flow is real or fake \\
		(further discriminate if flow related to caption)
		\end{itemize}
	\item Appearance Generator TextureGAN
		\begin{itemize}
		\item input: optic flow $\mathbf f$ \& concat [sampled $\mathbf c=\mathbf c_\text{tex}$, sampled $z=z_\text{tex}$]
		\item condition variables map $\mathbf f_c$: upsample [$\mathbf c, z$] by 2 upsampling blocks
		\item $\mathbf f$ as U-net input \& $\mathbf f_c$ concat in downsampling stage \\
		$\Rightarrow$ retain spatial info: reflect edges in $\mathbf f$ as shape of moving target \\
		$\Rightarrow$ output both foreground $f$ \& mask $m$ of shape $64\times64\times t$
		\item background $b$: upsampled from [$\mathbf c, z$] for a single frame \& replicated $t$ times
		\item fusion: video $v = f*m + (1-m)*b$
		\end{itemize}
	\item Video Discriminator
		\begin{itemize}
		\item concat input video \& a corresponding real video, downsample
		\item downsample the optic flow for input video $\mathbf f$ as $\mathbf f'$ \& further downsample
		\item dense layer compress caption $\psi$ as $\psi'$ \& concat to downsampled feature map \\
		(also discriminate if video related to caption)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item textureGAN trained initially using real optic flow calculated from real video \\
		(until flowGAN more stable \& trained to a extend, to avoid wasted training)
		\item use existing video dataset \& adding action-appearance caption
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item demonstrate ability for finer control on video content \\ 
		(not only action, but background, appearance) \\ 
		$\Rightarrow$ two-stage (action + appearance) generation for each frame for realistic complex scene\\
		$\Rightarrow$ caption for more descriptive control (v.s. initial frame)
		\item more abstract \& general control \\ 
		$\Rightarrow$ reflect more variety of complex actions and appearances by captions \\ 
		(compared to use image as initialization)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Point Cloud Data Processing}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Point Cloud
		\begin{itemize}
		\item from lidar $\Rightarrow$ $3$-D position x,y,z \& reflection intensity
		\item from radar $\Rightarrow$ $2$-D bird-view x,y \& intensity (rcs) \& speed (along radial direction)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item $3$-D Environment Modeling
		\begin{itemize}
		\item bounding box
		\item segmentation (per-point classification)
		\item instance segmentation, etc.
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Sparsity
		\begin{itemize}
		\item point gets much sparser in distance (e.g. $>$ 40m)
		\end{itemize}
	\item Varying Density
		\begin{itemize}
		\item due to occlusion, distance etc.
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Common Preprocessing}
\begin{itemize}
\item Voxelization
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item create $3$-D pixel, the voxel
		\end{itemize}
	\item Procedure
		\begin{itemize}
		\item apply $3$-D grid on the space \\ 
		$\Rightarrow$ each point resides in a spatial cell, the voxel
		\end{itemize}
	\end{itemize}
\item Bird-view Projection
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item project onto $2$-D map of a bird-view perspective \\
		$\Rightarrow$ better resolution due to reduced dimension
		\end{itemize}
	\item Procedure
		\begin{itemize}
		\item apply $2$-D grid on the ground \\
		$\Rightarrow$ each point resides in a cell, or, each cell consists of several point
		\item extract height information from points in each cell \\
		$\Rightarrow$ each pixel with a height $h$
		\end{itemize}
	\end{itemize}
\item Cylindrical Projection (Frontal View)
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item project onto $2$-D map of pilot perspective \\
		$\Rightarrow$ better align with camera perspective
		\end{itemize}
	\item Procedure
		\begin{itemize}
		\item given $3$-D cartesian coord $x,y,z$ \\ 
		$\Rightarrow$ spherical coord $r = \sqrt{x^2+y^2+z^2}, \theta = \arctan{\frac y x}, \phi=\arcsin{\frac {z} {r}}$
		\item slicing on horizontal \& vertical angle: each point resides in a $3$-D slice \\ 
		$\Rightarrow$ normalized by angle resolution $\theta' = \floor*{\frac \theta {\delta \theta}}, \phi' = \floor*{\frac \phi {\delta \phi}}$
		\item each pixel $(\theta', \phi')$ extract depth $d=\sqrt{x^2+y^2}$ \& height $z$ from its point(s)
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Point Cloud}
\begin{itemize}
\item PointNet
	\begin{itemize}
	\item 
	\end{itemize}
\item VolxelNet
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item voxelization
		\item random sampling points in each voxel to be at most $T$ points a voxel
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item $x,y,z$ for center position; $l,w,h$ for size; $\theta$ for orientation, the yaw rate \\
		$\Rightarrow x_g,y_g,z_g,l_g,w_g,h_g,\theta^g$ the ground truth box \\
		$\Rightarrow x_a,y_a,z_a,l_a,w_a,h_a,\theta^a$ an anchor box
		\item normalized residual position: $\Delta x=\frac{x_g-x_a}{\sqrt{l_a^2+w_a^2}}, \Delta y=\frac{y_g-y_a} {\sqrt{l_a^2+w_a^2}}, \Delta z=\frac{z_g-z_a}{h_a}$
		\item normalized size ratio: $\Delta l=\log(\frac {l_g}{l_a}), \Delta w = \log(\frac{w_g}{w_a}), \Delta h=\log(\frac{h_g}{h_a})$
		\item residual orientation: $\Delta \theta = \theta_g - \theta_a$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item voxel map with raw points as input, output $3$-D classification \& regression map
		\end{itemize}
	\item Voxel Feature Encoding Layer
		\begin{itemize}
		\item voxel $V=\{p_i=[x_i,y_i,z_i,r_i]\in \mathbb R^4 \}_{i=1,...t}, t<T$
		\item augment each point with its offset to the centroid $(v_x,v_y,v_z)$, the mean of $p\in V$ \\
		$\Rightarrow p'_i = [x_i,y_i,z_i,r_i,x_i-v_x, y_i-v_y, z_i-v_z]$
		\item $1\times1$ conv (with batch norm, ReLu) on each feature point: $p'_i\rightarrow f_i$
		\item element-wise max pooling on $f_i\in V$: voxel-wise feature $\hat f$
		\item augment each processed feature point with voxel feature: $f_i^\text{out}=[f_i,\hat f]$
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item stacked voxel feature encoding layer: deep net to extract feature for each voxel
		\item detection model: \hyperref[]{RPN} modified for $3$-D detection
		\end{itemize}
	\item Training
		\begin{itemize}
		\item per-voxel weighting: balanced according to num of positive example
		\item smooth L1 for regression loss
		\item further up-/down-sample positive/negative voxel in classification loss
		\item predicted box considered positive, if its IoU with label box $> 0.6$ \& is the highest \\
		(negative, if its IoU with any label box $<0.45$) \\
		(not-care, if its IoU with any label box $\in [0.45, 0.6]$)
		\item data augmentation: box (collision-free) perturbation in location, size, rotation
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item range: $[-3,1]\times[-20,20]\times[0,48]$ in height-width-length
		\item voxel feature extraction: voxel-level PointNet \\ 
		$\Rightarrow$ able to concat with various detection models (e.g. YOLO, etc.)
		\end{itemize}
	\end{itemize}
\item LMNet
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item cylindrical projection
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item given a pixel (corresponding to a lidar point) $p$ in a bounding box \\ 
		$\Rightarrow$ encode box under coord originated at $p$ \\
		$\Rightarrow$ axises: $x$ along radial direction; $y$ parallel with horizontal plane; $z$ accordingly
		\item encode $8$ corner $\Rightarrow$ $24$ channel encoding for each box
		\end{itemize}
	\item Classification Encoding
		\begin{itemize}
		\item per-pixel classification (car, pedestrian, cyclist, none)
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item per-pixel regression \& classification $\Rightarrow$ standard CV detection
		\item non-max suppression as postprocessing\\ 
		(score modified to be the num of nearby-box)
		\end{itemize}
	\item Structure
		\begin{itemize}
		\item convs - max pooling - dilated convs - unpooling - branch for regression/classification
		\end{itemize}
	\item Training
		\begin{itemize}
		\item point-wise weighting \\
		$\Rightarrow$ regression: consider box size for each class \\
		$\Rightarrow$ classification: downsample background pixel (standard)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item real-time timing due to simple structure with dilated conv \\
		(though performance hurt...)
		$\Rightarrow$ enable further fusion with image, pertaining real-time timing
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Point Cloud + Image Fusion}
\begin{itemize}
\item MV3D: Multi-view 3D Object Detection
	\begin{itemize}
	\item Preprocessing
		\begin{itemize}
		\item 
		\end{itemize}
	\item Bounding Box Encoding
		\begin{itemize}
		\item location
		\item size
		\item orientation
		\end{itemize}
	\item 3D Proposal Network
		\begin{itemize}
		\item bird-view of point cloud $\Rightarrow$ propose reliable 3D bbox
		\end{itemize}
	\item Region-based Fusion Network
		\begin{itemize}
		\item project 3D proposal to feature maps of multi-view
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Natural Language Processing}
\subsection{Language Representation} \label{DL_NLP_Langrep}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Language Token/Corpus
		\begin{itemize}
		\item words, sentences, paragraphs, ... $\Rightarrow$ can be language at various level
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Distributed Vector Representations as Embedding Matrix $E_{M\times N} = [e_1,...,e_N]$
		\begin{itemize}
		\item $e$ the column vectors, $M$ the desired embedding length, $N$ the total tokens num \\
		$\Rightarrow$ look up for the desired embedding \\
		(NOT using matrix multiplication due to sparsity from one-hot encoding)
		\item distributed representation: decomposed yet meaningful \\
		$\Rightarrow$ fight the curse of dimensionality
		\end{itemize}
	\item $\Rightarrow$ Meaningful Vector
		\begin{itemize}
		\item able to measure the (dis-)similarity of between tokens (words) \\
		$\Rightarrow$ semantic meaning: "Germany"-"Berlin" \& "France"-"Paris" \\
		$\Rightarrow$ syntactic meaning: "quick"-"quickly" \& "slow"-"slowly" \\
		(e.g. $e_\text{man} - e_\text{woman} \approx e_\text{king} - e_\text{queen}$, where $e_\text{text}$ the embedding for word "text") 
		\item $\Rightarrow$ allow NLP model to be more robust \& generalize better
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Problem of Bias
		\begin{itemize}
		\item word embedding reflect biases of text used to train the model \\
		e.g. "father-doctor" as "mother-nurse" $\Rightarrow$ gender bias
		\item $\Rightarrow$ can cause discrimination when making decision
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Overview}
\begin{itemize}
\item Character Embedding
	\begin{itemize}
	\item One-hot Encoding
		\begin{itemize}
		\item a one-hot vector with length $26$
		\end{itemize}
	\end{itemize}
\item Word Embedding
	\begin{itemize}
	\item Word Dictionary
		\begin{itemize}
		\item a collection of high-frequency word, embedded as one-hot vector
		\item special token \textless{}UNK\textgreater{} for unknown word
		\end{itemize}
	\item Features from Rule Model
		\begin{itemize}
		\item number at each vector location denotes the score for the word matching a rule \\
		(e.g. location for "is\textunderscore{}food" contains score $s\rightarrow1$ for "apple", $s\rightarrow0$ for "man")
		\end{itemize}
	\item Part-of-Speech (POS) Tag
		\begin{itemize}
		\item 
		\end{itemize}
	\item \hyperref[DL_NLP_Langrep_word2vec]{Word2Vec Embedding}
		\begin{itemize}
		\item construct supervised learning from UNlabeled corpus
		\end{itemize}
	\item \hyperref[DL_NLP_Langrep_GloVe]{Global Vector for Word Embedding (GloVe)}
		\begin{itemize}
		\item linear model with simple optimization goal
		\end{itemize}
	\item RNN Encoder
		\begin{itemize}
		\item apply RNN model as encoder on characters in the word \\
		$\Rightarrow$ no more <UNK> or unknown word
		\end{itemize}
	\end{itemize}
\item Sentence/paragraph Embedding
	\begin{itemize}
	\item RNN Encoder
		\begin{itemize}
		\item apply RNN model as encoder on words in the sentence \\ 
		(last hidden layer as encoding)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Word2Vec Embedding} \label{DL_NLP_Langrep_word2vec}
\begin{itemize}
\item N-gram Model
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item simple network to predict the $N+1^\text{th}$ word given previous $N$ words as input \\
		(e.g. using single softmax layer)
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item $E$ randomly initialized \& all words in corpus encoded in one-hot vector
		\item forward prop: word in one-hot $\rightarrow$ lookup $E$ $\rightarrow$ linear layer $\rightarrow$ softmax to predict
		\item training: update linear layer parameters \& matrix $E$ as weights \\ 
		(with cross-entropy loss)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn $P(t|c)$, where $t$ the target word, $c$ the previous $N$ context words
		\item setup a even larger training set from a large corpus
		\end{itemize}
	\item Generalization
		\begin{itemize}
		\item more context: take input from both previous and after words
		\item less \& close context: take only the last word as input $\Rightarrow$ 1-gram model
		\end{itemize}
	\end{itemize}
\item Skip-gram Model
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item choose only $1$ single word as context word \\
		$\Rightarrow$ balance sampling w.r.t. word frequency (e.g. prevent tons of "the", "a", ...)
		\item randomly choose other word(s) in the sentence as target word(s)
		\item $\Rightarrow$ to predict target word(s) given only context word as input \\ 
		$\Rightarrow$ learn word vector representations that are good at predicting the nearby words
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item same as N-gram model
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item harder supervised learning task, yet goal is to learn $E$
		\item better reflect the statistic: similar word appear in similar context \\
		(e.g. "soviet"-"union" appears much more often than "soviet"-"sasquatch") \\
		$\Rightarrow$ embedding for similar target word adjusted with similar gradients \\
		$\Rightarrow$ lie closer in vector space
		\item cons: softmax over large word dict $\Rightarrow$ low computation \\
		$\Rightarrow$ mitigated by hierarchical softmax, noise contrastive estimation (NCE)
		\end{itemize}
	\end{itemize}
\item Skip-gram with Negative Sampling
	\begin{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item choose a pair of context and target word $(c,t)$ as positive example
		\item generate $k$ negative examples by: same context word $c$ \& random word $t'$ as target
		\item given a pair of words, binary classification: is a (context, target) pair? \\
		$\Rightarrow$ distinguish valid target word from $k$ draws from noise distribution
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item detect meaningful $(c,t)$ pair/phrase by heuristic method \\ 
		e.g. if $c,t$ co-appear within 10-words distance more than a threshold, etc.
		\item $k=5-20$ for small train set; $k=2-5$ for large train set \\
		(larger noise to avoid overfitting)
		\item sample random word $t'$ from modified uniform distribution $\frac 1 Z U(t)^{3/4}$ over words
		\item subsample frequent words: sampled $t'$ discarded by probability $P(t') = 1-\sqrt{\frac {thr} {f(t')}}$ \\
		where $thr$ a threshold, $f(t')$ the frequency of $t'$ in corpus \\
		(to avoid meaningless words like "the", "a", etc.)
		\item $E$ randomly initialized \& all words in corpus encoded in one-hot vector \\
		(forward prop similarly)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item learn $P(y=1|c,t)$ via logistic regression \\ 
		$\Rightarrow$ much computationally affordable compared to giant softmax (less weights) \\
		$\Rightarrow$ much simpler approach than hierarchical softmax \& NCE
		\item non-linear model (logistic reg) also prefers linear structure of word embedding \\
		$\Rightarrow$ cosine distance still measures (dis-)similarity
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Global Vector for Word Embedding (GloVe)} \label{DL_NLP_Langrep_GloVe}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Context-Target Matrix $X$
		\begin{itemize}
		\item $x_{ij}$: the count of times word $w_i$ appear in the context of word $w_j$ \\
		(context definition can be non-symmetric)
		\end{itemize}
	\item Learning Objective Setup
		\begin{itemize}
		\item given embedding matrix $E$, minimize $\sum_{i,j}f(x_{ij})(\theta_i^Te_j-\log x_{i,j})^2$, \\
		where $e_j$ the embedding for $w_j$, $\theta_i$ the weights associated with $w_i$
		\item $f(x_{ij})$ a weighting term to balance infrequent-frequent words \\
		($f(x_{ij})$ for $x_{i,j}=0$, preventing $-\inf$ from $\log0$)
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item gradient decent directly optimize the simple objective
		\item final embedding for word $w, w_e = \frac 1 2 (e_w+\theta_w)$ \\
		$\Rightarrow$ as $\theta_w, e_w$ in objective interchangeable
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item directly model the linear structure in word representation \\ 
		(project input $e$ directly to output $\theta^T e$)
		\item final linear structure probably NOT align with human interpretable axis \\
		$\Rightarrow$ yet probably a combination of them (from a higher view)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Addressing Bias in Word Embedding}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Input Data
		\begin{itemize}
		\item a trained word embedding
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item identify the bias in embedding
		\item eliminate the bias if it appears in undesired places
		\end{itemize}
	\item Identify Bias Direction
		\begin{itemize}
		\item singular value decomposition to identify the axises where biases lie \\
		(similar to a PCA)
		\item e.g. principle component of $e_\text{man}-e_\text{woman}, e_\text{male}-e_\text{female}, ...$
		\end{itemize}
	\item Neutralize
		\begin{itemize}
		\item for all NOT definitional word (where bias should NOT appear) \\
		$\Rightarrow$ project to axises orthogonal to bias axises (to get rid of bias)
		\item e.g. project $e_\text{doctor}$ to the axises to reduce component in bias axises
		\end{itemize}
	\item Equalize Pairs
		\begin{itemize}
		\item for all definitional word (where bias should appear) \\
		$\Rightarrow$ adjust their distance towards non-definitional word to be the same \\
		(may train/handpick all definitional words, which is only a small set)
		\item e.g. make sure $d(e_\text{boy},e_\text{doctor}) = d(e_\text{girl},e_\text{doctor})$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Language Modeling}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item a word with sequence of characters
		\item a sentence with sequence of words/characters
		\item a paragraph with sequence of sentences/words/characters
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Probability Distribution
		\begin{itemize}
		\item model the appearance probability of the sequence $P(\text{z}^1,...,\text{z}^{t})$, \\
		where $z^t$ the token at time $t$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item at each time, output token distribution conditional on previous token(s) \\
		$\Rightarrow y^t = p(z^t|z^1,...,z^{t-1})$, where $z^t$ the token at time $t$
		\item $\Rightarrow$ sequence probability $\displaystyle P(z^1,...,z^{t}) = P(z^1)P(z^2|z^1)...P(z^T|z^1,...,z^{T-1})= \prod^T_{t}y^t$
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item $\mathbf 0$ vector as both (initial) hidden state \& input at time $0$ \\
		$\Rightarrow$ estimate $y^1=p(z^1)$, the distribution for being the $1^\text{st}$ token
		\item for time $t=2,...,T$, take input $x^2,...,x^T$ with hidden state $h^1,...,h^{T-1}$ \\ 
		$\Rightarrow$ estimate each conditional distribution (conditioning by passing hidden state)
		\end{itemize}
	\item Training
		\begin{itemize}
		\item for time $1$, input $x^1=\mathbf 0$, previous hidden state $h^0=\mathbf 0$
		\item for time $t=2,...,T$, input $x^t = {z^*}^{t-1}$ the true token of $t-1$ in the given sequence
		\end{itemize}
	\item Generative Model: Sampling New Sequence
		\begin{itemize}
		\item sampling the first token $\hat{z}^1$ according to the distribution $y^1$
		\item for $t=2,...$, take input $x^t=\hat{\text{z}}^{t-1}$, the token sampled from $y^{t-1}$
		\item until $t>T$ or the end signal sampled (e.g. the period "." in a sentence)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Masked Language Model}

\subsection{Name-Entity Recognition}
\subsubsection{}
\begin{itemize}
\item 
\end{itemize}

\subsection{Sentiment Classification}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item a sentence / paragraph
	\end{itemize}
\item Goal
	\begin{itemize}
	\item predict the degree of positive/negative attitude
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item small training set
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item Many-to-one RNN
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item each word encoded by word embedding
		\item RNN scanning through paragraphs
		\item last hidden layer as paragraph representation \& used to classify/regress
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Neural Machine Translation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence 
		\begin{itemize}
		\item typically sentence, can be also multiple sentences (paragraph)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item generated sentences in desired language
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN Encoder-Decoder
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item an RNN encodes the input sequence by its last hidden layer
		\item input encoding used as initial hidden state for decoder RNN
		\item decoder RNN generates (conditional) distribution over words at each step \\
		$\Rightarrow$ for time $t, y^t = p(z^t|x^1,...,x^{T_x}, \hat{z}^1,...,\hat{z}^{t-1})$, \\
		where $z^t$ the token at time $t$, $\hat{z}^1,...,\hat{z}^{t-1}$ the tokens chosen from $y^1,...,y^{t-1}$ \\
		($z^t$ a random variable, $\hat{z}^t$ a concrete assignment, $y^t$ a conditional distribution)
		\item unroll until stop sign generated
		\end{itemize}
	\item Understanding: Conditional Language Model
		\begin{itemize}
		\item decoder functions like language modeling, only different in its initial hidden state
		\item $\Rightarrow$ measure the conditional distribution $\displaystyle p(z^1,...,z^{T_y}|x^1,...,x^{T_x})=\prod_{t=1}^{T_y}y_t$, \\
		where $z^1,...,z^{T_y}$ the generated sequence, $x^1,...,x^{T_x}$ the input sequence
		\end{itemize}
	\item Improvement
		\begin{itemize}
		\item combined with attention model
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Choosing Output Sequence}
\begin{itemize}
\item Greedy Search
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item choose the words of highest (conditional) probability at each time step
		\end{itemize}
	\end{itemize}
\item Beam Search
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item with a vocabulary size of $N$, a bean with size $b$, input sequence $\mathbf x = x^1,...,x^{T_x}$
		\item to start, choose top-$b$ tokens (among $N$ tokens) at the $1^\text{st}$ step
		\item for step $t$, input each previous stored $b$ tokens to have $b$ conditional distributions
		\item choose the top-$b$ token pairs (among $b\times N$ pairs) regarding joint probability \\
		$\Rightarrow P(z^1,...,z^t|\mathbf x) = P(z^t|z^1,...,z^{t-1},\mathbf x)P(z^1, ..., z^{t-1}|\mathbf x)$ 
		\end{itemize}
	\item Normalization by Length
		\begin{itemize}
		\item reason: short sequence with less $y^t\in[0,1] \Rightarrow$ larger in general
		\item choose $t^\text{th}$ pair regarding the normalized probability $\frac 1 t P(z^1,...,z^t|\mathbf x)$
		\item $\Rightarrow$ more numerically stable
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item approximately search the sequence with highest joint (conditional) probability \\
		$\Rightarrow$ try to maximize $P(z^1,...,z^{T_y}|x^1,...x^{T_x})$
		\item similar to viterbi algorithm in HMM $\Rightarrow b = 1$ reduce to greedy search
		\item $B$ usually chosen in $10$ in research, $>1000$ in commercial system \\
		(still faster than BFS/DFS, yet no guarantee on finding best result)
		\end{itemize}
	\end{itemize}
\item 
\end{itemize}

\subsection{Speech Recognition}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item an audio sample, with each frame as a time step
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Sequence
		\begin{itemize}
		\item text (words/sentences) corresponding to the audio
		\end{itemize}
	\end{itemize}
\item Challenge
	\begin{itemize}
	\item Variable Timing
		\begin{itemize}
		\item output (letter/words) usually has much less time steps than input (audio frames) \\
		$\Rightarrow$ multiple input time steps corresponding to same output time step
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Learning Objective}
\begin{itemize}
\item Connectionist Temporal Classification (CTC) Loss
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item avoid learning boundaries and timings
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item two sequences considered equivalent if they differ only in alignment, ignoring blanks \\
		$\Rightarrow$ remove duplicate token (e.g. letters) from both sequence before comparison
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Classic Approach}
\begin{itemize}
\item RNN Encoder-Decoder
	\begin{itemize}
	\item Overview
		\begin{itemize}
		\item encoder scan through audio frames \& decoder output letter/punctuation/"blank"/"space"
		\item "blank": no symbol v.s. "space": delimiter for letters $\rightarrow$ words
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Trigger Word Detection}
\begin{itemize}
\item Goal
	\begin{itemize}
	\item trigger word: a specific predefined audio signal to invoke system (e.g. xiaodu xiaodu)
	\item detect where trigger word included in an audio (if any)
	\end{itemize}
\item Train Set Setup
	\begin{itemize}
	\item Basic
		\begin{itemize}
		\item $0$ for frames not corresponding to trigger word; $1$ for frames consisting trigger words
		\end{itemize}
	\item upsampling
		\begin{itemize}
		\item upsampling positive example: extends $1$ label a few frames after the trigger words \\
		(as trigger word often appears once in an interaction with system)
		\end{itemize}
	\end{itemize}
\item Classic Approach
	\begin{itemize}
	\item RNN Encoder-Decoder
		\begin{itemize}
		\item encoder scan through audio \& decoder output 0-1 classification at each step
		\end{itemize}
	\item Conv RNN
		\begin{itemize}
		\item a fixed window to better capture context for detecting trigger word
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Machine Reading Comprehension}
\subsubsection{RNN with Attention}
\subsubsection{Convolution with Self-attention - QAnet}

\subsection{Image Caption}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input as the target of description
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Natural Expression
		\begin{itemize}
		\item description of the image in natural language, e.g. English
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Neural Image Caption
	\begin{itemize}
	\item Visual Information
		\begin{itemize}
		\item encoded by CNN backbone into a $1$-D vector
		\end{itemize}
	\item Word Information
		\begin{itemize}
		\item a set of word selected beforehand
		\item word embedding performed
		\end{itemize}
	\item Language Generation
		\begin{itemize}
		\item generated by an LSTM decoder
		\item combining info: visual encoding as initial state of LSTM
		\item process: LSTM gives each word a to-be-selected probability at each time step
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item sampling: sample each word according to the distribution given by LSTM
		\item beam search: iteratively consider extending $k$ best sentence of length $t$ to $t+1$ \\
		$\Rightarrow$ select $k$ best sentence of length $t+1$ from all resulted sentences
		\end{itemize}
		(beam search selected in the paper)
	\end{itemize}
\end{itemize}


\subsection{Referring Segmentation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input for segmentation
		\end{itemize}
	\item Natural Language Expression
		\begin{itemize}
		\item expression to denote the interested object(s)/stuff(s)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Segmentation Mask of Referred Object(s)
		\begin{itemize}
		\item currently (till early 2019), mostly binary segmentation
		\end{itemize}
	\end{itemize}
\item Related Area
	\begin{itemize}
	\item NLP + CV
		\begin{itemize}
		\item referring localization
		\item image caption
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Segmentation from Natural Language Expressions
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item FCN-32s to encode the image into $2$-D feature maps (the last conv layer)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM to encode the sentence into $1$-D vector (the last hidden state)
		\end{itemize}
	\item Combining Info and Output
		\begin{itemize}
		\item per-pixel info: concat [coordinates of current pixel (coord info), language info]
		\item tile the per-pixel info into a feature map, then concat to the spatial info \\
		(per-pixel info concatenated at every pixel of spatial info)
		\item followed by a series of conv and finally a deconv for upsampling
		\end{itemize}
	\item Training
		\begin{itemize}
		\item per-pixel cross-entropy loss
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item special spatial info: coord of each pixel
		\item standard info combination: concatenation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no powerful spatial info encoder: FCN-32s instead of Resnet/Unet...
		\item weak upsampler, compared to encoder-decoder architecture
		\item language info comes late: after downsampling
		\item weak language info: only integrated once
		\end{itemize}
	\end{itemize}

\item Recurrent Multimodal Interaction for Referring Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder (Resnet as backbone, with atrous conv)
		\item then tiled (concat at every pixel) by coord info (coordinate of current pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item word embedding $w_t$ for $t=1,...,T$
		\item LSTM scanning the sentence, with hidden state $h_t$ at time $t$
		\item language info $l_t=$ concat [$h_t$, $w_t$]
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item $l_t$ tiled to spatial info, at each time step \\
		$\Rightarrow$ creating combined feature maps $F_t$ (of shape $[\text{height}, \text{wide}, \text{channel}])$
		\item combined feature maps $F_1,...,F_T$ fed to an convolutional LSTM, \\
		where the ConvLSTM shares weight over both space and time \\
		$\Rightarrow$ feature vector of $F_t[i,j]$ is the input of the ConvLSTM at time $t$ \\
		$\Rightarrow$ conv in ConvLSTM implemented as $1\times1$ conv
		\item a series of conv following the last hidden state of the ConvLSTM
		\end{itemize}
	\item Output
		\begin{itemize}
		\item bilinear interpolated to original input size
		\item optionally post-processed by dense CRF, using pydensecrf (hence inference only)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item more powerful spatial info extractor: DeepLab-101
		\item better language info: integrated at every time step, maintained by an ConvLSTM
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item weak architecture for spatial info: still no upsampling (blur segmentation)
		\item no spatial relation considered in ConvLSTM (?)
		\item weak language representation \\ 
		(better with pos tag, word2vec, word dict, biLSTM, and maybe even attention)
		\item language info still comes late: still after downsampling
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Current State-of-The-Art (early 2019)}
\begin{itemize}
\item Key-Word-Aware Network for Referring Expression Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder for comparability
		\item then tiled by coord info (coordinate of each pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, each hidden state as word info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item attention mask from combined info (spatial info with language info tiled) \\
		(at each time step)
		\item attention weighting over spatial info at each time step \\
		$\Rightarrow$ an $1$-D global encoding for each time step (via weighted mean over space) \\
		$\Rightarrow$ filling feature maps: global encoding if attention here $>$ threshold; else $\mathbf 0$ \\
		$\Rightarrow$ summing filled feature maps over time for the global spatial maps $c$
		\item attention weighting over tiled language info at each time step, correspondingly \\
		$\Rightarrow$ tiled language info maps summed over time for the global language maps $q$
		\item concat [spatial info, $c$, $q$], followed by $1\times1$ conv
		\end{itemize}
	\item Output
		\begin{itemize}
		\item upsampling performed
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item attention introduced: from combined info
		\item better combination: attention masked interact with both spatial \& language info
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item blur segmentation: no encoder-decoder architecture
		\item attention mask obtained sequentially: only last mask has complete language info
		\item language info comes late: after downsampling
		\end{itemize}
	\end{itemize}
\item Referring Image Segmentation via Recurrent Refinement Networks
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLabl ResNet-101 as encoder
		\item last feature maps tiled (concat at each pixel) with coord info
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, generating word info at each time step
		\item last hidden layer as language info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item combined info $=$ spatial info tiled with language info
		\item selecting set of feature maps from downsampling stages
		\item all selected feature maps resized and fed to $1\times1$ conv \\ 
		$\Rightarrow$ to match the dimensions of combined info
		\item convolutional LSTM applied to refine the combined info \\ 
		(with matched selected feature maps as input at each time step)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a conv after final hidden state of ConvLSTM for segmentation
		\item upsampled to original image size
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item ConvLSTM integrating info at dowsampling stage $\Rightarrow$ segmentation refined
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no upsampling: blur segmentation, mitigated by ConvLSTM though \\
		(yet no language info introduced in refinement)
		\item CNN fixed during training: relying on ConvLSTM
		\item single info combination: only by tiling \\
		(though, currently performing the best in all dataset)
		\end{itemize}
	\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Special Learning}
\subsection{Transfer Learning} \label{DL_Learning_Transfer}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Source Data
		\begin{itemize}
		\item a large amount of labeled data
		\item having different distribution then the desired target data
		\end{itemize}
	\item Target Data
		\begin{itemize}
		\item a small amount of labeled data, with a large amount of unlabeled data \\
		(due to hardness of labeling, etc.)
		\item from the distribution where model need to handle
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Model Performance
		\begin{itemize}
		\item good performance on val\&test set (containing target data)
		\item good generalization ability on the target distribution
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Standard Baseline}
\begin{itemize}
\item Pre-training \& Fine-tunning
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item distribution of source \& target data share some common features \\
		$\Rightarrow$ different task shares some common knowledge
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item train network on source data only
		\item swap/modify the last few layers (including prediction layer)
		\item retrain the last layer (limited target data) / all net (enough target data)
		\end{itemize}
	\item Guideline
		\begin{itemize}
		\item small dataset: freeze pretrained network \& use it as fixed feature extractor \\ 
		$\Rightarrow$ only train the last prediction layer
		\item medium dataset: freeze fewer layers, design some own last layers
		\item exceptionally large dataset with large computation budget: train from scratch \\
		$\Rightarrow$ pretrained weights as initialization (nor preferred in most cases)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item sharing weights/structure: low level feature extraction useful for both \\
		$\Rightarrow$ based on model ability
		\end{itemize}
	\end{itemize}
\item Transfer Ada Boost (trAdaBoost)
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item distribution of target \& source overlap more or less \\
		$\Rightarrow$ able to extract helpful guides from source data
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item setup train set with mixed target \& source data
		\item weighting example from target \& source differently: \\ 
		for source data weight $= \frac 1 {N_\text{source}}$; target data weight $= \frac 1 {N_\text{target}}$ \\
		$\Rightarrow$ target data more important (as smaller in number)
		\item for each weight-update iteration (may contain multiple epochs), update the weight: \\
		$\Rightarrow$ shift the weight (importance) towards target data \& normalize all the weight
		\end{itemize}
		$\Rightarrow$ based on data distribution
	\item Understanding
		\begin{itemize}
		\item learn the shared feature/knowledge with the help of source data
		\item focus more on target as making progress
		\end{itemize}
	\end{itemize}
\item Feature Projection
	\begin{itemize}
	\item Assumption
		\begin{itemize}
		\item few or NO overlap between source \& target (as data examples directly)
		\item source \& target can be mapped onto a shared feature space, where overlap can be discovered
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item project/map the source \& target data onto the same feature space
		\item transfer learning in the shared space
		\end{itemize}
		$\Rightarrow$ based on distribution transformation
	\item Understanding
		\begin{itemize}
		\item try discover common feature through transformation \\ 
		(may need a decoder to map back to desired output space)
		\end{itemize}
	\item Example
		\begin{itemize}
		\item \hyperref[DL_NLP_Langrep]{word embedding}: learn word relation as unsupervised learning \\
		$\Rightarrow$ a shared feature space to represent word
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Multi-task Learning}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Multi-labeled Data
		\begin{itemize}
		\item one input data corresponds to multiple desired outputs
		\item $\Rightarrow$ require similarity/common knowledge in different tasks \\
		(e.g. object detection for multiple object types)
		\end{itemize}
	\item Partial-labeled Data
		\begin{itemize}
		\item desired outputs may not be all labeled in the input \\ 
		(i.e. some may be missed)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item General Solution to Multi-task
		\begin{itemize}
		\item give all desired outputs from a single network
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Standard Baseline}
\begin{itemize}
\item Single Networks with Multiple Predictions
	\begin{itemize}
	\item Sharing
		\begin{itemize}
		\item shared low-level layers to extract features from the input
		\item shared loss as a sum over all prediction for corresponding label
		\item shared training as back-prop computed as a single network
		\item shared input data as trained together \\
		$\Rightarrow$ shared knowledge discovered when training on data for other tasks
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item each task help each other, by contributing to the common knowledge
		\item overcome data shortage: augmented by data for other tasks
		\item partial labeled still useful: help train the shared layers
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{K-shot Learning}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Training Set
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\section{Interpretable Machine Learning}

\section{Tactile Sensing}
\subsection{Tactile Sensor}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Single-point Contact Sensors (single tactile cells)
		\begin{itemize}
		\item able to confirm object-sensor contact
		\item force sensor to detect force at contact point
		\item biomimetic whiskers (dynamic tactile sensors) to detect vibration at contact point
		\end{itemize}
	\item High Resolution Tactile Arrays (fingertips)
		\begin{itemize}
		\item sensors array of tactile sensing elements (usually planar array)
		\item sensing elements: fiber optics, embedded camera, barometers, etc.
		\end{itemize}
	\item Large-Area Tactile Sensors (body skins)
		\begin{itemize}
		\item usually able to be curved for large robotic skin
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsection{Tactile Sensing and Perception}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input Data
	\begin{itemize}
	\item Tactile Data
		\begin{itemize}
		\item the data after on-board processing on the raw tactile sensor
		\item come with different form/meaning, depending on the sensor
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Detail Object Recognition
		\begin{itemize}
		\item recognize the object shape, material properties, in-hand pose and etc.
		\item action related: grasp control, slippage detection, etc.
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Feature Extraction}
\begin{itemize}
\item Acoustic Signal
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item microphone to collect signal from tapping/sliding/scratching
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item filter/function (e.g. fast Fourier transform) to map to frequency domain
		\item then analyzed/classified into texture properties
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fast, low-cost, light-burden
		\item need interaction
		\end{itemize}
	\end{itemize}
\item Vibration and Force Data
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item robot finger sliding with various speed to detect vibration/friction
		\item strain gauges sensor to detect stiffness, compliance, elasticity and etc.
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item transform into frequency domain, then analyze the vibration intensities
		\item or, directly use kNN, SVM, etc on mechanical impedance data to classify
		\end{itemize}
	\end{itemize}
\item Micro-structure Patterns
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item tactile sensors array for pressed/imprinted surface
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item sensor data with spatial relation $\Rightarrow$ a small image (usually $14\times6$) \\
		$\Rightarrow$ image processing (with hand-crafted/learned features)
		\item learning features: end-to-end learned for various task
		\end{itemize}
	\end{itemize}
\item Spatial-Temporal Data
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item a series acquired local tactile data, each with sensor-object contact location \& time
		\end{itemize}
	\item Processing
		\begin{itemize}
		\item viewed as a spatial points cloud, then recognize the object contour
		\item incorporate with LSTM algorithms: e.g. landmarks, grid based, particle filter, etc.
		\item incorporate sequential model: e.g. KF to sequentially refine object properties
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Application in Robot Tasks}
\begin{itemize}
\item Robot Manipulation
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item image from vision sensor, data from tactile sensor
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item successfully perform task involving object manipulation \\
		e.g. open a door via door handle
		\end{itemize}
	\item Single-Point Contact
		\begin{itemize}
		\item structure as spatial-temporal points
		$\Rightarrow$ in-hand pose estimation (usually with LSTM methods)
		\item challenge: sensor-object contact influence the environment \\
		(use lidar?)
		\end{itemize}
	\item Tactile Arrays
		\begin{itemize}
		\item image processing \& SLAM
		\end{itemize}
	\item Tactile \& Vision
		\begin{itemize}
		\item 3D vision (from lidar/camera) $\Rightarrow$ sensor fusion for localization
		\item vision for haptic\&location estimation + tactile for refinement
		\item vision \& tactile both treated as image: potentially sharing weights (multi-tasking)
		\end{itemize}
	\end{itemize}
\item Sensor Fusion for Environment Modeling
	\begin{itemize}
	\item Contact Verification
		\begin{itemize}
		\item verify robot-object contact
		\item active searching ambiguous region in visual perception
		\item jointly estimate likelihood of contact at each location in the environment
		\end{itemize}
	\item Object Properties Refinement
		\begin{itemize}
		\item model the in-hand object as in robot manipulation (can be end-to-end) \\
		e.g. estimate the angle of door handler while gripping
		\end{itemize}
	\end{itemize}
\end{itemize}