\chapter{Deep Learning}

\section{Interview of Fame}

\subsection{Geoffrey Hinton}
\subsubsection{Knowledge Embedding}
\begin{itemize}
	\item BP
	\begin{itemize}
		\item psychology view: knowledge in vectors
		\item semantic AI: knowledge graph
		\item BP algorithm can interpret \& convert between feature vector and graph representation (with some embedding)
	\end{itemize}
	\item Boltzmann Machine
	\begin{itemize}
	\item Leaning Algorithm on Density Net
		\begin{itemize}
		\item same information in forward \& backward propagation to learn feature embedding
		\end{itemize}
	\item Restricted Boltzmann Machine (RBM)
		\begin{itemize}
		\item ways of learning in deep dense net with fast inference
		\item iterative learning (adding layer after the above trained)
		\item ReLU $\Leftrightarrow$ a stack of sigmoid functions (approximately) in RBM
		\item ReLU units initialized to identity for efficient learning
		\end{itemize}
	\end{itemize}
	
	\item EM
		\begin{itemize}
		\item EM with Approximate E Step
		\end{itemize}
	
	\item vs. Symbolic AI
		\begin{itemize}
		\item Symbolic AI: symbolic logic-like expression to do reasoning
		\item yet, maybe state vector to represent knowledge
		\end{itemize}
\end{itemize}

\subsubsection{Brain Science}
\begin{itemize}
\item Brain: Nets Implemented by Evolution
	\begin{itemize}
	\item trying to train without BP
	\item doing BP (get derivatives) with re-construction error (auto-encoder)
	\end{itemize}
\end{itemize}

\subsubsection{Memory in Nets}
\begin{itemize}
\item Fast Weights for Short-term Memory
\item Capsule Net
	\begin{itemize}
	\item structured knowledge representation in each unit (feature with sets of property)
	\item $\Rightarrow$ enable nets to vote rather than filtering - thus better generalization
	\end{itemize}
\end{itemize}

\subsubsection{Unsupervised Learning}
\begin{itemize}
\item Importance
	\begin{itemize}
	\item better than human eventually (as supervised learning has limited maximum)
	\item GAN as a breakthrough
	\end{itemize}
\end{itemize}

\subsubsection{"Slow" Feature}
\begin{itemize}
\item Non-linear Transform to Find Linear Transform
	\begin{itemize}
	\item find a latent representation containing linear transform to do the work
	\item e.g. change viewpoints: pixels $\rightarrow$ coordinates $\rightarrow$ linear transform $\rightarrow$ back to pixels
	\end{itemize}
\end{itemize}

\subsubsection{Relations between Computers}
\begin{itemize}
\item showing computer data to work
	\begin{itemize}
	\item instead of programming it to work
	\end{itemize}
\end{itemize}


\subsection{Pieter Abbeel}
\subsubsection{Deep Reinforcement Learning}
\begin{itemize}
\item Overall Challenge 
	\begin{itemize}
	\item Representation
	\item Exploration Problem
	\item Credit Assignment
	\item Worst Case Performance
	\end{itemize}
\item Advantage (Deep Nets in RL)
	\begin{itemize}
	\item network capturing the representation (state vector)
	\end{itemize}
\item Question in DRL
	\begin{itemize}
	\item how to learn safely
	\item how to keep learning (under small negative samples) e.g. better than human
	\item can we learn the reinforcement learning program (RL in the RL)
	\item long time horizon
	\item use experience across tasks
	\end{itemize}
\item Success of DRL
	\begin{itemize}
	\item simulated robot inventing walking... $\Rightarrow$ single general algorithms to learn
	\end{itemize}
\end{itemize}

\subsection{Ian Goodfellow}
\subsubsection{Generative Adversarial Networks}
\begin{itemize}
\item Generative Models
	\begin{itemize}
	\item Resembling
		\begin{itemize}
		\item trained to optimized the distribution behind training data \\ 
		(then sampled from that distribution to get more imaginary training data)
		\item $\Rightarrow$ produce data to resemble the training data
		\end{itemize}
	\item Usage
		\begin{itemize}
		\item semi-supervised learning
		\item data augmentation
		\item simulating scientific experiment
		\end{itemize}
	\item Previous Ways
		\begin{itemize}
		\item Boltzmann Machine
		\item Sparse Coding
		\end{itemize}
	\item Now: Generative Adversarial Networks (GANs)
	\item Future
		\begin{itemize}
		\item increase reliability of GANs (stabilizing)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Yoshua Bengio}
\subsubsection{Thoughts}
\begin{itemize}
\item Fallacy
	\begin{itemize}
	\item Smoothness in Nonlinearity
		\begin{itemize}
		\item to ensure non-zero gradients every where
		\end{itemize}
	\end{itemize}
\item Surprising Fact
	\begin{itemize}
	\item ReLU in Deep Net
		\begin{itemize}
		\item inspired initially by biological connection
		\end{itemize}
	\end{itemize}
\item \textbf{Distribution v.s. Symbolic Representation}
	\begin{itemize}
	\item Distributed Representation 
		\begin{itemize}
		\item distributed in lots of units, instead of a symbolic representation in a single cell \\
		(agree on Geoffrey Hinton)
		\end{itemize}
	\item Curse of Dimensionality
		\begin{itemize}
		\item neural net's distributed representation for joint distribution over random variables
		\end{itemize}
	\item $\Rightarrow$ Word Embedding
		\begin{itemize}
		\item generalized to joint distribution over sequence of words
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Works}
\begin{itemize}
\item Piecewise Linear Activation (PLU)
\item Unsupervised Learning
	\begin{itemize}
	\item Focus
		\begin{itemize}
		\item Denoising auto-encoder
		\item GANs
		\end{itemize}
	\item Importance
		\begin{itemize}
		\item Human Ability
			\begin{itemize}
			\item self-teaching, building world-model from perception
			\end{itemize}
		\item Unsupervised Learning + Reinforcement Learning
			\begin{itemize}
			\item underlying concept across two fields: machine can learn through interactions \\
			$\Rightarrow$ learning "good" representation (yet, what is "good")
			\end{itemize}
		\end{itemize}
	\item Possibility
		\begin{itemize}
		\item Loss Function
			\begin{itemize}
			\item not even defined for each task (not knowing which is good for what?)
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Machine Translation (Founder)
	\item Generalized into Other Fields
	\end{itemize}
\item Back-prop in Brains (Neural Science
	\begin{itemize}
	\item Reasons for Efficiency of Backprop
	\item Larger Family behind Credit Assignment
	\end{itemize})
\end{itemize}

\subsection{Yuanqing Lin}
\subsubsection{National Deep Learning Lab}
\begin{itemize}
\item Paddle Paddle
\item Baidu Lab
\end{itemize}

\subsection{}
\subsubsection{title}
\begin{itemize}
\item 
\end{itemize}

\subsection{Research}
\subsubsection{Topics}
\begin{itemize}
\item Unsupervised Learning
\item Reinforcement Learning
\item AI Security
	\begin{itemize}
	\item Anti Inducing
		\begin{itemize}
		\item NOT to be fooled/induced to do unappropriated things \\
		(even if algorithm is right)
		\end{itemize}
	\item Built-in Security
	\end{itemize}
\item Fairness in AI
	\begin{itemize}
	\item Dealing Societal Issue
	\item Reflecting Preferred Bias
	\end{itemize}
\item \textbf{Auto Optimization (Hyperparameter Tunning)}
	\begin{itemize}
	\item Swarm Optimization
	\item Expectation Maximization
		\begin{itemize}
		\item target variable $\theta=$ hyperparameters
		\item hidden variable $Z=$ weights of network
		\item data $X=$ dataset
		\end{itemize}
	$\Rightarrow$ 
		\begin{itemize}
		\item E-step: evaluate $\displaystyle \mathbb E_{Z|\theta_n,X}(\ln P(Z,X|\theta))$
			\begin{itemize}
			\item $\ln P(Z, X|\theta)$: log likelihood of hyperparam $\theta$ (for weights \& data to be observed)
			\item $P(Z|\theta_n, X)$: posterior of weights $Z$ 
			\end{itemize}
		$\Rightarrow$ evaluate (approximate) the expectation of the log likelihood of hyperprarm $\theta$ \\
		(from a functional view, train with $\theta_0-\theta_N$, evaluate model $M$ times in training, thus with weights $Z_{00}-Z_{NM}$) \\
		$\Rightarrow$ a matrix with $n$ as row entry, $m$ as column entry, mapping to both $\ln P(Z, X|\theta), P(Z|\theta_n, X)$
		$\Rightarrow$ then marginalize (taking the expectation) over $Z$, to get a (sampled) function over $\theta$
		\item M-step: maximize the result function from E-step
			\begin{itemize}
			\item fit a curve \& maximize w.r.t hyperparams $\theta$
			\end{itemize}
		\end{itemize}
	\end{itemize}
\item World Understanding: after perception
	\begin{itemize}
	\item Unsupervised Learning + Reinforcement Learning
		\begin{itemize}
		\item machine learns from interactions
		\item machine builds a representation of world (like human ability, without fine label)
		\end{itemize}
	$\Rightarrow$ building world-model from perception
	\item Causality Mining
	\end{itemize}
\item Model Interpretation
	\begin{itemize}
	\item Logical Formalization
		\begin{itemize}
		\item deep learning can be understood logically \\
			e.g. what make deep net training harder? understand the limit of current algorithm/model and \textbf{why}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Advises}
\begin{itemize}
\item Learning Direction
	\begin{itemize}
	\item Math
		\begin{itemize}
		\item statistic
		\item linear algebra
		\item calculus
		\item optimization
		\end{itemize}
	\end{itemize}
\item Reading
	\begin{itemize}
	\item read a little bit \& find somewhere intuitively not right
	\begin{itemize}
		\item good intuition: eventually work; \\ 
		bad intuition: not working no matter what it is doing
		\item if other doubts your idea as bullshit $\Rightarrow$ a sign for real good result
	\end{itemize}
	\item a supervisor with similar belief
	\item PhD vs. Company
		\begin{itemize}
		\item amount of mentoring
		\item faster if dedicated supervisor available
		\end{itemize}
	\end{itemize}

\item Practice
	\begin{itemize}
	\item open-source learning resource
	\item implement the paper
	\item work on a projected and open source it \\
	$\Rightarrow$ the stage (e.g. github) will bring people to you
	\item implement the tools: to find out how \& why it works \\
	$\Rightarrow$ derive theories from the origin
	\end{itemize}
\end{itemize}

\section{Basic Neutral Network}

\subsection{Advantages}
\subsubsection{Large/Big Data}
\begin{itemize}
\item Larger Maximum Capability
	\begin{itemize}
	\item Curve given Amount of Data
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/background-largedata".png}
	\end{figure}
	\item Reasons
		\begin{itemize}
		\item the scale of data (labeled)
		\item the scale of neural network (computability)
		\item the scale of efficiency: e.g. ReLu, faster parallel algorithm
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Flexibility}
\begin{itemize}
\item Different Structures for Different Tasks
	\begin{itemize}
	\item Same Data \& Task
		\begin{itemize}
		\item changing settings/structures of deep learning model can make a difference \\
		(v.s. SVM, etc.)
		\end{itemize}
	\end{itemize}

\item Ability to Choose Basis Functions
	\begin{itemize}
	\item Functional View
		\begin{itemize}
		\item $\displaystyle y(\mathbf {x}, \mathbf w)=f(\mathbf w^T\phi(\mathbf x)), \text{ where } \phi \text{ is basis function }, f(\cdot) \text{ is net as a function}$
		\end{itemize}
	\item Learning $\phi$: choose embedding $\Rightarrow$ choose basis function
	\item Learning $\mathbf w$: choose which feature / basis functions more useful
	\end{itemize}
	
\item Solving Bias-Variance Trade-off
	\begin{itemize}
	\item Complexity + Data/Regularization
		\begin{itemize}
		\item easy complexity via depth, size \\
		$\Rightarrow$ reduce bias, without hurting variance by utilizing big data
		\item easy regularization via L$2$ ant etc.\\ 
		$\Rightarrow$ prevent high variance without hurting bias much in a deep/big net
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Power of Depth}
\begin{itemize}
\item Deep Representation
	\begin{itemize}
	\item Low-level $\rightarrow$ High-level
		\begin{itemize}	
		\item multiple layers to choose \& combine useful information (creating new feature/basis) \\
		$\Rightarrow$ next layer use chosen/combined simple basis to build more complex one
		\item $\Rightarrow$ an hierarchy from low-level information to high-level information
		\end{itemize}
	\end{itemize}

\item Circuit Theory
	\begin{itemize}
	\item Power of Combination
		\begin{itemize}
		\item functions that can be compactly represented by a depth $k$ architecture might require an exponential number of computational nodes using a depth $k-1$ architecture \\
		(from the perspective of factorization)
		\end{itemize}
	\end{itemize}
\textbf{Yet, start from the SHALLOW (logistic regression) before trying the deep}
\end{itemize}

\subsection{Problem}
($n$ units in one hidden layer)

\subsubsection{Weight-space Symmetries} 
\begin{itemize}
\item Symmetries in Activation Function
	\begin{itemize}
	\item $\mathcal{O}(2^n)$, e.g. $\arctan(-x) = -\arctan(x) \Rightarrow$ changing signs of all input \& output has the same mapping (reduce effective data)
	\end{itemize}
\item Positional Combination in One Layer
	\begin{itemize}
	\item $\mathcal{O}(n!)$ exchange unit with each other (together with their input output weights) $\Rightarrow$ mapping stay the same
	\end{itemize}
\end{itemize}
$\Rightarrow \mathcal O(n!2^n)$ overall weight-space symmetries

\subsubsection{High-Dimension Search Space}
\begin{itemize}
\item Multiple Critical Points
	\begin{itemize}
	\item Symmetries
		\begin{itemize}
		\item at least $\mathcal O (n!2^n)$ critical points ($\nabla E(w) = 0$), where $E(w)$ is error function \\
		due to weight-space symmetries
		\end{itemize}
	\item Saddle Points
		\begin{itemize}
		\item both the bottom (in one dimension) and the top for another
		\item due to high-dimension weight space \\ 
		$\Rightarrow$ more likely to have functions being convey/convex in different dimensions
		\end{itemize}
	\item Local Optima
		\begin{itemize}
		\item less then saddle points in amount, due to high-dimension weight space \\ 
		e.g. usually $\ge 10^4$-D for modern deep nets
		\end{itemize}
	\end{itemize}
\item Plateaus
	\begin{itemize}
	\item a large flat region where gradient $\rightarrow 0$ \\
	$\Rightarrow$ gradient descent slowly down the flat surface (before exiting)
	\item $\Rightarrow$ slow down gradient descent significantly
	\end{itemize}
\item Expensive in Finding Critical Point
	\begin{itemize}
	\item expensive for even local optima with gradient decent
	\item as expensive as $\mathcal O(n^3)$ if using Laplace approximation
	\end{itemize}	
\end{itemize}

\subsubsection{Gradient Vanishing/Exploding}
\begin{itemize}
\item Gradient Vanishing
	\begin{itemize}
	\item Saturated Function
		\begin{itemize}
		\item sigmoid/tanh function: gradient $\rightarrow 0$ when input $\rightarrow \pm \infty$
		\end{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item with depth $L$, each activation (e.g. tanh) output $a^l < 1$ and weight $\mathbf w^l <1$ \\ 
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'<1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $<1$ \\
		$\Rightarrow$ gradient exponentially decayed in back-prop
		\end{itemize}
	\end{itemize}
\item Gradient Exploding
	\begin{itemize}
	\item Exponential Effect
		\begin{itemize}
		\item similarly, each activation (e.g. ReLU) output $a^l>1$ and weight $\mathbf w^l > 1$ \\
		$\Rightarrow y(\mathbf x, W) \approx \mathbf \mathbf w^L \mathbf w'^{L-1} \mathbf x$, with $\mathbf w'>1$ \\
		$\Rightarrow$ all the gradient along the way get multiplied by number $>1$ \\
		$\Rightarrow$ gradient exponentially augmented in back-prop
		\end{itemize}
	\end{itemize}
\item Possible Solutions
	\begin{itemize}
	\item Random Initialization
		\begin{itemize}
		\item \hyperref[Init_Xavier]{Xavier Initialization}: for gradient vanishing \& exploding
		\end{itemize}
	\item Activation 
		\begin{itemize}
		\item \hyperref[Act_ReLU]{ReLU}: for gradient vanishing
		\end{itemize}
	\end{itemize}	
\end{itemize}

\subsection{Learning}
\subsubsection{Forward-Backward Propagation}
\begin{itemize}
\item Representation
	\begin{itemize}
	\item Layers
		\begin{itemize}
		\item input layer
		\item hidden layer(s): layer with NO ground truth (for the associated weights) available \\
		note: input \& hidden layers have associated biases as well (usually)
		\item output layer
		\end{itemize}
	\item Neuron (Unit)
		\begin{itemize}
		\item $s_l$: num of units in layer $l$
		\item $w^l$: weight matrix of mapping from layer $l$ to $l+1$, with shape of $\left( s_{l+1}, s_l + 1 \right)$
		\item $h(\cdot)$: activation function (usually shared)
		\item $a_j^l$: activation output of unit $j$ at layer $l$
		\item $z_j^l$: output of unit $j$ at layer $l$ \\ 
		(represent parameterized basis, also the input for layer $l+1$)
		
		\end{itemize}
	\item Intuition
		\begin{itemize}
		\item all stacked vertically (vertical vector) \\
		$\Rightarrow$ horizontally for different examples; vertically for different units
		\end{itemize}
	\end{itemize}
	
\item Forward Propagation (Inference)
	\begin{itemize}
	\item Activation $a^{j+1} = w^j \cdot [z_0^j, ..., z_{s_j}^j]^T, \text{ with } z_0=1$
	\item Unit Output $z^{j+1} = h(a^{j+1}) = [z_1^{1}, ..., z^{j+1}_{s_{j+1}}]^T$
	\end{itemize}

\item Backward Propagation
	\begin{itemize}
	\item Loss $\mathcal L(W) = $
	\end{itemize}

\item Practice of Back Prop
	\begin{itemize}
	\item Caching Intermediate Result
		\begin{itemize}
		\item naturally cached: input $a^0=x$, weights matrix $w$ and bias $b$
		\item activation input/output $a/z$ \\
		(since will be used in back-prop)
		\end{itemize}
	\item Auto Difference
		\begin{itemize}
		\item achievement: calculate the derivatives along the forward prop \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Gradient Descent}
\begin{itemize}
\item Batch Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate on entire training set; then update weights
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item largest optimization every time
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item greedy optimizing
		\item slow \& memory demanding on large dataset
		\end{itemize}
	\end{itemize}
\item Stochastic Gradient Descent
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item shuffle data to have training set $X_\text{train}$, further split into $X_\text{train}^{1}, ..., X_\text{train}^{T}$
		\item train the net iteratively with $\forall t\in[1,T], X_\text{train}^t$ \\
		i.e. one mini-batch for a gradient descent (weights update)
		\item after training through all $T$ batches, an epoch of training is finished \\
		$\Rightarrow 1$ epoch = $1$ full scan of training set
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item faster: more weight upgrade over the same amount of data
		\item better chance to reach global change: not greedy anymore
		\item more affordable for training in GPU memory
		\end{itemize}
		$\Rightarrow$ preferred choice
	\item Cons
		\begin{itemize}
		\item observing noisy training loss: not monotonically decreasing (but overall decreasing)
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Recommended Practice}

\subsection{Tunning Hyperparameters}
\subsubsection{Hyperparameters}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Structures and Architectures
		\begin{itemize}
		\item type of layers and size of layers
		\item type of activation
		\item depth of networks
		\item so on...
		\end{itemize}
	\item Learning
		\begin{itemize}
		\item learning rate
		\item optimizer (learning process)
		\end{itemize}
	\end{itemize}
\item Challenges
	\begin{itemize}
	\item NO Consistent Prescience
		\begin{itemize}
		\item popular choices from one domain usually NOT carry over to other domains
		\end{itemize}
	\item 
	\end{itemize}
\end{itemize}

\subsubsection{Systematic Searching}
\begin{itemize}
\item Random Sampling
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item NOT able to know the importance of different hyperparameters \\
		$\Rightarrow$ not wasting grid search step on the unimportant
		\item NOT able to know the effective range of a hyperparameters \\
		$\Rightarrow$ may be skipped by grid search step
		\item decouple the search for different hyperparameters $\Rightarrow$ more richly explore \\
		(whereas grid search fix one while searching on others)
		\end{itemize}
	\item Coarse to Fine Scheme
		\begin{itemize}
		\item explore whole space uniformly (equally random)
		\item exploit region where good results show up (with more densely sampled)
		\end{itemize}
	\item Sampling on Scale
		\begin{itemize}
		\item instead of sampling the value of hyperparameter, sample the scale of it \\
		e.g. sampling learning rate $\alpha = 10^r, r\sim U(-4,0)$
		\item $\Rightarrow$ distribute the density across desired scale\textbf{s} \\ 
		(by using transfered scale, e.g. applying $\log, e^x$ e.t.c)
		\item reason: depends on the 
			\begin{itemize}
			\item use of hyperparameter e.g. in an exponential/linear/log way
			\item whether intend to sample on scale e.g. across one or more scales
			\end{itemize}
		\end{itemize}
	\end{itemize}

\item Swarm Optimization
	\begin{itemize}
	\item Intuition
		\begin{itemize}
		\item searching over a space with continuous$\times$discrete across various scale \\ 
		$\Rightarrow$ encoded into a list
		\item search using permutation / group behavior \\ 
		$\Rightarrow$ inherently imposing explore-exploit strategy
		\end{itemize}
	\item Popular Framework
		\begin{itemize}
		\item genetic algorithm (GA)
		\item particle swarm optimization (PSO)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Tunning Practice}
\begin{itemize}
\item Single Model
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item supervising one model at a time
		\item interactively justify the hyperparameter in training process  \\
		$\Rightarrow$ gain knowledge through interaction \& ensure a good performance \\
		$\Rightarrow$ early feedback
		\end{itemize}
	\item Reason
		\begin{itemize}
		\item too many data (online advertisement, computer vision etc.)
		\item few computing resource
		\end{itemize}
	\end{itemize}
\item Parallel Training
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item shoot out multiple model with various settings
		\item compare at the end (after trained \& evaluated)
		\end{itemize}
	\item Reason
		\begin{itemize}
		\item small data/model, enough computability / fast training process
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Designing Networks}
\subsubsection{}


\section{Operations \& Layers Structure}
\subsection{Operations in Network}
\subsubsection{Activations}
\begin{itemize}
\item Sigmoid $a=\sigma(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mapping to $(0,1)$, with $\sigma(0)=0.5$
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item gradient vanishing: $\displaystyle \sigma(z)' = \sigma(z)(1-\sigma(z)) \Rightarrow \lim_{z\rightarrow \pm \infty} \sigma(z)' \rightarrow 0$ \\
		(as the gradient passed through (via chain rule) $=\frac{a}{z} \frac{z}{w}$)
		\end{itemize}
	\end{itemize}
\item Tangent $a=\tanh(z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item empirically, almost always better than sigmoid (in hidden layers)
		\item maps to $(-1,1)$, with $\tan(0)=0 \Rightarrow$ help centering data ($0$-mean) \\ $\Rightarrow$ make the learning of next layer easier
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item still, gradient vanishing when $z\rightarrow \pm \infty$
		\end{itemize}
	\end{itemize}
\item Rectified Linear Unit (ReLU) $\max(0, z)$ \label{Act_ReLU}
	\begin{itemize}
	\item Derivation: approximated by a stack of sigmoid
		\begin{itemize}
		\item 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate gradient vanishing: $\forall z>0, a=z \Rightarrow$ learn much faster
		\end{itemize}
		$\Rightarrow$ the default choice!
	\item Cons
		\begin{itemize}
		\item undefined behavior at $x=0$ (actually, gradient becomes the sub-gradient)
		\item gradient totally vanished for $x<0$
		\item $\Rightarrow$ dead units: weights learned/initialized to always output negatives \\ 
		$\Rightarrow$ activation always output $0$ \\
		$\Rightarrow$ the unit always output $0$
		\end{itemize}		
	\end{itemize}
\item Leaky Relu $a=\max(\alpha z, z), \alpha \rightarrow 0^+$ (e.g. $\alpha=0.01$)
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item mitigate the gradient vanishing problem for $(-\infty, +\infty)$
		\item avoid dead units problem
		\end{itemize}
		(yet not that popular as ReLU)
	\end{itemize}

\item Piecewise Linear Unit (PLU) $a=\max(\alpha(z+\beta)-\beta, \min(\alpha(z-\beta)+\beta, z)$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item hybrid of tanh \& ReLU: three linear pieces approximating tanh in a given range \\
		\item more expressive than ReLU: more nonlinear, better to fit smooth nonlinear function
		\item mitigate gradient vanishing problem: due to linearity
		\end{itemize}
	\item Cons
	\end{itemize}

\item Linear (Identity) Activation $a=z$
	\begin{itemize}
	\item Pros
		\begin{itemize}
		\item used in regression to output real number $\in (-\infty, +\infty)$
		\item used in compression net
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item stacked units with linear activation $\Leftrightarrow$ single linear transformation
		\item logistic regression with linear activation in hidden layer is NO more expressive than logistic regression with no hidden layer \textbf{!}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Normalization in Network}
\begin{itemize}
\item Batch Normalization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for an activation in hidden layer with input $z$, a batch with size $N_b$
		\item calculate the mean of current batch $\displaystyle \mu=\frac 1 {N_b} \sum_n z_n$, where $z_n$ for the $n^{th}$ example 
		\item calculate the deviation of current batch $\displaystyle \sigma = \sqrt{\frac 1 {N_b} \sum_n(z_n-\mu)^2}$
		\item normalize to be $z'_n = \frac {z_n-\mu}{\sigma}$
		\item allow model to recover/manipulate original distribution: $\hat z_n=\gamma z'_n + \beta$, \\
		where $\gamma, \beta$ being trainable (updated by optimizer using gradients)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item preferred to apply batch norm on $z$ (before activation), instead of after it
		\item for math stability, $z'_n=\frac {z_n-\mu}{\sigma+\epsilon}$, with $\epsilon\rightarrow 0+$
		\item (usually) with mini-batch, calculate the mean \& variance from only the mini-batch
		\item with batch norm, original bias $b$ in calculating $z=wx+b$ becomes pointless \\
		$\Rightarrow$ integrated into the $\beta$ in batch norm
		\item at test time ($1$ example a time): need an estimation for $\mu, \sigma$ \\
		$\Rightarrow$ exponentially weighted average over $\beta, \sigma$ in training time
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item normalize the intermediate data to have $0$ mean, unit variance \\
		$\Rightarrow$ to speed up the training from some hidden layers (as normalization does)
		\item remain the ability to transfer the data to have other mean \& variance \\
		(controlled by $\gamma,\beta$)
		\item control the distribution of data in hidden layer \\ 
		$\Rightarrow$ suppress the change of input data distribution for the layer after it \\
		$\Rightarrow$ increase robustness for later layers, against covariate shift \\ 
		(from both the weight update in early layers and the input data change)
		\item regularize the net by adding noise to the input data of hidden layer \\ 
		(due to computing mean/variance only on mini-batch) \\
		$\Rightarrow$ enforce robustness against noise, hence unintended slight regularization effect \\
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Operations on Network}
\subsubsection{Initialization}
\begin{itemize}
\item Random Initialization for Weights
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item weights initialized to a random variable in a small range e.g. $(-0.03, 0.03)$
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid symmetry problem: \\
		if identical initialization for weights $\Rightarrow$ units in same layer computing exactly same function \\
		$\Rightarrow$ get the same learning step propagated back \\
		$\Rightarrow$ then always compute exactly the same function (by induction)
		\item avoid gradient vanishing: especially for gradient of sigmoid/tanh activation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item NOT concern various nets: sampling in a fixed range may not work for all nets
		\end{itemize}
	\end{itemize}
\item Xavier Initialization for Weights \label{Init_Xavier}
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item set $\forall l\in [1,L], \text{Var}(w^l) = \frac 1{n_l}$ for tanh, $\frac 2{n_l}$ for ReLU, \\
		where $n_l$ is the number of unit in layer $l$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item draw random variable $r\sim \mathcal N(0,1)$
		\item set each of $w^l=r\cdot \sqrt{\frac 2 {n_l}}$ for ReLU, $r\cdot \sqrt{\frac 2 {n_l}}$ for tanh \\ 
		or $r\cdot \sqrt{\frac 2 {n_{l-1}+n_l}}$ proposed by 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item theoretically justified to initialized weights to be around $\pm 1$ \\
		$\Rightarrow$ mitigate gradient vanishing\& exploding problem statistically 
		\end{itemize}
	\end{itemize}
\item Zero Initialization for Bias
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item default to use $0$ bias \\
		(can NOT used for weights as explained)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Regularization}
\begin{itemize}
\item $L2$ Regularization
	\begin{itemize}
	\item Understanding
		\begin{itemize}
		\item forcing weights to be smaller
			\begin{itemize}
			\item single node has smaller effect
			\item input of activation closer to $0$ \\
			$\Rightarrow$ activation becomes more linear-alike (e.g. sigmoid, tanh) \\
			$\Rightarrow$ layers perform more linear-alike transformation
			\end{itemize}
		$\Rightarrow$ simpler network, less able to fit extreme curly decision boundary \\
		(hence less able to overfit)
		\end{itemize}
	\end{itemize}
	
\item $L1$ Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each weight ww we add the term $\lambda \abs w$ to the objective. It is possible to combine the L1 regularization with the L2 regularization: $\lambda_1 \abs w + \lambda_2 w^2$ (this is called Elastic net regularization). The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the “noisy” inputs. In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Dropout Regularization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for each of selected units, set a drop probability \\
		i.e. for each forward/back-prop, nodes are "dropped" according to the probability \\
		$\Rightarrow$ for each time, a randomly reduced net is trained
		\end{itemize}
	\item Implementation: Inverted Dropout
		\begin{itemize}
		\item set a keep prob $k$ instead of drop prob, for a selected layer
		\item generate random numbers for all units \& turned into a boolean "keep" vector $\mathbf k$
		\item dropped activation $\mathbf d = \mathbf a \times \mathbf k$ (element-wise), \\
		 where $\mathbf a$ is original activation output vector from the layer
		\item $\Rightarrow$ activation becomes $0$ for dropped units in $\mathbf d$
		\item scaling up by dividing the keep prob: $\mathbf d / k$ \\
		$\Rightarrow$ so that expected output value of each activation remains the same
		\item test time: no dropout $\Rightarrow$ no random output \& consider all robust features learned\\
		(randomness in training, mitigated by big data)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item can NOT rely on any one feature $\Rightarrow$ have to spread out weights \\
		$\Rightarrow$ results in shrinking the squared norm of weights (as $L2$)
		\item used on layers with enormous features as input (e.g. computer vision) \\
		$\Rightarrow$ reduce the chance of relying on small set of features
		\end{itemize}
	\begin{figure}[ht]
	\includegraphics[width=0.5\linewidth, center]{"./Deep Learning/regularization-dropout".jpeg}
	\end{figure}
	\item Cons
		\begin{itemize}
		\item training loss may have bigger glitch $\Rightarrow$ harder to debug \\
		(make sure loss decreasing before introduced dropout)
		\end{itemize}
	\end{itemize}

\item Max norm constraints
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $w^l_n$ of every neuron to satisfy (). Typical values of cc are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot “explode” even when the learning rates are set too high because the updates are always bounded.
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item 
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}

\item Early Stopping
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item stop the training at lowest validation loss (with training loss decreasing) \\
		$\Rightarrow$ at the start point of overfitting
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item evaluate both train \& val loss, saving models along the way \\
		$\Rightarrow$ use the model corresponding to the start of overfitting
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item at relatively early stage, weights are still relatively small \\ 
		(due to random initialization in $[0^-, 0^+]$)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item couples task of optimizing loss and task of not overfitting \\
		$\Rightarrow$ no longer one task at a time
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Optimization}
\begin{itemize}
\item Gradient Descent with Momentum
	\begin{itemize}
	\item Definition: exponentially weighted average
		\begin{itemize}
		\item calculate the gradient for weight update: $dW'_t = \beta dW'_{t-1} + (1-\beta) dW_t$, \\ 
		where $dW$ the original gradient
		\item $\Rightarrow$ average over past gradients with exponentially decaying weight, \\
		$\Rightarrow$ for past $k\in[0,K]$ gradient, coefficient becomes $\beta(1-\beta)^k$ \\ 
		(with $k=0$ denoting current gradient)
		\item bias correction: avoid slow start \\
		(due to: gradient $dW_0$ initialized to $0$ \& not enough gradients for averaging) \\
		$\Rightarrow$ set $dW_t=\frac {dW_t}{1-\beta^t}$ in the early stage \\
		(after starting stage, bias correction $\rightarrow 0$ for large $t$)
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item approximation: weighted average over past $K=\frac 1 {1-\beta}$ gradients \\
		due to $(1-\epsilon)^{1/\epsilon} \approx \frac 1 e$, recognized as small enough \\
		$\Rightarrow$ discard gradients with further exponentially small weights
		\item apply element-wise multiplication on gradients and pre-calculated coefficient
		\item sum up to be the gradient for weight update \\ 
		(include bias correction term if necessary, yet often omitted)
		\item note: $dW'_t = \beta dW'_{t-1}+dW_t$ is another version, yet discouraged \\
		(coupling momentum $\beta$ with learning rate $\alpha$, as $\alpha$ needs to cooperate)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item averaging/smoothing out the regular oscillation in stochastic gradient descent \\
		$\Rightarrow \beta$ popularly chosen to be $0.9$ (averaging over last $10$ gradients) 
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item avoid some regular oscillation (slowing down the training \& not true randomness)
		\end{itemize}
	\end{itemize}
	
\item Root Mean Square Propagation (RMS prop)
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $S_t = \beta S_{t-1} + (1-\beta) dW_t^2$ ($S_0$ initialized to $0$),\\ 
		where $dW^2$ the original gradient being element-wisely squared\\
		$\Rightarrow$ exponentially weighted square of gradients
		\item calculate the gradient for weight update $dW'_t=\frac {dW_t}{\sqrt{S_t}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item calculate $S_t$ similarly (as an exponentially weighted average)
		\item $\sqrt{S_t}$ becomes $\sqrt{S_t+\epsilon}$, where $\epsilon\rightarrow 0^+$ for mathematical stability
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item for gradients with large variance in training $\Rightarrow S_t$ large $\Rightarrow \frac 1{\sqrt{S_t}}$ small \\ 
		$\Rightarrow$ weighted less, hence stabilized (as it should be noisy \& taking smaller step)
		\item for gradients with small variance \\ 
		$\Rightarrow$ weighted more, encouraged (as it should be on the "trend" towards optimum)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item recognize trend from noise via variance of their gradient $\Rightarrow$ speedup training
		\item auto-fixing learning rate for each weight given the recorded behavior \\ 
		(protect learning process from a too large learning rate)
		\end{itemize}
	\end{itemize}
	
\item Adaptive Momentum (Adam) Optimization Optimization
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item compute $M_t = \beta_1 M_{t-1} + (1-\beta_1) M_t$ as momentum
		\item compute $S_t = \beta_2 S_{t-1} + (1-\beta_2) dW_t^2$ as root mean square
		\item apply bias correction on both: $M'_t=\frac {M_t}{1-\beta_1^t}, S'_t=\frac {S_t}{1-\beta_2^t}$
		\item $\Rightarrow$ calculate gradient for update $dW'_t=\frac {M'_t} {\sqrt{S'_t+\epsilon}}$, where $\epsilon\rightarrow 0^+$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item implement $M_t,S_t$ as momentum and root mean square \\
		(popular choice: $\beta_1=0.9, \beta_2=0.999, \epsilon=10^{-8}$)
		\item do implement bias correction
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item combine momentum with root mean square \\
		$\Rightarrow$ for each weight
			\begin{itemize}
			\item smooth out regular oscillation
			\item encourage the trend \& adapt learning rate given history record
			\end{itemize}
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item effective for a large range of problem
		\end{itemize}
	\end{itemize}

\item Learning Rate Decay
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item update learning rate $\alpha = \frac 1 {1+r\cdot e}$, where $r$ the decay rate, $e$ the epoch number
		\item other decay formula:
			\begin{itemize}
			\item exponential decay: $\alpha=r^e\cdot\alpha_0$, where $\alpha_0$ the base learning rate
			\item $\alpha=\frac k {\sqrt{e}} * \alpha_0$, where $k$ a constant
			\end{itemize}
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item set learning rate for each epoch, or after some global steps
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item fast learning at the beginning, more cautious when approaching the optimum \\ 
		$\Rightarrow$ in order to finally converge
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Layers}

\subsubsection{Prediction Layer}
\begin{itemize}
\item Sigmoid Layer
\item Softmax Layer
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item arbitrary input $\mathbf z^{L}$ being logits, containing multiple multi-class predictions $z^{L}$ \\
		$\Rightarrow$ each prediction being the same dimension as one-hot encoded label
		\end{itemize}
	\item Output
		\begin{itemize}
		\item probabilistic-alike prediction $\mathbf a^L$, with the same shape as the input (logits)
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item for $K$ classes to predict $\Rightarrow$ $\dim (z^L)=K$
		\item for each dimension $k\in[1,K]$, compute $\displaystyle a^L_k=\frac{e^{(z^L_k)}}{\displaystyle \sum_{k=1}^K e^{(z^L_k)}}$
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item vecotrize the exponential computation $\hat z^L = \exp{(z^L)}$
		\item compute normalization $N=\displaystyle \sum_{k=1}^K {\hat z^L_k}$
		\item normalize as $a^L=\frac 1N \hat z^L$
		\item maximum likelihood with softmax: $\displaystyle L = \frac 1N \sum_{\mathbf Y}-\mathbf y^T \cdot \log \hat {\mathbf y}$, \\
		where $\mathbf y$ the one-hot encoded label, $\hat {\mathbf y}$ the prediction \\
		$\Rightarrow$ easy gradients: $dz^L = \hat{\mathbf y} - \mathbf y$, where $z^L$ the logits (vector)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item contrasting the hard-max function (non differentiable): $a_k = 1 \text{ if } \displaystyle \arg\max_k(z); \text{ else } 0$
		\item exponentially normalizing the output of arbitrary net into probabilistic form \\
		(reduced to logistic for binary class i.e. $K=2$) \\
		$\Rightarrow$ generalize logistic prediction to $K$-class prediction
		\item for maximum likelihood loss, only the gap with true class generate gradients \\ 
		(due to one-hot encoding) \\ 
		$\Rightarrow$ trying to predict the class true with higher probability
		\end{itemize}
	\end{itemize}
\item Normalization Layer
\end{itemize}

\subsubsection{Convolution Layer}
\begin{itemize}
\item Normal Convolution
\item Atrous Convolution
\item Deconvolution
\end{itemize}

\subsubsection{Pooling Layer}
\begin{itemize}
\item Normal Pooling
\item Unpooling
\item Spatial Pyramid Pooling (SPP)
\item Region of Interest Pooling (RoI Pooling)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item feature maps from CNN
		\item RoIs i.e. proposal region (from selective search etc.) projected on feature map
		\end{itemize}
	\item Operation
		\begin{itemize}
		\item divide each RoI with grid of desired size (proportional to the RoI size)
		\item max pooling from each cell
		\end{itemize}
		$\Rightarrow$ single-size SPP for each RoI
	\item Output
		\begin{itemize}
		\item a fixed size feature maps for each RoI
		\end{itemize}
	\end{itemize}
\end{itemize}


\section{Architectures}
\subsection{Encoder-Decoder Architecture}
\subsubsection{Description}
\begin{itemize}
\item Encoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item downsample/encode input into rich feature maps/vectors
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual input: CNN backbone
		\item natural expression input: RNN backbone
		\end{itemize}
	\end{itemize}
\item Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item upsample/decode rich feature maps back to the original size
		\item actually, impose requirement onto the encoder
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item visual output: CNN backbone
		\item natural expression output: RNN backbone
		\end{itemize}
	\end{itemize}
\item Connection
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item combine high level information with low level information
		\item image $\rightarrow$ image: outline refinement ...
		\item language $\rightarrow$ language: sentence style capturing
		\end{itemize}
	\item Implementation
		\begin{itemize}
		\item concatenation
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Extension}
\begin{itemize}
\item Multiple Encoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item project different information into the same space
		\item combine those information via some shared layers at the end
		\end{itemize}
	\end{itemize}
\item Multiple Decoder
	\begin{itemize}
	\item Functionality
		\begin{itemize}
		\item impose multiple requirements to the encoder (via auxiliary loss)
		\end{itemize}
	\end{itemize}
\end{itemize}




\section{Deep Learning Framework}
\subsection{}




\section{Advanced Topic}
\subsection{Machine Reading Comprehension}
\subsubsection{Problem Formulation}
\begin{itemize}
\item 
\end{itemize}

\subsubsection{RNN with Attention}
\subsubsection{Convolution with Self-attention - QAnet}

\subsection{Image Caption}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input as the target of description
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Natural Expression
		\begin{itemize}
		\item description of the image in natural language, e.g. English
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Neural Image Caption
	\begin{itemize}
	\item Visual Information
		\begin{itemize}
		\item encoded by CNN backbone into a $1$-D vector
		\end{itemize}
	\item Word Information
		\begin{itemize}
		\item a set of word selected beforehand
		\item word embedding performed
		\end{itemize}
	\item Language Generation
		\begin{itemize}
		\item generated by an LSTM decoder
		\item combining info: visual encoding as initial state of LSTM
		\item process: LSTM gives each word a to-be-selected probability at each time step
		\end{itemize}
	\item Inference
		\begin{itemize}
		\item sampling: sample each word according to the distribution given by LSTM
		\item beam search: iteratively consider extending $k$ best sentence of length $t$ to $t+1$ \\
		$\Rightarrow$ select $k$ best sentence of length $t+1$ from all resulted sentences
		\end{itemize}
		(beam search selected in the paper)
	\end{itemize}
\end{itemize}


\subsection{Referring Segmentation}
\subsubsection{Problem Formulation}
\begin{itemize}
\item Input
	\begin{itemize}
	\item Image 
		\begin{itemize}
		\item visual input for segmentation
		\end{itemize}
	\item Natural Language Expression
		\begin{itemize}
		\item expression to denote the interested object(s)/stuff(s)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Segmentation Mask of Referred Object(s)
		\begin{itemize}
		\item currently (till early 2019), mostly binary segmentation
		\end{itemize}
	\end{itemize}
\item Related Area
	\begin{itemize}
	\item NLP + CV
		\begin{itemize}
		\item referring localization
		\item image caption
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsubsection{Baseline Approach \& Previous Work}
\begin{itemize}
\item Segmentation from Natural Language Expressions
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item FCN-32s to encode the image into $2$-D feature maps (the last conv layer)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM to encode the sentence into $1$-D vector (the last hidden state)
		\end{itemize}
	\item Combining Info and Output
		\begin{itemize}
		\item per-pixel info: concat [coordinates of current pixel (coord info), language info]
		\item tile the per-pixel info into a feature map, then concat to the spatial info \\
		(per-pixel info concatenated at every pixel of spatial info)
		\item followed by a series of conv and finally a deconv for upsampling
		\end{itemize}
	\item Training
		\begin{itemize}
		\item per-pixel cross-entropy loss
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item special spatial info: coord of each pixel
		\item standard info combination: concatenation
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no powerful spatial info encoder: FCN-32s instead of Resnet/Unet...
		\item weak upsampler, compared to encoder-decoder architecture
		\item language info comes late: after downsampling
		\item weak language info: only integrated once
		\end{itemize}
	\end{itemize}

\item Recurrent Multimodal Interaction for Referring Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder (Resnet as backbone, with atrous conv)
		\item then tiled (concat at every pixel) by coord info (coordinate of current pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item word embedding $w_t$ for $t=1,...,T$
		\item LSTM scanning the sentence, with hidden state $h_t$ at time $t$
		\item language info $l_t=$ concat [$h_t$, $w_t$]
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item $l_t$ tiled to spatial info, at each time step \\
		$\Rightarrow$ creating combined feature maps $F_t$ (of shape $[\text{height}, \text{wide}, \text{channel}])$
		\item combined feature maps $F_1,...,F_T$ fed to an convolutional LSTM, \\
		where the ConvLSTM shares weight over both space and time \\
		$\Rightarrow$ feature vector of $F_t[i,j]$ is the input of the ConvLSTM at time $t$ \\
		$\Rightarrow$ conv in ConvLSTM implemented as $1\times1$ conv
		\item a series of conv following the last hidden state of the ConvLSTM
		\end{itemize}
	\item Output
		\begin{itemize}
		\item bilinear interpolated to original input size
		\item optionally post-processed by dense CRF, using pydensecrf (hence inference only)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item more powerful spatial info extractor: DeepLab-101
		\item better language info: integrated at every time step, maintained by an ConvLSTM
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item weak architecture for spatial info: still no upsampling (blur segmentation)
		\item no spatial relation considered in ConvLSTM (?)
		\item weak language representation \\ 
		(better with pos tag, word2vec, word dict, biLSTM, and maybe even attention)
		\item language info still comes late: still after downsampling
		\end{itemize}
	\end{itemize}

\subsubsection{Current State-of-The-Art (early 2019)}
\item Key-Word-Aware Network for Referring Expression Image Segmentation
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLab-101 as encoder for comparability
		\item then tiled by coord info (coordinate of each pixel)
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, each hidden state as word info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item attention mask from combined info (spatial info with language info tiled) \\
		(at each time step)
		\item attention weighting over spatial info at each time step \\
		$\Rightarrow$ an $1$-D global encoding for each time step (via weighted mean over space) \\
		$\Rightarrow$ filling feature maps: global encoding if attention here $>$ threshold; else $\mathbf 0$ \\
		$\Rightarrow$ summing filled feature maps over time for the global spatial maps $c$
		\item attention weighting over tiled language info at each time step, correspondingly \\
		$\Rightarrow$ tiled language info maps summed over time for the global language maps $q$
		\item concat [spatial info, $c$, $q$], followed by $1\times1$ conv
		\end{itemize}
	\item Output
		\begin{itemize}
		\item upsampling performed
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item attention introduced: from combined info
		\item better combination: attention masked interact with both spatial \& language info
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item blur segmentation: no encoder-decoder architecture
		\item attention mask obtained sequentially: only last mask has complete language info
		\item language info comes late: after downsampling
		\end{itemize}
	\end{itemize}
\item Referring Image Segmentation via Recurrent Refinement Networks
	\begin{itemize}
	\item Spatial Info
		\begin{itemize}
		\item DeepLabl ResNet-101 as encoder
		\item last feature maps tiled (concat at each pixel) with coord info
		\end{itemize}
	\item Language Info
		\begin{itemize}
		\item LSTM scanning sentence, generating word info at each time step
		\item last hidden layer as language info
		\end{itemize}
	\item Combining Info
		\begin{itemize}
		\item combined info $=$ spatial info tiled with language info
		\item selecting set of feature maps from downsampling stages
		\item all selected feature maps resized and fed to $1\times1$ conv \\ 
		$\Rightarrow$ to match the dimensions of combined info
		\item convolutional LSTM applied to refine the combined info \\ 
		(with matched selected feature maps as input at each time step)
		\end{itemize}
	\item Output
		\begin{itemize}
		\item a conv after final hidden state of ConvLSTM for segmentation
		\item upsampled to original image size
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item ConvLSTM integrating info at dowsampling stage $\Rightarrow$ segmentation refined
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item no upsampling: blur segmentation, mitigated by ConvLSTM though \\
		(yet no language info introduced in refinement)
		\item CNN fixed during training: relying on ConvLSTM
		\item single info combination: only by tiling \\
		(though, currently performing the best in all dataset)
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsubsection{Research Direction}
\begin{itemize}
\item Integrating Encoder-Decoder Architecture
	\begin{itemize}
	\item Upsampling
		\begin{itemize}
		\item similar to Unet, concat low-level spatial info
		\item introduce language info as well \\
		(e.g. early combination, explicit introducing, ...)
		\end{itemize}
	\end{itemize}
\item Early Info Combination
	\begin{itemize}
	\item Tiling at First Conv
		\begin{itemize}
		\item downsampling more responsible for language info processing \\
		$\Rightarrow$ hopefully get more fine-tuning alone with conv filters
		\item can be used with pre-trained net: \\ 
		$ReLU(conv_1*X_1 + conv_2*X_2) = ReLU( [conv_1,conv_2]*[X_1,X_2] )$
		\end{itemize}
	\item Multiple Entries
		\begin{itemize}
		\item combining info at different stages of downsampling / upsampling
		\end{itemize}
	\end{itemize}
\item Attention
	\begin{itemize}
	\item Attention from Combined Info
		\begin{itemize}
		\item as key-word-aware net
		\end{itemize}
	\item Attention on Language Info
		\begin{itemize}
		\item $1$-D spatial pyramid pooling / attention mask on the sentence encoding
		\end{itemize}
	\end{itemize}
\item Language Info Throughout Network
	\begin{itemize}
	\item Encoder-Decoder for Language Info
		\begin{itemize}
		\item network asked to recover language info after processing combined info \\ 
		(potentially via a separate branch only at training time) \\ 
		$\Rightarrow$ auxiliary loss
		\end{itemize}
	\item Language as Conv Filter
		\begin{itemize}
		\item Language Info, through a subnet, becoming a set of conv filters \\
		$\Rightarrow$ then imposed in downsampling, tunnel, upsampling or bridge stage(s)
		\end{itemize}
	\end{itemize}
\item Data Augmentation
	\begin{itemize}
	\item Translation Module
		\begin{itemize}
		\item using the same image
		\item expression translated to a middle language and then back to English \\
		$\Rightarrow$ language info trained more finely
		\end{itemize}
	\end{itemize}
\end{itemize}
