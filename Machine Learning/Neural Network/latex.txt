\hypertarget{basic-neural-networks}{%
\subsubsection{Basic Neural Networks}\label{basic-neural-networks}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Goal:

  \begin{itemize}
  \tightlist
  \item
    Learn basis function(s) rather than manually choose

    \begin{itemize}
    \tightlist
    \item
      Generalized Linear Model:
      \(\displaystyle h_\theta(x)=f(\phi(x)\theta), \text{ where $\phi$ is basis function, f($\cdot$) is activation function}\)\\
    \item
      Let \(\phi(x)\) depend on parameters, and then adjust parameters
      together with \(\theta\)
    \end{itemize}
  \item
    Complex non-linear hypotheses

    \begin{itemize}
    \tightlist
    \item
      hard to train: too many features with their polynomial terms
      (computer vision)
    \end{itemize}
  \end{itemize}
\item
  Problem in Neural Networks:

  \begin{itemize}
  \item
    Weight-space Symmetries (\(n\) units in one hiden layer)

    \begin{itemize}
    \item
      Occure when activation function for hidden units has symmetries
      \(\Rightarrow\) \(\mathcal O (2^n)\)

      \(\small \text{Ex. $\arctan(-a)=-\arctan(a)$: changing sign of all input & output units has the same mapping}\)
    \item
      In one hidden layer, exchange unit with each other (along with
      their inout \& output weights).

      Mapping stay the same \(\Rightarrow \mathcal O(n!)\)
    \item
      \(\Rightarrow \mathcal O(n!2^n)\) overall weight-space symmetries
    \end{itemize}
  \item
    Error function not convex

    \begin{itemize}
    \item
      At least \(\mathcal O(n!2^n)\) critical points (
      \(\small\nabla E(w)=0, \text{ where $E(w)$ is Error function}\))
    \item
      \(\Rightarrow\) Can be expensive in finding even local minimum
      with Gradient Descent

      ​ \(\mathcal O(n^3)\), using Laplace Approximation
    \end{itemize}
  \item
    Deep networks result in gradient vanishing

    \begin{itemize}
    \tightlist
    \item
      But functions that can be compactly represented by a depth k
      architecture might require an exponential number of computational
      nodes using a depth k-1 architecture
    \item
      Potential solution: Xavier initialization, ReLU Activations
      \(h(x)=\max(x,0)\)
    \end{itemize}
  \end{itemize}
\item
  Representation:

  \begin{itemize}
  \tightlist
  \item
    input layer (layer \(1\)), hidden layer(s), output layer

    \begin{itemize}
    \tightlist
    \item
      each layer has \(x_0=1\) as bias unit
      \(\small\text{(except for output layer)}\)
    \end{itemize}
  \item
    \(s_l\) = num of units in layer \(l\)
  \item
    \(z_i^j\) = output of unit \(i\) in layer \(j\)
    (\(\text{represents parametrised basis}\))
  \item
    \(\Theta^j\) = matrix of weights controlling function mapping from
    layer \(j\) to layer \(j+1\)

    \begin{itemize}
    \tightlist
    \item
      with \(s_j\)units in layer \(j\) and \(s_{j+1}\) units in layer
      \(j+1\), \(\Theta^j\) is of dimension \(s_{j+1}\times(s_j+1)\)
    \end{itemize}
  \end{itemize}
\item
  Forward propagation:

  \begin{itemize}
  \tightlist
  \item
    Activation
    \(a^{j+1} = \Theta^j \cdot [z_0^j,z_1^j,...,z_{s_j}^j]^T, \space z_0^j = 1\)
  \item
    Activation function \(h(\cdot)\) to get output of a hidden units

    \begin{itemize}
    \tightlist
    \item
      Need to be nonlinear
    \item
      \textbf{ReLU activation}: \(h(x) = \max(x,0) \Rightarrow\) help
      avoiding saturation (gradient vanishing)
    \end{itemize}
  \item
    \(z^{j+1} = h(a^{j+1})=[z_1^{j+1},z_2^{j+1},...,z_{s_{j+1}}^{j+1}]^T\)
  \end{itemize}
\item
  \textbf{Backward propagation}:

  \begin{itemize}
  \item
    \(\displaystyle J(\Theta)=-\frac{1}{m}\sum^m_{i=1}\sum^K_{k=1}[ y_k^i log((h_\Theta(x^i))_k) + (1-y_k^i)log(1-(h_\Theta(x^i))_k)] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l+1}}\sum_{j=1}^{s_l}(\Theta_{i,j}^l)^2\)

    \begin{itemize}
    \tightlist
    \item
      sum the cost for all k categories
    \item
      \(s_l\): num of units in \(l\)th layer; \(\Theta^l\):
      \(s_{l+1}\times(s_i+1)\) =\textgreater{} in
      \(\Theta_{i,j},\space i\neq0\)
    \item
      can introduce regularised error
      \(\widetilde J = J(\Theta)+\frac \lambda 2 \Theta^T\Theta\)
    \end{itemize}
  \item
    \(\displaystyle \frac{\partial J(\Theta)}{\partial \Theta_{i,j}^l} = \frac{\partial J(\Theta)}{\partial a^{l+1}_i}\frac{\partial a^{l+1}_i}{\partial\Theta^l_{i,j}} = \delta_i^{l+1}z_j^l\)
    ,
    \(\text{where } \delta_i^l=\frac{\partial J(\Theta)}{\partial a_i^l}\)

    \begin{itemize}
    \tightlist
    \item
      \(\displaystyle \delta_i^{l+1} = h'(a_i^{l+1}) \sum_{k=1}^{s_{l+1}} \delta_k^{l+2}\Theta_{ki}^{l+1}\)
    \item
      Intuition:
      \(\text{Gradient$=$output error $\delta \space \times$ input associatd with $\Theta_{i,j}$}\)
    \end{itemize}
  \item
    Efficiency:
    \(\mathcal O(n), \text{where $n$ is the num of parameters in all $\Theta$}\)

    \begin{itemize}
    \tightlist
    \item
      whereas numerical differentiation
      (\(\small \frac{J(\Theta_{i,j}+\epsilon) - J(\Theta_{i,j}-\epsilon)}{2\epsilon}\))
      need \(\mathcal O(n^2)\)
    \end{itemize}
  \item
    \textbf{Algorithm}:

    \begin{itemize}
    \item
      \(\forall l,i,j,\space \Delta^l_{ij}:=0\)
    \item
      For \(i=1\) to \(m\)

      ​ \(a^1:=x^i\), compute \(a^l,\space l=2,...,L\) (forward)

      ​ \(\delta^L:=a^L-y^i\), compute \(\delta^{L-1},...,\delta^2\)
      (backward, \(a,y^i\) are column vectors )

      ​ \(\Delta^l_{ij} += a_j^l\delta_i^{l+1}\) (vectorization:
      \(\Delta^l+=\delta^{l+1}(a^l)^T\))

      ​
      \(\begin{equation} D_{i,j}^l=   \begin{cases} \frac1m(\Delta_{i,j}^l + \lambda\Theta_{i,j}^l) & j\neq0, \\  \frac1m(\Delta_{i,j}^l & j=0. \end{cases}  \end{equation}\)
    \item
      \(D_{i,j}^l=\frac{\partial J(\Theta)}{\partial \Theta^l_{i,j}}\)
    \end{itemize}
  \end{itemize}
\item
  Initialization:

  \begin{itemize}
  \item
    Symmetry problem:\includegraphics{./Symmetry Problem .png}

    \begin{itemize}
    \tightlist
    \item
      so that units in the same layer are computing the same features
    \end{itemize}
  \item
    Random initialization:
    \(\Theta_{i,j}^l := random[-\epsilon, \epsilon]\)

    \begin{itemize}
    \item
      Default choice:
      \(\displaystyle \epsilon=\frac{\sqrt{6}}{\sqrt{s_l + s_{l+1}}}\),
      \(s_l\) is num of units in layer \(l\)
    \item
      Matlab/Octave: \(\Theta = rand(m, n) * 2\epsilon - \epsilon\)

      ​ \(rand(m, n)\) =\textgreater{} m*n matrix with randome num
      between 0,1
    \end{itemize}
  \item
    \textbf{Xavier Initialization}:

    \begin{itemize}
    \item
      Avoid saturation \(\text{ (gradient vanishing)}\)
    \item
      For each layer, assume: Input \(x_i \sim \mathcal N(0,1)\),
      weights \(w_i\sim \mathcal N(0,\sigma^2)\) and activation
      \(a=\sum x_iw_i\)

      \(\Rightarrow Var(a)=\mathbb E(a-\mathbb E(a))^2=\mathbb E(a^2)=\sum\mathbb E ((x_iw_i)^2)= \sum\mathbb E(x_i^2)\mathbb E(w_i^2)=m\sigma^2 \\ \Rightarrow \text{ Set }\sigma^2 = \frac 1 m \\ \text{, where } m \text{ is total num of input }x_i\)
    \end{itemize}
  \end{itemize}
\item
  Training

  \begin{itemize}
  \item
    Gradient checking:
    \(D_{i,j} \approx \frac{J(\Theta_{i,j}+\epsilon) - J(\Theta_{i,j}-\epsilon)}{2\epsilon}\)

    \begin{itemize}
    \tightlist
    \item
      turn checking off when running the backpropagation algorithm
    \end{itemize}
  \item
    Healthy update process:

    \begin{itemize}
    \item
      update scale = learning rate * gradient

      \$ =\textgreater{}
      \frac {\text{update scale}} {\text{parameter scale}}
      \approx 0.001\$
    \end{itemize}
  \end{itemize}
\item
  Network architecture

  \begin{itemize}
  \tightlist
  \item
    Num of input units: dimension of eatures in \(x^i\)
  \item
    Num of output units: num of classes (in Classification)
  \item
    Num of hidden layer:

    \begin{itemize}
    \tightlist
    \item
      default: 1
    \item
      if \textgreater{}1: same num of units in each hidden layer
    \end{itemize}
  \item
    Num of units in layer: comparable to num of layer
  \end{itemize}
\item
  Pre-train Network:

  \begin{itemize}
  \tightlist
  \item
    Use pre-trained CNN convolution layers as features selection \& fine
    tune the output layer

    \begin{itemize}
    \tightlist
    \item
      work well when data is small
    \end{itemize}
  \end{itemize}
\end{enumerate}

\hypertarget{bayesian-neural-networks}{%
\subsubsection{Bayesian Neural
Networks}\label{bayesian-neural-networks}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Assumption:

  \begin{itemize}
  \tightlist
  \item
    \(\text{Conditional distribution: } p(t|x,w) = \mathcal N(t|h(x,w),\beta^{-1})\)
  \item
    \(\text{Prior: } p(w)=\mathcal N (w|0,\alpha^{-1}I)\)
  \end{itemize}
\item
  Likelihood:

  \begin{itemize}
  \tightlist
  \item
    \(\displaystyle p(\boldsymbol t|w) = \prod_{n=1}^N \mathcal N(t_n|h(x_n,w), \beta^{-1})\)
  \end{itemize}
\item
  Posterior:

  \begin{itemize}
  \item
    \(\displaystyle p(w|\boldsymbol t) \propto p(\boldsymbol t|w) p(w)\),
    Non-Guassian because of non-linear \(h(x_n,w)\)
  \item
    Apply Laplace Approximation:

    \(\displaystyle \Rightarrow\) find \(w_{MAP}\) for
    \(\ln p(w|\boldsymbol t)=-\frac \beta 2 \sum_{n=1}^N (h(x,w)-t_n)^2 - \frac \alpha 2 w^Tw + \text{const}\),

    ​ * \(w_{MAP}\) is local maximal due to non-convex
    \(\ln p(x|\boldsymbol t)\), which due to non-linear \(h(x,w)\)

    \(\Rightarrow \boldsymbol A = - \nabla\nabla \ln p(w|\boldsymbol t) =\alpha I + \beta \boldsymbol H\),

    ​ where \(\boldsymbol H\) is Hessian matrix of sum-of-square error
    function with respect to \(w\)
  \item
    \(p(w|\boldsymbol t) \simeq \mathcal N(w|w_{MAP}, \boldsymbol A^{-1})\)
  \end{itemize}
\item
  Predictive Distribution:

  \begin{itemize}
  \item
    \(\displaystyle p(t|x,\boldsymbol t) = \int p(t|x,w)p(w|\boldsymbol t) dw\),
    still analytically intractable due to non-linear \(h(\cdot)\) in
    \(p(t|x,w)\)
  \item
    Taylor Expansion:
    \(h(x,w) \simeq h(x,w_{MAP}) + \boldsymbol g^T(w-w_{MAP})\), where
    \(\boldsymbol g=\nabla h(x,w)|_{w=w_{MAP}}\)

    \(\Rightarrow \text{Conditional distribution: }p(t|x,w) \simeq \mathcal N(t|h(x,w_{MAP})+\boldsymbol g^T(w-w_{MAP}), \beta^{-1})\)
  \item
    \(p(t|x,\boldsymbol t) = \mathcal N(t|h(x,w_{MAP}),\sigma^2(x)), \text{ where } \sigma^2(x)=\beta^{-1}+\boldsymbol{g}^T\boldsymbol A^{-1}\boldsymbol g\)
  \end{itemize}
\end{enumerate}

\hypertarget{autoencoder---neural-network}{%
\paragraph{Autoencoder - Neural
Network}\label{autoencoder---neural-network}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Problem:

  \begin{itemize}
  \tightlist
  \item
    Functions that can be compactly represented by a depth k
    architecture might require an exponential number of computational
    nodes using a depth k-1 architecture

    \begin{itemize}
    \tightlist
    \item
      \(\Rightarrow\) deep network
    \end{itemize}
  \item
    Problem in Deep network:

    \begin{itemize}
    \tightlist
    \item
      More local minimum, plateaus and saddle points \(\Rightarrow\)
      hard to generalize well
    \end{itemize}
  \end{itemize}
\item
  Goal:

  \begin{itemize}
  \tightlist
  \item
    Unsupervised pre-training to find distributed representation
  \item
    Find good lossy compression of data
  \end{itemize}
\item
  Undercomplete Architecture:

  \begin{itemize}
  \item
    Input: data \(X\)
  \item
    Target output: \(X\) (input itself)
  \item
    Reconstruction error: \$\displaystyle J=\sum\_\{n=1\}\^{}N
    \ell (x\textsuperscript{n,f(c(x}n))) \$

    \begin{itemize}
    \item
      \(c(x) \text{ is encoder; }f(x) \text{ is decoder; } f(c(x)) \text{ is reconstruction from network}\)
    \item
      Minimize negative log likelihood: \(-\ln P(x|c(x))\)

      \(\text{if } P(x|c(x)) \text{ Guassian} \Rightarrow \ell \text{ is squared error} \\ \text{if } x_i \text{ obey binomial} \Rightarrow -\ln P(x|c(x)=-x_i\ln f_i(c(x))+(1-x_i)\ln(1-f_i(c(x))\space)\)

      ​ \(\small\text{$f_i$ is the $i$-th component of decoder}\)
    \end{itemize}
  \item
    Undercomplete:

    \begin{itemize}
    \tightlist
    \item
      Small num of hidden units \(\Rightarrow c(x)\) as a lossy
      compression of \(x\)
    \end{itemize}
  \item
    Stacking Encoders:

    \begin{itemize}
    \item
      \begin{figure}
      \centering
      \includegraphics{./Stacking autoencoder.png}
      \caption{Stacking autoencoder}
      \end{figure}
    \item
      chain non-linear encoder \(c_i(x)\)

      ​
      \(\small\text{Compared to PCA: chain linear transformations $\Leftrightarrow$ single linear transformation}\)
    \item
      Discard decoding layer \(\Rightarrow\) use
      \(z_i=c_i(\cdots c_1(x)\cdots)\) as features for subsequent
      learning
    \end{itemize}
  \end{itemize}
\item
  Higher Dimensional Hidden Layer:

  \begin{itemize}
  \tightlist
  \item
    Reason:

    \begin{itemize}
    \tightlist
    \item
      Want many features wider than input
    \end{itemize}
  \item
    Constraint to force compression:
    \(\small\text{(avoid learning identity function - "perfect reconstruction")}\)

    \begin{itemize}
    \tightlist
    \item
      Regularization
    \item
      Early stopping of stochastic gradient descent
    \item
      Adding noise into input
    \item
      Sparsity constraint on encoding \(c(x)\)
    \end{itemize}
  \end{itemize}
\item
  Denoising Autoencoder:

  \begin{itemize}
  \tightlist
  \item
    Idea:

    \begin{itemize}
    \tightlist
    \item
      Add noise to input \& noise free data as target output
    \end{itemize}
  \item
    Goal:

    \begin{itemize}
    \tightlist
    \item
      preserve information of input
    \item
      undo corruption process
    \end{itemize}
  \item
    Reconstruction likelihood:

    \begin{itemize}
    \tightlist
    \item
      \(P(x|c(\hat x)), \text{ where $x$ noise free, $\hat x$ corrupted}\)
    \end{itemize}
  \end{itemize}
\item
  Sparse Representations

  \begin{itemize}
  \item
    Idea:

    \begin{itemize}
    \item
      Some basis functions for a partcular \(x\) may be of minor
      variation

      \(\Rightarrow\) many hidden nodes (basis functions collected in
      \(\Phi\)) but a few active for a given code \(c(x)\)

      ​ \(\small\text{use few element from $\Phi$ to represent $x$}\)
    \end{itemize}
  \item
    \(\ell_0\) Norm Penalty:

    \begin{itemize}
    \item
      \$\displaystyle \min\emph{\{w\} = \sum}\{n=1\}\^{}N \frac 1 2
      \textbar{}x\textsuperscript{n-\Phi w}n\textbar{}\_2\^{}2+\lambda\textbar{}w\textbar{}\_0
      \$
    \item
      Sparse: small num of non-zero \(w_i\)
      \(\text{($\|w\|_0$ is small for $x\approx \Phi w$)}\)
    \item
      Problem: \(\|w\|_0\) non-convex

      ​ \(\Rightarrow\) relax to \(\|w\|_1\)
    \end{itemize}
  \item
    \(\ell_1\) Norm Penalty:

    \begin{itemize}
    \item
      \$\displaystyle \min\emph{\{w\} = \sum}\{n=1\}\^{}N \frac 1 2
      \textbar{}x\textsuperscript{n-\Phi w}n\textbar{}\_2\^{}2+\lambda\textbar{}w\textbar{}\_1
      \$
    \item
      Let \(K=\Phi\Phi^T\) be Gram matrix over
      \(\Phi \small \text{, ($\Phi$ normalized)}\)

      ​
      \(\displaystyle\text{Mutual Coherence: } M = M(\Phi)=\max_{i\neq j}|K_{i,j}| \\ \Rightarrow \text{similar rows, then }M\approx 1\)
    \item
      Therom:

      \(\text{For $w^\star$ minimize $\ell_0$ problem }\)

      ​
      \$\text{If $\|w^\star\|_0 < \frac 1 M \Rightarrow w^\star$ is the unique sparsest solution}
      \$

      ​
      \$\text{If $\|w^\star\|_0 < \frac 1 2(1+\frac 1M) \Rightarrow $ minimizer of $\ell_1$ problem has the same sparsity pattern as $w^\star$}
      \$

      ​ \(\small \text{ (has 0 in the same places)}\)
    \end{itemize}
  \item
    Probabilistic View of \(\ell_1\):

    \begin{itemize}
    \tightlist
    \item
      Maximize Joint distribution: \(p(x,w)=p(w)p(x|w),\)
      \(\small\text{ gives rise to } \|\cdot\|_2, \text{where } w \text{ can be considered as latent variable}\)
    \item
      Laplace Prior: \(p(w_i)=\frac \lambda 2 e^{-\lambda |w_i|},\)
      \(\small \text{ gives rise to $\ell_1$ regularization}\)
    \end{itemize}
  \end{itemize}
\end{enumerate}

\hypertarget{regularization}{%
\subsubsection{Regularization}\label{regularization}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{L2 regularization}

  \begin{itemize}
  \tightlist
  \item
    Description:

    \begin{itemize}
    \tightlist
    \item
      Penalizing the squared magnitude of all parameters directly in the
      objective. That is, for every weight \(w\) in the network: add the
      term \(\frac 12 \lambda w^2\) to the objective, where \(\lambda\)
      is the regularization strength. It is common to see the factor of
      \(\frac 12\) in front because then the gradient of this term with
      respect to the parameter \(w\) is simply \(\lambda w\) instead of
      \(2 \lambda w\).
    \end{itemize}
  \item
    Interpretation:

    \begin{itemize}
    \tightlist
    \item
      heavily penalizing peaky weight vectors and preferring diffuse
      weight vectors.
    \item
      encouraging the network to use all of its inputs a little rather
      that some of its inputs a lot, due to multiplicative interactions
      between weights and inputs this has the appealing property of
    \item
      every weight is decayed linearly: \texttt{W\ +=\ -lambda\ *\ W}
      towards zero during gradient descent parameter update
    \end{itemize}
  \end{itemize}
\item
  \textbf{L1 regularization} is another relatively common form of
  regularization, where for each weight ww we add the term λ∣w∣λ∣w∣ to
  the objective. It is possible to combine the L1 regularization with
  the L2 regularization: λ1∣w∣+λ2w2λ1∣w∣+λ2w2(this is called
  \href{http://web.stanford.edu/~hastie/Papers/B67.2\%20\%282005\%29\%20301-320\%20Zou\%20\&\%20Hastie.pdf}{Elastic
  net regularization}). The L1 regularization has the intriguing
  property that it leads the weight vectors to become sparse during
  optimization (i.e.~very close to exactly zero). In other words,
  neurons with L1 regularization end up using only a sparse subset of
  their most important inputs and become nearly invariant to the
  ``noisy'' inputs. In comparison, final weight vectors from L2
  regularization are usually diffuse, small numbers. In practice, if you
  are not concerned with explicit feature selection, L2 regularization
  can be expected to give superior performance over L1.
\item
  \textbf{Max norm constraints}. Another form of regularization is to
  enforce an absolute upper bound on the magnitude of the weight vector
  for every neuron and use projected gradient descent to enforce the
  constraint. In practice, this corresponds to performing the parameter
  update as normal, and then enforcing the constraint by clamping the
  weight vector w⃗ w→ of every neuron to satisfy ∥w⃗
  ∥2\textless{}c‖w→‖2\textless{}c. Typical values of cc are on orders of
  3 or 4. Some people report improvements when using this form of
  regularization. One of its appealing properties is that network cannot
  ``explode'' even when the learning rates are set too high because the
  updates are always bounded.
\item
  \textbf{Dropout} is an extremely effective, simple and recently
  introduced regularization technique by Srivastava et al.~in
  \href{http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf}{Dropout:
  A Simple Way to Prevent Neural Networks from Overfitting} (pdf) that
  complements the other methods (L1, L2, maxnorm). While training,
  dropout is implemented by only keeping a neuron active with some
  probability pp (a hyperparameter), or setting it to zero otherwise.

  \begin{figure}
  \centering
  \includegraphics{./dropout.jpeg}
  \caption{img}
  \end{figure}

  Understanding:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    训练多个子网络，(视dropout之后的网络为子网络)，通过对网络输出取平均值(期望值)来抵消不同的过拟合。

    \begin{itemize}
    \tightlist
    \item
      在预测时，只需要输出一个子网络的值 =\textgreater{}
      原文提议对dropout之后的神经元激活值进行rescale
      (对期望值进行估计)，实际上只需要关掉dropout 即可得到一个更好的
      期望的估计值
    \end{itemize}
  \item
    使网络更robust，因为网络必须克服神经元丢失的现象，所以会避免过度依赖于输入的某些单一特征，然后可以学习到更稳定的特征。

    \begin{itemize}
    \tightlist
    \item
      预测时，应当考虑每个已获得的 robust feature \(\Rightarrow\) 关掉
      dropout
    \end{itemize}
  \end{enumerate}
\end{enumerate}

\hypertarget{conv-layer}{%
\subsubsection{Conv Layer}\label{conv-layer}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Convolution as function:

  \begin{itemize}
  \tightlist
  \item
    1-D:

    \begin{itemize}
    \tightlist
    \item
      \(\displaystyle s(x) = (f * g)(x) = \sum_{x=-\infty}^\infty f(x)g(t-x)\)
    \end{itemize}
  \item
    2-D:

    \begin{itemize}
    \tightlist
    \item
      \(\displaystyle S(i,j)= (I*K)(i,j) = \sum_m\sum_n I(m,n)K(i-m, j-n)\)
    \item
      \(\displaystyle S(i,j)= (K*I)(i,j) = \sum_m\sum_n I(i-m, j-n)K(m,n), \text{ because of }\)
    \end{itemize}
  \end{itemize}
\item
  Convolution demonstrated in image:

  \begin{itemize}
  \item
    1D convolution with stride 2

    \begin{itemize}
    \tightlist
    \item
      grey cell =\textgreater{} padded 0; green cell =\textgreater{}
      activated weights in filter
    \end{itemize}

    \begin{figure}
    \centering
    \includegraphics{./Convolution in 1d.PNG}
    \caption{Convolution in 1d}
    \end{figure}
  \end{itemize}
\item
  Upsampling:

  \begin{itemize}
  \tightlist
  \item
    A specific case of \textbf{Resampling}:

    \begin{itemize}
    \tightlist
    \item
      re-construct the original continuous signal
    \item
      sample again from the re-constructed signal
    \end{itemize}
  \item
    In Conv case =\textgreater{} Deconvolution:

    \begin{itemize}
    \tightlist
    \item
      transposed conv
    \item
      fractional conv
    \end{itemize}
  \end{itemize}
\item
  Transposed Conv vs.~Fractional Conv

  \begin{itemize}
  \item
    ​

    \begin{figure}
    \centering
    \includegraphics{./transposed conv vs frationally strided conv .PNG}
    \caption{transposed conv vs frationally strided conv}
    \end{figure}

    \begin{itemize}
    \item
      \begin{enumerate}
      \def\labelenumii{(\alph{enumii})}
      \tightlist
      \item
        Transposed Conv \(\Leftrightarrow\) Backward Conv:
      \end{enumerate}

      ​ grey in \(y\) : cropping (previous padding becomes cropping in
      upsampling)

      ​ matrix is just the transposed matrix of a downsampling conv
      (\textbf{transposed kernel})

      ​ same as the backward operation of a downsampling conv

      ​ (can always been seen as a backword process of \emph{some} conv)

      ​ (\textbf{Note}: practically, implemented as swapping
      back/forward-prop of another conv)
    \item
      \begin{enumerate}
      \def\labelenumii{(\alph{enumii})}
      \setcounter{enumii}{1}
      \tightlist
      \item
        Fractional Conv \(\Leftrightarrow\) Sub-pixel Conv:
      \end{enumerate}

      ​ imaginary sub-pixel (inserted as 0) between each pixel
    \end{itemize}
  \item
    Arithmic in Deconvolution:

    \begin{itemize}
    \item
      The simplest way: given input of deconv, think it as output of a
      conv from a same-size kernel
    \item
      A conv with stride \(s=1\), padding \(p=0\) and kernel size \(k\)
      has an equivalent deconv, in the form of conv, with stride
      \(s' = s\), kernel size \(k'=k\) and padding \(p'=k-1\) (padding
      at each side)

      (a full-padding conv with unit stride becomes deconv of a no
      padding conv)

      \(\Rightarrow\) In general, can use another conv as deoncv of a
      given conv

      ​ while mentaining the mapping relation between orignal conv and
      corresponding deconv
    \item
      A conv with strde \(s>1\) will have a fractional conv as deconv,
      when mentaining the mapping relation
    \end{itemize}
  \item
    Equivalence within Deconvolution:

    \begin{itemize}
    \item
      Only difference are the indices of weights used when contributing
      to \(y\) from \(x\)

      \(\Rightarrow\) reshape (re-order) the output will get to each
      other (swapping the odd and even pairs)
    \end{itemize}
  \end{itemize}
\item
  Convolution vs.~Deconvolution

  \begin{itemize}
  \item
    Actually can be \textbf{equivalent} !

    \begin{itemize}
    \item
      \(r^d\) channels ouput from conv can represent \(1\) deconv
      output,

      ​
      \(\text{where } d \text{ is spatial dimension of data (usually 2 in inage), } r \text{ is upsampling rate}\)
    \end{itemize}
  \item
    Deconvolution in 2D:

    \begin{figure}
    \centering
    \includegraphics{./decovolutin (fractional conv).PNG}
    \caption{decovolutin (fractional conv)}
    \end{figure}

    (not all padding is shown =\textgreater{} 2 rows of 0 padding on the
    top-left are hidden)

    \begin{itemize}
    \tightlist
    \item
      weights are divided into classes, e.g.~purple, blue, green and
      red.
    \item
      weights within each class are activated at the same time
    \item
      weights between classes will NOT activated at the same time
      \(\Rightarrow\) independent
    \item
      The output are coloured into classes according to the class of
      activated weights
    \end{itemize}
  \item
    Convolution in 2D, with more filters:

    \begin{figure}
    \centering
    \includegraphics{./Convolution equivelant to deconv.PNG}
    \caption{Convolution equivelant to deconv}
    \end{figure}

    (all padding is shown)

    \begin{itemize}
    \tightlist
    \item
      explicitly break weights into independent filters according to
      their classes
    \item
      Convolve filters on the original image
    \item
      Each output map contains the result contributed by each weights
      class
    \item
      Re-order (periodic shuffling) the result into the deconvolution
      result
    \end{itemize}
  \item
    Benefit \& Loss:

    \begin{itemize}
    \item
      Using convolution to represent deconvolution will have more
      parameters

      \(\Rightarrow\) more representation power, harder to train
    \end{itemize}
  \end{itemize}
\end{enumerate}

\hypertarget{traning}{%
\subsubsection{Traning}\label{traning}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Searching hyper-parameter:

  \begin{itemize}
  \tightlist
  \item
    Sampling hyper-parameter from uniform distribution over the
    parameters space
  \item
    \textbf{Not} using fix steps

    \begin{itemize}
    \tightlist
    \item
      Usually some parameters have more influence on the performance
      =\textgreater{} fix steps waste effort
    \end{itemize}
  \end{itemize}
\item
  ​
\end{enumerate}
