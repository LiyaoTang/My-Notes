\chapter{Introduction}

\section{General Concern}
\subsection{Types of Learning}

\subsubsection{by Learning Procedure}
\begin{itemize}
\item Supervised Learning
	\begin{itemize}
	\item Training
		\begin{itemize}
		\item data comprises examples of input vectors with corresponding target vectors
		\item usually, training data comprises a tiny fraction of all possible input data
		\end{itemize}
	\item Task
		\begin{itemize}
		\item regression: output one or more continuous variable
		\item classification: assign input to one of a finite number of discrete categories
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item generalization ability \\
		i.e. discovery general rule from limited data
		\end{itemize}
	\end{itemize}
\item Unsupervised Learning
	\begin{itemize}
	\item Training
		\begin{itemize}
		\item data consists of a set of input vectors without target vectors
		\end{itemize}
	\item Task
		\begin{itemize}
		\item clustering: discover groups of similar examples
		\item density estimation: determine the distribution of data within the input space
		\item dimension reduction: project data into low dimension \\
		(for the purpose of e.g. visualization, feature extraction, etc.)
		\end{itemize}
	\end{itemize}
\item Reinforcement Learning
	\begin{itemize}
	\item Training
		\begin{itemize}
		\item input with time series \& discover optimal output by a process of trial and error
		\end{itemize}
	\item Goal
		\begin{itemize}
		\item find actions to take under given circumstance to maximize a reward
		\end{itemize}
	\item Challenge
		\begin{itemize}
		\item credit assignment
		\item exploration-expoitation Trade-off
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{by Modeling Goal}
\begin{itemize}
\item Generative Model
	\begin{itemize}
	\item Approach
		\begin{itemize}
		\item classification: model $p(x|\mathcal C)$ \& incorporate with prior $p(\mathcal C)$
		\item regression: model joint distribution $p(x, t)$ and then normalize to have $p(t|x)$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item more flexible due to full Bayesian treatment
			\begin{itemize}
			\item able to incorporate with prior to reduce bias in skewed dataset
			\item able to combine other prob model systematically
			\end{itemize}
		\item Bayesian treatment performed in
			\begin{itemize}
			\item solve $p(t|x) / p(\mathcal C|x)$ via predictive distribution
			\item making prediction decision using $p(t|x)$ - see \hyperref[Intro_Decision_Stat]{decision for statistical model}
			\end{itemize} 
		\item much more compute hungry
		\end{itemize}
	\end{itemize}
\item Discriminative Model
	\begin{itemize}
	\item Approach
		\begin{itemize}
		\item model $p(\mathcal C|x)$ for classification, $p(t|x)$ for regression, directly
		e.g. directly use function $f(\mathbf x)$ output as predictive distribution
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item less flexible yet much more efficient learning
		\item Bayesian analysis performed on making prediction decision
		\end{itemize}
	\end{itemize}
\item Discriminant
	\begin{itemize}
	\item Approaches
		\begin{itemize}
		\item find a function $f:x\rightarrow \mathcal C$ or $y$ (not under Bayes framework at all)
		\end{itemize}
	\end{itemize}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Decision Theory}
\subsection{Overview}
\subsubsection{Formulation}
\begin{itemize}
\item Analysis Target
	\begin{itemize}
	\item Human Behavior
		\begin{itemize}
		\item goal-directed behaviors in the presence of options (alternative actions)
		\end{itemize}
	\end{itemize}
\item Goal
	\begin{itemize}
	\item Normative Decision Theory
		\begin{itemize}
		\item how the decision should be made, in order to be national, etc. \\
		$\Rightarrow$
		\item an normative theory is \textit{weakly falsified}, if $\exists$ a decision problem in which an agent can perform in contradiction with the theory without being irrational
		\item an normative theory is \textit{strictly falsified}, if $\exists$ a decision problem in which an agent performing in accordance with the theory cannot be a rational agent
		\end{itemize}
	\item Descriptive Decision Theory
		\begin{itemize}
		\item how decisions are actually made in practice
		\end{itemize}
	\item Specific Concern
		\begin{itemize}
		\item interaction of agents: collective decision-making
		\item behavior of individuals in decisions
		\item rationality in decisions
		\item coordinating decision over time, in a changing environment
		\end{itemize}
	\end{itemize}
\item Preference in Decision Making
	\begin{itemize}
	\item Preference Logic
		\begin{itemize}
		\item defined relation $\ge$ for two options $a,b$, according to agent's preference \\
		$\Rightarrow$ form a partially ordered set $(S, \ge)$, where $S$ the set of options
		\item other relation $<, \equiv$ can be derived from $\ge$
		\end{itemize}
	\item Completeness
		\begin{itemize}
		\item complete: $\forall i,j\in X, $ a relation is defined \\
		(otherwise incomplete preference)
		\item yet, usually incomplete preference, as determining preference takes effort
		\end{itemize}
	\item Transitivity
		\begin{itemize}
		\item $a\ge b \land b\ge c \rightarrow a\ge c$ holds true for all $a,b,c$ \\
		(hence all other relation $<,\equiv$)
		\item $\Rightarrow$ problem of accumulated negligible indifference ($\equiv$) \\
		e.g. $a_0$ as no sugar, $a_{100}$ as full sugar, given $a_{i}\equiv a_{i+1}$ due to negligible difference \\
		$\Rightarrow$ finally $a_0\equiv a_{100}$ due to transitivity
		\end{itemize}
	\item Guideline
		\begin{itemize}
		\item given the constructed partial order set $(S, \ge)$, choose one of its upper bound \\
		(with prevalent assumption of transitivity and completeness)
		\end{itemize}
	\end{itemize}
\item Utility in Decision Making
	\begin{itemize}
	\item Numerical Representation
		\begin{itemize}
		\item using pre-defined relation in number, with a preference of maximal utility \\
		$\Rightarrow$ naturally transitive \& complete
		\item relative number $\Rightarrow$ meaningful in comparison within current option set \\
		(i.e. not comparable across different decision making process)
		\end{itemize} 
	\item \textbf{Comparability}
		\begin{itemize}
		\item only comparable between options in the same decision process
		\end{itemize}
	\item Guideline
		\begin{itemize}
		\item maximization: choose one of the options with maximal utility \\ 
		(employed by default)
		\item satisfying: choose one of the options with sufficient utility
		\end{itemize}
	\end{itemize}
\item Decision Matrices
	\begin{itemize}
	\item $\text{Outcome } O_{n\times m}=\text{actions }\{a_1,...,a_n\} \times\text{states }\{s_1,...,s_m\}$
		\begin{itemize}
		\item with $\{a_1,a_2\} =$ \{umbrella, no umbrella\}; $\{s_1,s_2\}=$ \{rain, no rain\} \\
		\begin{tabular*}{0.5\linewidth}{l|ll}
			            & rain       & no rain    \\
			\hline
			umbrella    & dry\&heavy & dry\&heavy \\
			no umbrella & wet\&light & dry\&light
		\end{tabular*}
		\end{itemize}
	\item Utility Assignment
		\begin{itemize}
		\item requirement: shared perspective \\
		$\Rightarrow$ all participants share a common concern \\
		(otherwise, different incomparable utility assigned to the same outcome)
		\item assign each outcomes with utility $\Rightarrow$ utility matrices $U$ \\
		\begin{tabular*}{0.5\linewidth}{l|ll}
			            & rain       & no rain    \\
			\hline
			umbrella    & 15 & 15 \\
			no umbrella & 0  & 18
		\end{tabular*}
		\end{itemize}
	\end{itemize}
\item Probability Estimation of Environment
	\begin{itemize}
	\item Modeling State
		\begin{itemize}
		\item certainty: deterministic knowledge of the environment after each action \\
		$\Rightarrow$ thus deterministic outcomes for each action
		\item risk: complete probabilistic knowledge of environment \\
		(may conditioned on action)
		\item uncertainty: partial probabilistic knowledge
		\item ignorance: no probabilistic knowledge, or not meaningful
		\end{itemize}
	\item Objective Probability
		\begin{itemize}
		\item based on empirical known frequencies
		\item indirect estimation (e.g. similar experiment), with calibration \\
		may use subjective probability, which is unreliable due to psycho-effect \\
		$\Rightarrow$ better not use subjective estimates of prob
		\end{itemize}
	\item Bayesian Description
		\begin{itemize}
		\item a coherent and complete set of probabilistic beliefs \\
		(in compliance with laws of probability)
		\item probability subjected to the observation
		\end{itemize}		
	\end{itemize}
\item Action Discovering
	\begin{itemize}
	\item Category
		\begin{itemize}
		\item closure: settle down with current available actions
		\item active postponement: searching for other possible actions (postpone the decision)
		\item semi-closure: consider only reversible actions \& searching for other actions
		\end{itemize}
	\item Guideline
		\begin{itemize}
		\item all current actions have severe drawbacks?
		\item is searching possible actions costly?
		\item is problem aggregating over time? \\
		etc...
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item a meta-decision problem before the scenario-specific decision
		\end{itemize}
	\end{itemize}
\item $\Rightarrow$ Assumption
	\begin{itemize}
	\item Closed Set of Actions
		\begin{itemize}
		\item no new alternative action allowed to be added \\
		(v.s. open: new actions can be discovered and taken into consideration)
		\end{itemize}
	\item Mutually Exclusive Actions
		\begin{itemize}
		\item no two actions can be both realized
		\end{itemize}
	\item Defined States
		\begin{itemize}
		\item all possible states of environment are recognized
		\end{itemize}
	\item Outcome \& Utility
		\begin{itemize}
		\item the joint result of environment (state) and chosen action
		\item utility assigned to each outcome (similar to rewards in RL) \\
		(though the utility for each result is usually subject to agent) \\
		$\Rightarrow$ completeness \& transitivity assumed		
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Decision under Risk}
\begin{itemize}
\item Maximizing Expected Utility
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item for action $a$, expected utility $\displaystyle u = \mathbb E_{p(s)} U(a,s) = \sum_{s\in S} p(s)U(a,s)$, \\ 
		where $U(a,s)$ the utility assigned to outcome $O(a,s)$
		\end{itemize}
	\item Objective Utility
		\begin{itemize}
		\item utility assigned according to the objective rewards (e.g. money)
		\item $\Rightarrow$ St, Petersburg paradox: a fair coin tossed, if its head comes up at $n^{th}$ toss, $2^n$ gold coins are rewarded, stop tossing if its back comes up \\
		$\Rightarrow$ expected wealth $=\lim_{n\rightarrow \infty} \frac 12\cdot 1 + \frac 14\cdot 2 + ... + \frac 1{2^n}\cdot 2^{n-1} = +\infty$ \\
		$\Rightarrow$ should play the game for any finite entry fee
		\end{itemize}
	\item Subjective Utility
		\begin{itemize}
		\item in economic: utility of next increment of wealth $\propto \frac 1w$, where $w$ the current wealth \\
		$\Rightarrow$ utility of wealth $=\log(w)$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item objective utility to maintain inter-subjective validity \\
		(hence expert advice remains effective)
		\item maximizing utility to maximize long-term outcome \\ 
		(suitable for large-scale / long-term decision) \\
		$\Rightarrow$ law of large number to level out randomness
		\item $\Rightarrow$ suitability depends on the scale (the leveling-out effect) \\ 
		(e.g. isolated cases may not suitable to maximize expected utility)
		\item may not suitable in extreme effect, to maintain fairness \\
		(e.g. avoid imposing high-prob risk on individuals)
		\end{itemize}
	\end{itemize}
\item Variations
	\begin{itemize}
	\item Conditionalized Expected Utility
		\begin{itemize}
		\item prob of state $p(s)$ described conditional on action \\
		$\displaystyle \Rightarrow u = \mathbb E_{p(s|a)} U(a,s) = \sum_{s\in S} p(s|a)U(a,s)$
		\item model the influence of actions on states
		\end{itemize}
	\item Generalized Expected Utility
		\begin{itemize}
		\item process utility: accounts for attitude towards risk and certainty
		\item maximize expected (utility of outcome + process utility)
		\end{itemize}
	\item Regret Theory
		\begin{itemize}
		\item regret: the gap between reward received and highest possible reward from other actions
		\item maximize expected utility of outcome \& minimize regret
		\end{itemize}
	\item Prospect Theory
		\begin{itemize}
		\item function f$(0,1)\rightarrow(0,1)$ to adjust probability \\
		(description to be merely weights, not satisfying prob law)
		$\Rightarrow$ overweight the prob close to $0\&1$, as tendentious to certainty
		\item maximize weighted utility
		\end{itemize}
	\end{itemize}
\item Causal Decision
\end{itemize}
\subsubsection{Decision under Uncertainty}
\begin{itemize}
\item Estimation of Environment
	\begin{itemize}
	\item Binary Measure
		\begin{itemize}
		\item segment out a range for probability describing the state \\
		e.g. $p(s)\in(0.05, 0.2)$
		\end{itemize}
	\item Second-Order Probability
		\begin{itemize}
		\item a Bayesian probability for each state $s$
		\item a further measurement of the reliability of probability estimates $p(s)$ \\
		(may consider as a meta subjective prob)
		\end{itemize}
	\item Fuzzy Set Membership
		\begin{itemize}
		\item vagueness, instead of randomness, to describe uncertainty \\
		$\Rightarrow$ assign a degree of membership for each $p(s)$ \\
		$\Rightarrow$ based on laws of fuzzy membership (non-statistical concept)
		\item measure the degree of "$p(s)$ is the prob of state $s$ happening" being true
		\end{itemize}
	\item Epistemic Reliability
		\begin{itemize}
		\item a weight $\in (0,1)$ as the measure of reliability of a prob, \\
		with NO mathematical properties pre-defined
		\end{itemize}
	\end{itemize}
\item Decision Criteria
	\begin{itemize}
	\item Maxmin Expected Utility
		\begin{itemize}
		\item maximize the minimal expected utility \\
		$\Rightarrow$ an extremely prudent/pessimistic criterion
		\end{itemize}
	\item Reliability-weighted Expected Utility
		\begin{itemize}
		\item use the weighted average probability as true probability \\
		(then reduced to maximizing expected utility) \\
		$\Rightarrow$ an optimistic criterion
		\end{itemize}
	\item Index
		\begin{itemize}
		\item $p'$ the best prob estimate and $\rho$ as its degree of confidence \\
		$\Rightarrow$ calculate $u_\text{best}$ using $p'$; $u_\text{min}$ the minimal expected utility
		\item summarize as $u = \rho u_\text{best} + (1-\rho)u_\text{min}$ \\
		$\Rightarrow$ maximize the $u$ (choose the action with maximal $u$)
		\item $\rho\in(0,1)$ as a balance factor between pessimism \& optimism 
		\end{itemize}
	\item Clipped Maxmin Expected Utility
		\begin{itemize}
		\item discard any prob with a reliability lower than a threshold \\
		(then max-min expected utility with remaining prob)
		\end{itemize}
	\item Filtered Maxmin Expected Utility
		\begin{itemize}
		\item $3$ filters applied in sequential order on actions (rather than prob)
		\item $E$-test: action $a$ passes, iff $\exists$ reliable prob $p(s)>0$ \& $\forall a'\in A, U(a,s)\ge U(a',s)$ \\
		(action $a$ is possible to be the best)
		\item $P$-test: passes, iff it can be the best in all candidates
		\item $S$-test: passes, iff it is the best under maxmin expected utility in all candidates \\
		$\Rightarrow$ directly produce action to choose
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Decision under Ignorance}
\begin{itemize}
\item Category
	\begin{itemize}
	\item Known States
		\begin{itemize}
		\item the prob for each state $p(s) > 0 \Rightarrow$ unknown non-zero prob
		\end{itemize}
	\item Unknown States
		\begin{itemize}
		\item the prob $p(s) \ge 0 \Rightarrow$ unknown prob
		\end{itemize}
	\end{itemize}
\item Unknown Non-zero Prob
	\begin{itemize}
	\item Max-Min Utility
		\begin{itemize}
		\item maximize the minimal utility (extremely pessimistic)
		\item lexicographic maximin: consider their $2^\text{nd}$ worst outcome
		\end{itemize}
	\item Max-Max Utility
		\begin{itemize}
		\item maximize the maximal utility (extremely optimistic)
		\end{itemize}
	\item Index
		\begin{itemize}
		\item calculate $u_\text{max}, u_\text{min}$ by max-max, max-min
		\item $u = \alpha u_\text{} + (1-\alpha) u_\text{min}$ \\
		$\Rightarrow \alpha$ as tge degree of optimism
		\end{itemize}
	\item Min-Max Regret
		\begin{itemize}
		\item produce a regret matrix $R$ from utility matrix $U$, \\ 
		where $R(a,s) = $ maximal possible utility in state $s - U(a,s)$
		\item then, minimize the maximal regret, according to $R$
		\end{itemize}
	\item Common Prior
		\begin{itemize}
		\item assign uniform distribution over states, thus reduced to decision under risk
		\item yet, depends on how the states are partitioned \\ 
		i.e. the num of states affect the prob
		\end{itemize}
	\end{itemize}
\item Unknown Prob
	\begin{itemize}
	\item Challenge
		\begin{itemize}
		\item action may lead to catastrophic outcomes (unforeseen outcomes) \\
		$\Rightarrow$ hard to decide if the chance of a severe consequence is negligible or not \\
		$\Rightarrow$ uncertainty towards severe consequence
		\end{itemize}
	\item Empirical Guideline
		\begin{itemize}
		\item choosing / not choosing an action may have different uncertainty \\
		(asymmetry of uncertainty)
		\item novelty: empirically, novelty brings in more uncertainty
		\item spatial and temporal limitations: more limitation, less uncertainty
		\item more interference with complex system, the more uncertainty
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{Social Decision}
\begin{itemize}
\item Assumption
	\begin{itemize}
	\item Conflicting Concern
		\begin{itemize}
		\item participants usually have different/conflicting goals\&concerns
		\end{itemize}
	\end{itemize}
\item Cyclic Preference
	\begin{itemize}
	\item No Stable Actions
		\begin{itemize}
		\item the directed set (actions, preference) forms a directed graph \\
		$\Rightarrow \forall$ action $a$, $\exists $ action $a', a' > a$
		\end{itemize}
	\item Arrow's Theorem
		\begin{itemize}
		\item 4 rationality criteria are satisfied by decision rule \\
		$\Rightarrow$ then cyclic preference unavoidable
		\item understanding: some rationality demands are NOT compatible
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Decision for Statistical Model} \label{Intro_Decision_Stat}
\begin{itemize}
\item Formulation
	\begin{itemize}
	\item Probability Description
		\begin{itemize}
		\item known distribution $p(\mathbf t|\mathbf x)$ from model inference, where $\mathbf x$ the input, $\mathbf t$ the label \\
		$\Rightarrow$ all states (label) estimated with probability
		\end{itemize}		
	\end{itemize}
\item Decision Criteria
	\begin{itemize}
	\item Minimizing Misclassification Rate (Maximizing Correct-classification Rate)
		\begin{itemize}
		\item action: for each class $k$, assign a region $\mathcal R_k$ s.t. if $\mathbf x\in \mathcal R_k$, predict $\hat {\mathbf t} := \mathcal C_k$
		\begin{align*}\Rightarrow \displaystyle P(\text{mistake}) &= \sum_{k=1}^K P(\mathbf x\not\in \mathcal R_k, \mathbf t=\mathcal C_k) = 1-\underbrace{\sum_{k=1}^K P(\mathbf x\in\mathcal R_k, \mathbf t=\mathcal C_k)}_{P(\text{correct})} \\ &= 1-\sum_k\int_{\mathcal R_k}p(\mathcal C_k|\mathbf x) p(\mathbf x) \D\mathbf x \end{align*}
		$\Rightarrow$ segment $\mathbf x$ to be in region $\mathcal R_k$ governed by the maximal $p(\mathcal C_k | \mathbf x)$ \\
		i.e. predict $\hat {\mathbf t}$ to be the $\mathcal C_k$ with maximal posterior $p(\mathcal C_k | \mathbf x)$
		\item in binary classification ($K=2$) with $\mathbf x = x$ being a scalar: \\
		\begin{minipage}[r]{.5\linewidth}
		\includegraphics[width=\linewidth, center]{"./Introduction/decision theory-decision region".jpg}
		\end{minipage}
		\begin{minipage}[l]{.5\linewidth}
		where, \\
		blue = error of mis-classify $x\in\mathcal C_1$ to $\mathcal C_2$ \\ 
		red+green = mis-classify $x\in\mathcal C_2$ to $\mathcal C_1$
		\end{minipage}
		$\Rightarrow \hat x$ a sub-optimal decision region segment \\
		$\Rightarrow$ shift to $x_0$ to be optimal (remove the red-region error)
		\end{itemize}
	\item Minimizing Expected Loss - Classification
		\begin{itemize}
		\item loss for a kind of misclassification can vary from other kinds \\
		(e.g. recall v.s. precision) \\
		$\Rightarrow$ a loss function $f:$ (label $\mathbf t=\mathcal C_k$, pred ${\mathbf x}\in\mathcal R_j$) $\rightarrow $ loss $L_{kj}$,  \\
		where $L_{kj}$ as the loss of mis-classify $\mathcal C_k$ into $\mathcal C_j$ \& $\forall k, L_{kk}=0$ \\
 		\item expected loss $\displaystyle \mathbb E[L] = \sum_{k,j}L_{kj}P(\mathbf x\in \mathcal R_{j}, \mathbf t=\mathcal C_k) = \sum_{k,j}\int_{\mathcal R_j}L_{kj} p(\mathcal C_k|\mathbf x)p(\mathbf x)d\mathbf x$ \\
		$\Rightarrow$ expected loss from decision region $\displaystyle \mathcal R_j: \sum_k \int_{\mathcal R_j}L_{kj} p(\mathcal C_k|\mathbf x)p(\mathbf x) d\mathbf x$ \\
		$\Rightarrow$ segment $\mathbf x$ into region $\mathcal R_j$ governed by the minimal $\displaystyle \sum_{k}L_{kj}p(\mathcal C_k|\mathbf x)$ \\
		i.e. predict $\hat{\mathbf t}$ to be the $\mathcal C_j$ with minimal $\displaystyle \sum_{k}L_{kj}p(\mathcal C_k|\mathbf x)$
		\end{itemize}
	\item Minimizing Expected Loss - Regression
		\begin{itemize}
		\item minimize $\displaystyle \mathbb E[L]=\int\int L(t, y(x))\cdot p(x, t) \D\mathbf x\D t$, \\
		where $y(x)=y(x, \mathbf w)$
		\item with common square root loss: $L(t, y(x))=[t-y(x)]^2$ \\
		$\displaystyle \Rightarrow \mathbb{E}[L] = \int\int[t-y(x)]^2p(x, t)\D x\D y$ \\
		$\displaystyle \Rightarrow \frac{\delta\mathbb{E}[L]}{\delta y(x)}=2\int(y(x)-t)p(x,t)\D t = 0$ to find the best $y(x)$ \\
		(by \hyperref[Math_Calc_VarCalc]{variational calculus - functional derivative}) \\
		$\displaystyle \Rightarrow y(x) = \frac{\int tp(x, t)\D t}{\int p(x,t)\D t} = \int tp(t|x)\D t = \mathbb{E}_t[t|x]$, the regression function \\
		i.e. predict using the conditional average of $t$ conditioned on $x$ (as $p(t|x)$ known/solved)
		\item also, can be derived as: $L=(y(x)-t)^2 = (y(x)-\mathbb E_t[t|x]+\mathbb E_t[t|x]-t)^2$ \\ 
		\phantom{also, can be derived as: $L$} $= (y-\mathbb E)^2+2(y-\mathbb E)(\mathbb E-t) + (\mathbb E - t)^2$ \\
		$\displaystyle \begin{alignedat}{2}
		& \Rightarrow \int L\cdot p(x,t)\D t &&= (y-\mathbb E)^2p(x) + 2(y-\mathbb E)\int(\mathbb E-t)p(x,t)\D t + \int (\mathbb E-t)^2p(x,t)\D t \\
		& &&= (y-\mathbb E)^2p(x) + \int (\mathbb E-t)^2p(x,t)\D t
		\end{alignedat}$ \\
		$\displaystyle \begin{alignedat}{2}
		&\Rightarrow \mathbb E [L] &&= \int\int L\cdot p(x,t)\D t\D x
		& &&= \int (y(x)-\mathbb E)^2p(x) \D x + \int \int (\mathbb E-t)^2p(x,t)\D t\D x
		\end{alignedat}$
		$\Rightarrow y(x)=\mathbb E = \mathbb E_t[t|x]$ to have minimal $\mathbb E[L]$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item minimize expected loss $\Leftrightarrow$ maximize expected utility \\
		(utility function can be just the negative loss function)
		\item classification: take the class with largest (potentially weighted) posterior $p(\mathcal C|x)$
		\item regression: take the expectation of posterior $p(t|x)$, i.e. $\mathbb E_t[t|x]$
		\end{itemize}
	\end{itemize}
\item Reject Option
	\begin{itemize}
	\item Analysis: Decision Error Source
		\begin{itemize}
		\item the largest posterior $p(\mathcal C_k|\mathbf x) << 1$ \\
		$\Rightarrow$ large $p(\mathbf t \neq \mathcal C_k)$ on region $\mathcal R_k$ \\
		($\Leftrightarrow$ all posteriors are comparable)
		\end{itemize}
	\item Reject by Thresholding
		\begin{itemize}
		\item reject to make decision when the maximal posterior $p(\mathcal C_k|\mathbf x) <$ threshold $\theta$ \\
		($\theta=frac 1 k$ to not reject any example)
		\end{itemize}
	\item Reject Criterion to Minimize Expected Loss
		\begin{itemize}
		\item consider the expected loss instead of raw posterior
		\end{itemize}
	\item Reject on Outlier Detection
		\begin{itemize}
		\item with $p(\mathbf x) = \sum_k p(\mathbf x|\mathcal C_k)p(\mathcal C_k)$ obtained \\
		$\Rightarrow$ reject making decision if new $\mathbf x$ have low probability
		\end{itemize}
	\end{itemize}
\item Approaches
	\begin{itemize}
	\item 
	\end{itemize}
\end{itemize}

\section{Information Theory}
\subsection{Essence}
\subsubsection{Entropy}
\begin{itemize}
\item Measuring Information of Events
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item entropy $\displaystyle H[x]=-\sum_xp(x)\log_2p(x)$, where $p(x)$ the probability of observing $X=x$
		\end{itemize}
	\item Derivation - Random Variable
		\begin{itemize}
		\item let $h(x)$ the information in the observation of $X=x$ \\
		$\Rightarrow$ more information in having rare observation i.e. $x$ with small $p(x)$ has large $h(x)$ \\
		$\Rightarrow h(x)$ monotonic function negatively related to $p(s)$ s.t. $\lim_{p(x)\rightarrow0}h(x)\rightarrow\infty, \lim_{p(x)\rightarrow\infty}h(x)\rightarrow0$
		\item observing independent events $x,y$ with $p(x), p(y)$ \\
		$\Rightarrow \begin{cases} h(x, y) = h(x) + h(y) \\ p(x,y) = p(x)p(y)\end{cases}$ \\
		$\Rightarrow h(x) = -\log_a p(x)$
		\item $h(x)=-\log_2p(x)$ to consider 'bits' as units of $h(x)$ \\
		$h(x)=-\ln p(x)$ to consider 'nats' as units of $h(x)$
		\item $\Rightarrow$ expectation of information $H[x] = \sum_x p(x)\log_2p(x)$, the entropy
		\end{itemize}
	\item Derivation - Permutation
		\begin{itemize}
		\item given $N$ identical object, to put $n_i$ objects into $i^\text{th}$ bin s.t. $N=\sum_{i} n_i$ \\
		$\Rightarrow$ for $i^\text{th}$ bin there are $n_i!$ permutation \\
		$\Rightarrow$ total permutation = $W\prod_i n_i!=N!$, where $W$ the num of ways of allocation \\
		$\Rightarrow W=\frac{N!}{\prod n_i!}$, generalized $C_N^n$
		\item let $H=\frac1N\ln W = \frac1N\ln N!=\frac1N\sum_i\ln n_i!$ \\
		$\displaystyle \begin{alignedat}{2}
		&\Rightarrow \lim_{N\rightarrow\infty}H &&= \lim_{N\rightarrow\infty}\frac{1}{N}\ln N! - \frac{1}{N}\sum_i\ln n_i! \\
		& &&=\lim_{N\rightarrow\infty} \ln N -1 - \frac{1}{N}\sum_i n_i\ln n_i - n_i \\
		& && \text{ (by stirling approximation } \ln n!\simeq n\ln n - n) \\
		& &&= -\lim_{N\rightarrow \infty} \sum_i \frac{n_i}{N}\ln \frac{n_i}{N} \\
		& &&= -\sum_i p_i\ln p_i, 
		\end{alignedat}$ \\
		where $p_i$ the probability of an object being put into $i^\text{th}$ bin (approached by frequency in $N\rightarrow\infty$)
		\end{itemize}
		\item Certainty
		\begin{itemize}
		\item $\lim_{p\rightarrow0}p\ln p = 0$, thus define $h(x)=0$ for $x$ with $p(x)=0$
		\item $1\ln 1 = 0$, thus $h(x)=0$ for $x$ with $p(x)=1$ \\
		$\Rightarrow$ entropy = 0 for 100\% certainty
		\end{itemize}
	\item Largest Entropy
		\begin{itemize}
		\item discrete rand variable $X$, with state $x_n$ \\
		$\Rightarrow$ entropy $H = -\sum_n p(x_n)\ln p(x_n)$ s.t. $p(x_n)\in[0,1]$ and $\sum_n p(x_n)=1$ \\
		$\Rightarrow$ construct Lagrangian $\tilde H = -\sum_n p(x_n)\ln p(x_n) + \lambda \left( \sum_n p(x_n) - 1 \right)$ \\
		$\Rightarrow \frac{\partial \tilde{H}}{\partial p_i} = -\ln p_i - 1 + \lambda = 0$ i.e. $p_i=e^{\lambda-1}$ \\
		given $\sum_i p_i = 1$ (or, by $\frac{\partial \tilde{H}}{\partial \lambda}=0$) \\
		$\Rightarrow p_i = \frac1M$, where $p_i=p(x_i)$ \\
		$\Rightarrow$ corresponding largest entropy $H=\ln M$
		\item to verify the stationary is a maximum \\
		$\frac{\partial \tilde H}{\partial p(x_i) \partial p(x_j)} = \frac{\partial}{\partial p_j} (-\ln p_i -1+\lambda) = -I_{ij}\frac{1}{p_i} \le 0$ \\
		where $I_{ij}$ the element of identity matrix, thus maximal
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item measuring the expectation of information gained by observing a random variable $x$
		\end{itemize}
	\end{itemize}

\item Measuring Information in Continuous Distribution
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item differential entropy $H[x] = -\int p(x)\ln p(x)\D x$
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\item with small enough $\Delta$, there exists $x_i$ s.t. $\int_{i\Delta}^{(i+1)\Delta}p(x)\D x p(x_i)\Delta$, by mean value theorem \\
		$\Rightarrow$ quantize the continuous distribution to have $H_\Delta = -\sum_ip(x_i)\Delta \ln (p(x_i) \Delta) = -\sum_i p(x_i)\Delta \ln p(x_i) - \ln \Delta$ \\
		(as $\sum_i p(x_i)=1$) \\
		$\begin{alignedat}{2} & \Rightarrow \lim_{\Delta\rightarrow0}H_\Delta &&= -\lim_{\Delta\rightarrow0} \left\{ \sum_i p(x_i)\Delta\ln p(x_i) \right\} - \lim_{\Delta\rightarrow0} \ln\Delta \\
		& &&= \underbrace{-\int p(x)\ln p(x)\D x}_\text{differential entropy} - \lim_{\Delta\rightarrow0} \ln\Delta \end{alignedat}$
		\item as $\lim_{\Delta\rightarrow0} \ln\Delta$ diverge: $\Rightarrow$ differential entropy 
		\end{itemize}
	\item Largest Entropy
		\begin{itemize}
		\item with normlalization constraint and specifying mean \& derivation of distributino to be $\mu,\sigma^2$ \\
		$\Rightarrow$ construct Lagrangian $\tilde H = -\int p(x)\ln p(x) \D x + \lambda_1 \left( \int p(x)\D x -1 \right)+ \lambda_2\left( \int xp(x)\D x - \mu  \right) + \lambda_3 \left( \int (x-\mu)^2p(x)\D x - \sigma^2 \right)$ \\
		$\Rightarrow \frac{\delta L}{\delta p(x)} = -1 -\ln p(x)+\lambda_1 + \lambda_2 x + \lambda_3(x-\mu)^2 = 0$, by calculus of Variations \\
		$\Rightarrow p(x)=\exp \{ -1+\lambda_1+\lambda_2+\lambda_3(x-\mu)^2 \}$
		\item solving Lagrangian multiplier to have $\lambda_1=1-\frac12\ln(2\pi\sigma^2), \lambda_2=0, \lambda_3=\frac{1}{2\sigma^2}$ \\
		$\Rightarrow p(x)=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp \{ -\frac{(x-\mu)^2}{2\sigma^2} \} = \mathcal N(x|\mu, \sigma^2)$ \\
		i.e. Gaussian distribution to maximize differential entropy \\
		$\Rightarrow$ corresponding largest entropy $H[x] = \frac12 (1+\ln(2\pi\sigma^2))$, which \\
			\begin{itemize}
			\item increase as $\sigma^2\uparrow$ (distr gets broader)
			\item can be negative, when $\sigma^2<\frac{1}{2\pi e}$
			\end{itemize}
		\end{itemize}
	\item Multiple Variable
		\begin{itemize}
		\item given joint distribution $p(x, y)$, $H[x,y]=-\int\int p(x, y)\ln p(x, y)$
		\end{itemize}
	\end{itemize}
\item Conditional Entropy
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item discrete variable $H[Y|X] = -\sum_{y, x}p(Y=y, X=x)\ln p(Y=y|X=x)$
		\item conditional variable $H[y|x] = -\int\int p(y, x)\ln p(y|x) \D y\D x$
		\end{itemize}
	\item Derivation
		\begin{itemize}
		\item for discrete variable, $H[Y|X] = \sum_x p(x) H[Y|X=x] = -\sum_x p(x)\sum_y p(y|x)\ln p(y|x) = -\sum_{y,x}p(y, x)\ln p(y|x)$
		\item for continuous variable, $H[y|x] = -\int\int p(y, x)\ln p(y|x) \D y\D x$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item $H[x, y] = H[y|x] + H[x]$, by prob rule
		\end{itemize}
	\end{itemize}
\item Relative Entropy - KL Divergence
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $D_{KL}(p||q) = -\int p(x)\ln q(x)\D x - (-\int p(x)\ln p(x)\D x) = -\int p(x)\ln \frac{q(x)}{p(x)} \D x$
		\end{itemize}
	\item Smallest Divergence
		\begin{itemize}
		\item as $-\ln x$ a strictly convex function, by \hyperref[Math_Calc_Jensen]{Jensen's inequality} \\
		$\Rightarrow KL(p||q) = -\int \ln \frac{q(x)}{p(x)} \D x \ge -\ln\int q(x) \D x = 0$, and equality holds iff $q(x)=p(x)$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item measure the distance between 2 distribution, yet not a symmetrical quantity i.e. $KL(p||q) \not\equiv KL(q||p)$ \\
		$\Rightarrow$ measure the amount of information lost between estimation $q$ and true distribution $p$
		\end{itemize}
	\end{itemize}
\item Mutual Information
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item $I[x,y] = KL(p(x,y) || p(x)p(y)) = -\int\int p(x,y) \ln \frac{p(x)p(y)}{p(x,y)}$
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item if variable $x,y$ are independent, $p(x,y) = p(x)p(y)$ \\
		$\Rightarrow I[x,y]$ to measure the degree of independence between $x, y$ ($I[x,y]=0$ iff $x,y$ independent)
		\item related to conditional entropu by $I[x,y] = H[x] - H[x|y] = H[y] - H[y|x]$ \\
		$\Rightarrow$ represent the reduced uncertainty of $x$ after $y$ observed, where $p(x)$ as prior, $p(x|y)$ as posterior after observation
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Mutual Information}
\begin{itemize}
\item
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Recommended Practice}

\subsection{Data}
\subsubsection{Data Augmentation}
\begin{itemize}
\item Artificial Data Synthesis
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item easily prepare a large amount of similar (yet, different) data
		\item add canonical noise to the similar data
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item convert similar data to the desired distribution
		\end{itemize}
	\item Caution
		\begin{itemize}
		\item collected canonical noise may only represent a subset of all possible noise \\
		$\Rightarrow$ may overfit to those collected noise \\
		(e.g. distortion on image from game for car detection: too less unique cars)
		\end{itemize}
	\item Examples
		\begin{itemize}
		\item in the task of classifying image uploaded by users \\ 
		$\Rightarrow$ collect web image \& blur the image
		\item in the task of in-car NLP interaction \\
		$\Rightarrow$ collect well-recorded sentence audio \& add in-car noise
		\end{itemize}
	\end{itemize}
	
\item Distortion
	\begin{itemize}
	\item Practice in Computer Vision
		\begin{itemize}
		\item mirroring horizontally/vertically, rotation, shirring, etc.
		\item random crop a reasonably large subset of image
		\item color shifting: e.g. PCA color augmentation
		\end{itemize}
	\item 
	\end{itemize}
\end{itemize}
\subsubsection{Data Preprocessing}
\begin{itemize}
\item Mean Centering
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item for all training examples, compute mean (on each features) $\displaystyle \mu=\frac 1 N \sum_{n=1}^N{\mathbf x_n}$, \\
		where $\{\mathbf x_1,...,\mathbf x_N\}=X_\text{train}$ the training set
		\item preprocess each $\mathbf x \in X_\text{train}, X_\text{val}, X_\text{test}$ to be $\mathbf x'=\mathbf x -\mu$
		(all data go through the same process)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item (training) data has a zero mean (statistically, most data close to $0$)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item different features may reside in various scales
		\end{itemize}
	\end{itemize}
\item Standardizing
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item compute mean $\mu$, standard deviation $\displaystyle \sigma=\left( \frac 1 N \sum_{n=1}^N{(\mathbf x_n-\mu)^2} \right)^{1/2}$, \\
		where $\{\mathbf x_1,...,\mathbf x_N\}=X_\text{train}$ the training set
		\item preprocess each $\mathbf x \in X_\text{train}, X_\text{val}, X_\text{test}$ to be $\mathbf x'=\frac{\mathbf x -\mu} \sigma$ \\ 
		(all data go through the same process)
		\item note: with big data, usually computed iteratively due to limited memory
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item (training) data has zero mean \& unit variance \\
		$\Rightarrow$ approximated to normal distribution
		\item for deep learning: different features in same small range close to $0$ \\
		$\Rightarrow$ weights for different features are in roughly the same scale \\
		$\Rightarrow$ easier to train
		\end{itemize}
	\end{itemize}

\item Cleaning Incorrect Label
	\begin{itemize}
	\item Practice 
		\begin{itemize}
		\item before cleaning: measure its contribution to the error rate \& its cause
		\item random error (e.g. occasional mistake, etc.) with a big dataset: okay to ignore
		\item systematic error: should be corrected, at least for val\&test set \\
		$\Rightarrow$ to evaluate the model on the target data distribution
		\item if mislabeled data cause inability to evaluate\&compare model: must be cleaned
		\item val\&test set should be cleaned together \\
		$\Rightarrow$ to have the same distribution
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item in train set: statistic model (deep net etc.) quite robust to random errors \\
		while model can learn the systematic error $\Rightarrow$ not able to generalize
		\item in val set: random error can cause inability  \\ 
		$\Rightarrow$ 
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Dataset}
\subsubsection{Train-Val-Test}
\begin{itemize}
\item Reason
	\begin{itemize}
	\item Iterative Process
		\begin{figure}[!ht]
		\includegraphics[width=0.3\linewidth, center]{"./Deep Learning/plot/experiment practice-iterative process".png}
		\end{figure}
		\begin{itemize}
		\item intuition usually do NOT transfer across domains (NLP, CV, Search, etc.)
		\item do NOT hope to have the correct hyperparameters at the first try
		\end{itemize}
		$\Rightarrow$ need feedback from experiment result \\
		$\Rightarrow$ make sure the feedback is CORRECT and FAST
	\end{itemize}
\item Recommended Usage
	\begin{itemize}
	\item Splitting 
		\begin{itemize}
		\item classic split for small dataset $\Rightarrow$ train:val:test $= 60:20:20$, or K-fold
		\item in big data (e.g. $100$ million) $\Rightarrow$ train:val:test $= 98:1:1$
		\end{itemize}
		(as long as val-test sets cover enough data variance)
	\item Training Set
		\begin{itemize}
		\item to find the model parameter estimation (used for learning process of model) \\
		$\Rightarrow$ over-fit by complex model
		\item can incorporate methods to train the model to have desired property \\
		(where augment data goes)
		\end{itemize}
	\item Validation Set (Val)
		\begin{itemize}
		\item to indicate generalization ability of a range of trained models on target data \\
		(correct if enough various input covered) \\
		$\Rightarrow$ for model comparison, selection \& hyperparameters tunning
		\item should have consistent distribution with test set \\
		(as val set is also evaluating the generalization ability)
		\end{itemize}
	\item Test Set
		\begin{itemize}
		\item to evaluate the \textbf{generalization ability} of final model on target data \\
		(correct if enough various input covered)
		\item should represent the distribution of target data \\
		i.e. data that the deployed model will need to handle
		\end{itemize}
	\item Training-Validation Set (Train-Val)
		\begin{itemize}
		\item another val set split from original training set
		\item used if training set are from different distribution then the val/test set \\
		(e.g. due to augmented data etc.)
		\item performance gap between train\&val set: variance + distribution mismatch \\
		$\Rightarrow$ separate each measurement
		\item performance gap between train\&train-val set: measuring variance \\
		$\Rightarrow$ performance gap between train-val\&val set: measuring distribution mismatch
		\end{itemize}
	\end{itemize}

\item Potential Problem
	\begin{itemize}
	\item Mismatched Distribution across Sets
		\begin{itemize}
		\item classic supervised learning assumption: all sets drawn from SAME distribution \\
		(yet transfer/adaptive learning focus on violation of such assumption)
		\item measured by train-val set
		\item should ensure at least that val\&test set have the SAME distribution
		\end{itemize}
		$\Rightarrow$ yet, make sure val\&test set from the SAME distribution as the desired one
	\item \textbf{Overfitting Val Set}
		\begin{itemize}
		\item iteratively tunning model is a processing of learning (fitting to the val set) \\
		$\Rightarrow$ with enough iteration, val set can be overfit
		\item may consider test set as $2^\text{nd}$ val set, and further have $3^\text{rd}, 4^\text{th}...$ val sets
		\end{itemize}
	\item Limited Data
		\begin{itemize}
		\item better model $\Rightarrow$ more training data 
		\item $\Rightarrow$ less validation $\Rightarrow$ noisy estimation of generalization ability
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Train-Test}
\begin{itemize}
\item No Val Set
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item may use the "test" set as val set $\Rightarrow$ generalization ability NOT reported
		\item should be confident in that dataset cover/represent true distribution of data \\ 
		(yet, not recommended)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item to utilize as many data as possible for ultimate performance
		\end{itemize}
	\end{itemize}
	
\item K-fold Cross Validation
	\begin{itemize}
	\item Procedure
		\begin{itemize}
		\item split all data into $K$ folds, $K-1$ folds for train, $1$ for validation
		\item $\Rightarrow$ average over all $C^1_K$ combination to indicate the generalization ability
		\item extreme case: leave-out-one $\Rightarrow K=N$, where $N$ is number of all data
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item $\mathcal O(K) \Rightarrow$ slow, especially if training process already slow \\
		$\Rightarrow$ trade off between time vs. constraint on validation
		\item hence, \textbf{not} often used in big data era
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Orthogonalization Practice}
\subsubsection{Definition}
\begin{itemize}
\item Decoupling Goals and Models
	\begin{itemize}
	\item Designing Metrics
		\begin{itemize}
		\item to evaluate the models \\ 
		$\Rightarrow$ capture how well the problem solved as desired
		\item decouple different aspect of concern into different metrics
		\end{itemize}
	\item Designing Models
		\begin{itemize}
		\item to do well on the previously chosen metrics \\
		(including training \& tunning hyperparameters)
		\end{itemize}
	\end{itemize}
\item Decoupling Hyperparameters
	\begin{itemize}
	\item disjoint set of hyperparameters to optimize for train-val-test set
	\item hyperparameter taking effect on single goal \\
	at least, NOT to impose negatively related effect on multiple goals \\
	(e.g. early stopping on performances on train and val sets $\Rightarrow$ NOT preferred)
	\item $\Rightarrow$ clearer control on model behaviors
	\end{itemize}
\end{itemize}

\subsubsection{Practice}
\begin{itemize}
\item Designing Metrics (Goals)
	\begin{itemize}
	\item Single Metric Reporting Overall Performance
		\begin{itemize}
		\item a metric accounting for multiple metrics \\
		e.g. F1 score instead of precision and recall
		\item weighted average over metrics \\
		(capturing tendency by different weights)
		\end{itemize}
	\item Satisficing Metrics
		\begin{itemize}
		\item optimizing single metric with some minimum requirement must being satisfied \\ 
		$\Rightarrow$ single optimizing metric + several satisficing metrics \\ 
		$\Rightarrow$ optimizing under constraints \\
		e.g. optimizing accuracy with false positive rate $< 0.2$ satisfied
		\end{itemize}
	\end{itemize}
	
\item Tunning Hyperparameters (Designing Model)
	\begin{itemize}
	\item Inherent Separation for Set-level Goals
		\begin{itemize}
		\item fit model on train set for good fitting \\
		$\Rightarrow$ tune model/network structure, optimization, preprocessing, etc.

		\item evaluate on val set for good generalization ability \\
		$\Rightarrow$ tune regularization, etc.

		\item evaluate on test set for hopefully good generalization ability reported \\
		$\Rightarrow$ consider bigger val set (if an overfit val set indicated)

		\item apply in real world hopping model to generalize well indeed \\
		$\Rightarrow$ consider mismatched data distribution, redesign cost function / metrics etc. \\
		(if failed to generalize)
		\end{itemize}
		note: size of dataset can be hyperparameter sometimes
	\item Separating Tunning for Performance Metrics
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Orthogonalization}
\begin{itemize}
\item Definition
	\begin{itemize}
	\item Decoupling Tunning for Different Goals
		\begin{itemize}
		\item disjoint set of hyperparameters to optimize on train-val-test set
		\item hyperparameter taking effect on single goal \\
		(at least, NOT to impose negatively related effect on multiple goals)
		\end{itemize}
	\item Single Metric Reporting Performance
		\begin{itemize}
		\item a metric accounting for multiple metrics \\
		e.g. F1 score instead of precision and recall
		\item optimizing under constraints (must-satisfied metrics) \\
		e.g. optimizing accuracy with false positive rate $< 0.2$ satisfied
		\end{itemize}
	\end{itemize}
\item Practice
	\begin{itemize}
	\item Separating Tuning for Set-level Goals
		\begin{itemize}
		\item for train set: model/network structure, optimization, preprocessing, etc.
		\item for val set:  regularization, etc.
		\item for test set: bigger val set (as indicating an overfit val set)
		\item for real world: mismatched cost function / data distribution, etc.
		\end{itemize}	
	\item Separating Tunning for Performance Metrics
		\begin{itemize}
		\item 
		\end{itemize}
	\end{itemize}
\item Understanding
	\begin{itemize}
	\item Inherently Separated Goals
		\begin{itemize}
		\item fit model on train set: adjust model for good fitting
		\item evaluate on val set: adjust model for good result on metrics \\ 
		(indicating generalization ability)
		\item evaluate on test set: hope to report good generalization ability
		\item apply in real world: hope to generalize well indeed
		\end{itemize}
	\item Clear Control on Behaviors
		\begin{itemize}
		\item form iterative process among various goals
		\item prevent tunning practice with unaware negatively coupled effect on different goals \\
		(e.g. early stopping on performances on train and val sets $\Rightarrow$ NOT preferred)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Tunning Hyperparameters}
especially for deep learning
\subsubsection{Hyperparameters}
\begin{itemize}
\item Overview
	\begin{itemize}
	\item Structures and Architectures
		\begin{itemize}
		\item type of layers and size of layers
		\item type of activation
		\item depth of networks
		\end{itemize}
	\item Learning
		\begin{itemize}
		\item learning rate
		\item optimizer (learning process)
		\end{itemize}
	\item Robustness and Generalizability
		\begin{itemize}
		\item regularization(s)
		\item data preprocessing/augmentation
		\end{itemize}
	\end{itemize}
\item Challenges
	\begin{itemize}
	\item NO Consistent Prescience
		\begin{itemize}
		\item popular choices from one domain usually NOT carry over to other domains
		\end{itemize}
	\item NOT Predictable Effect
		\begin{itemize}
		\item hyperparameter does NOT have predictable effect on specific model behavior \\
		$\Rightarrow$ need a search for the best one
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Systematic Searching}
\begin{itemize}
\item Random Sampling
	\begin{itemize}
	\item Reason
		\begin{itemize}
		\item NOT able to know the importance of different hyperparameters \\
		$\Rightarrow$ not wasting grid search step on the unimportant
		\item NOT able to know the effective range of a hyperparameters \\
		$\Rightarrow$ may be skipped by grid search step
		\item decouple the search for different hyperparameters $\Rightarrow$ more richly explore \\
		(whereas grid search fix one while searching on others)
		\end{itemize}
	\item Coarse to Fine Scheme
		\begin{itemize}
		\item explore whole space uniformly (equally random)
		\item exploit region where good results show up (with more densely sampled)
		\end{itemize}
	\item Sampling on Scale
		\begin{itemize}
		\item instead of sampling the value of hyperparameter, sample the scale of it \\
		e.g. sampling learning rate $\alpha = 10^r, r\sim U(-4,0)$
		\item $\Rightarrow$ distribute the density across desired scale\textbf{s} \\ 
		(by using transfered scale, e.g. applying $\log, e^x$ e.t.c)
		\item reason: depends on the 
			\begin{itemize}
			\item use of hyperparameter e.g. in an exponential/linear/log way
			\item whether intend to sample on scale e.g. across one or more scales
			\end{itemize}
		\end{itemize}
	\end{itemize}

\item Swarm Optimization
	\begin{itemize}
	\item Intuition
		\begin{itemize}
		\item searching over a space with continuous$\times$discrete across various scale \\ 
		$\Rightarrow$ encoded into a list
		\item search using permutation / group behavior \\ 
		$\Rightarrow$ inherently imposing explore-exploit strategy
		\end{itemize}
	\item Popular Framework
		\begin{itemize}
		\item genetic algorithm (GA)
		\item particle swarm optimization (PSO)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Tunning Practice}
\begin{itemize}
\item Single Model
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item supervising one model at a time
		\item interactively justify the hyperparameter in training process  \\
		$\Rightarrow$ gain knowledge through interaction \& ensure a good performance \\
		$\Rightarrow$ early feedback
		\end{itemize}
	\item Reason
		\begin{itemize}
		\item too many data (online advertisement, computer vision etc.)
		\item few computing resource
		\end{itemize}
	\end{itemize}
\item Parallel Training
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item shoot out multiple model with various settings
		\item compare at the end (after trained \& evaluated)
		\end{itemize}
	\item Reason
		\begin{itemize}
		\item small data/model, enough computability / fast training process
		\end{itemize}
	\end{itemize}
\end{itemize}
%\subsection{Training Model}
%\subsubsection{Data Feeding}
%\begin{itemize}
%\item Multi-processing/threading
%	\begin{itemize}
%	\item Loading Data
%		\begin{itemize}
%		\item load the data from disk \& wait for io
%		\end{itemize}
%	\item Applying Preprocessing
%		\begin{itemize}
%		\item normalization
%		\item augmentation
%		\end{itemize}
%	\end{itemize}
%\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Analysis}

\subsection{Measurements of Problem}
\subsubsection{Performance Metrics}
\begin{itemize}
\item Intersection over Union (IoU)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item two regions, from prediction and label \\
		e.g. bounding box, segmentation mask
		\end{itemize}
	\item Definition
		\begin{itemize}
		\item $\text{I}=$ intersection of the regions
		\item $\text{U}=$ union of the regions
		\item $\Rightarrow \text{IoU} = \frac I U$ 
		\end{itemize}
	\item Use Case
		\begin{itemize}
		\item evaluation of object detection/segmentation
		\item post-processing in object detection/segmentation \\
		(e.g. \hyperref[DL_CV_Objdet_nonmax]{non-max suppress})
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item measure how well two regions overlap \\
		$\Rightarrow$ usually $\text{IoU}>0.5$ considered a decent match
		\item average IoU over each prediction class \& scale of threshold for an overview report \\
		(e.g. average over all obstacle types, threshold range $[0.05~0.95]$ with step $0.05$)
		\end{itemize}
	\item Extension
		\begin{itemize}
		\item intersection over label/prediction region $\Rightarrow$ in case of unstable label/prediction region \\ 
		(while matching relation considered important in the task)
		\end{itemize}
	\end{itemize}
\item Bilingual Evaluation Understudy (BLEU)
	\begin{itemize}
	\item Input
		\begin{itemize}
		\item one sequence as prediction; other sequence(s) as references \\ 
		(can be more than one) i.e. label
		\end{itemize}
	\item Definition
		\begin{itemize}
		\item $n$-gram $g_n$: $n$ continuous tokens
		\item count of $g_n$: the number of appearance of $g_n$ in a sequence \\
		$\Rightarrow$ num of $g_n \displaystyle =\sum_{g_n} (\text{count of }g_n) = l-n+1$, in sequence of len $l$
		\item ref count: $\displaystyle \sum_{g_n\in\text{pred}} (1 \text{ if it appears in any reference; else } 0)$ \\
		$\Rightarrow$ count of $g_n$ in prediction that also appears in a reference \\
		$\Rightarrow g_n\in$ pred matched as long as $g_n\in$ a reference
		\item $\Rightarrow$ precision $\displaystyle = \frac {\text{true positive}} {\text{positive pred}} = \frac {\text{ref count}} {\text{num of }g_n\in \text{pred}}$\\
		(yet, "the the the the" has high score when $n=1$, as "the" almost always appear)	
		\item clipped ref count: $\displaystyle \min\{ \text{ count of }g_n \text{ in pred }, \max(\text{ num of }g_n\text{ in a reference }) \}$ \\
		$\Rightarrow$ count only unique $n$-gram in pred \\
		$\Rightarrow g_n\in$ pred matched up to the max num of $g_n\in$ reference
		\item $\Rightarrow$ clipped precision $\displaystyle p_n = \frac {\text{clipped true positive}}{\text{positive pred}} = \frac {\text{clipped ref count}} {\text{num of }g_n \in \text{pred}}$
		\item $\Rightarrow$ BLUE score $\displaystyle = BP\exp\left(\frac 1N \sum_{n=1}^N p_n\right)$, \\
		where $BP=1$ if $\text{len}_\text{pred}>\text{len}_\text{ref}$; else $\exp(1-\text{len}_\text{pred}/\text{len}_\text{ref})$ \\
		$\Rightarrow$ the brevity penalty to penalize too short pred \\
		(as short pred has larger chance to have all its gram contained in ref)
		\end{itemize}
	\item Use Case
		\begin{itemize}
		\item machine translation
		\item image caption
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item clipped ref count: has a maximal number for $g_n$ appearance, given the reference\\
		$\Rightarrow$ same $g_n$ exceeding the number becomes false positive, as can NOT be matched \\
		(analogy: metrics in obj detection with bounding box)
		\item evaluate the appearance of generated tokens (prediction) in references (label)
		\item highly correlated with human evaluation
		\item bias towards statistical model (when compared against rule-based model)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Expected Generalization Ability Measurement}
\begin{itemize}
\item Bayes Optimal Performance
	\begin{itemize}
	\item Measurement
		\begin{itemize}
		\item the theoretically best performance on all data \\ 
		$\Rightarrow$ modeling only the indent mapping without the noise (a perfect model) \\
		(note: in practice, only approximated bayes performance available)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item measure the inherent noise in data as the best possible performance on all data
		\item $\Rightarrow$ measure \textbf{avoidable} bias: indicate the upper-bound performance
		\item $\Rightarrow$ measure degree of \textbf{overfitting}: indicate how much the model fit to the noise
		\end{itemize}
	\end{itemize}

\item Human Performance: Approximation to Bayes Performance
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item for best approximation: best achievable performance by human \\  (e.g. group of experts, as bayes optimal performance is even better)
		\item for specific focus: depends on use case \\
		e.g. for self-diagnose model, may define as the performance of a normal doctor
		\end{itemize}
	\item Practice
		\begin{itemize}
		\item usually done in supervised learning \\
		note: the label (i.e. $0$-error) is NOT bayes performance
		\item on unstructured data, human almost achieves bayes performance \\
		(as human good at natural perception task, like cv, nlp)
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item hard to distinguish surpassing human performance from overfitting training set
		\end{itemize}
	\end{itemize}

\item (Avoidable) Bias
	\begin{itemize}
	\item Measurement
		\begin{itemize}
		\item gap between train set performance and bayes performance \\
		(note: in practice, only approximated bayes performance available)
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item measure the model capacity of handling given (train) data \\
		as (approximately) measuring the gap between the theoretically best model
		\end{itemize}
	\end{itemize}

\item Variance
	\begin{itemize}
	\item Measurement
		\begin{itemize}
		\item performance gap between val set \& train set (if under same distribution)
		\item if different distribution for train\&val set $\Rightarrow$ train-val set instead of val set
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item measure the generalization ability: \\ 
		as measuring how much model can cope with unseen data \\ 
		$\Rightarrow$ model the indent mapping, instead of the noise
		\end{itemize}
	\end{itemize}

\item Mismatch Distribution
	\begin{itemize}
	\item Measurement
		\begin{itemize}
		\item performance gap between train-val set \& val set
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item train-val set contains the unseen train data; val set the unseen target data \\
		$\Rightarrow$ gap only caused by different distribution between sets
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Model Complexity}
\begin{itemize}
\item Number of Parameters
	\begin{itemize}
	\item 
	\end{itemize}
\item Effective Number of Parameters
	
\end{itemize}

2. Interaction with regularization:

- Improper $\lambda$:
- large $\lambda$ => high bias
- small $\lambda$ => high variance
- Choosing $\lambda$:
- try $\lambda=0,0.01,0.02,0.04,...,10$
- select the model with lowest $J_{cv}(\theta)$ without regularization term

3. Interaction with training set size:

- Normal Learning curve:

![Normal learning curve](../../Machine%20Learning/Statistical%20Machine%20Learning/Normal%20learning%20curve.png) 

- Learning curve with high bias:

- where getting more training data **doesn't** help

![Learning curve with high bias](../../Machine%20Learning/Statistical%20Machine%20Learning/Learning%20curve%20with%20high%20bias.png) 

- Learning curve with high variance:

- where getting more training data **helps**

![Learning curve with high variance](../../Machine%20Learning/Statistical%20Machine%20Learning/Learning%20curve%20with%20high%20variance.png) 

4. Ways to fix:

- High bias:
- more features / more polunomial terms of features
- decreasing $\lambda$

- High variance:

- larger data setOrtho
- fewer features
- increasing $\lambda$

- **In neural network:**

- High bias => larger neural networks (more hidden layers / more units in one layer)

- High variance => smaller neural networks

**Larger network with regularization ($\lambda$) is more powerful**

\subsection{Improving Model}

\subsubsection{Bias-Variance Guideline}
\begin{itemize}
\item Solving High (Avoidable) Bias
	\begin{itemize}
	\item Increasing Model Capability
		\begin{itemize}
		\item increase complexity: more weights, latent variable / hidden layer etc.
		\item use more suitable model specifically designed for the data (e.g. CNN for image)
		\end{itemize}
	\end{itemize}
	$\Rightarrow$ until fitting training set well
\item Solving High Variance
	\begin{itemize}
	\item Data Augment
		\begin{itemize}
		\item get/simulate more training data (via crowd sourcing, distortion, GANs, etc.)
		\end{itemize}
	\item Model Regularization
		\begin{itemize}
		\item control the complexity of model (e.g. L0/1/2 normalization)
		\end{itemize}
	\end{itemize}
\item Solving Trade-off
	\begin{itemize}
	\item Iterative Process
		\begin{itemize}
		\item solve bias, then solve variance, iteratively
		\end{itemize}
	\item Complexity + Data/Regularization
		\begin{itemize}
		\item increase complexity to solve bias \\
		without hurting variance (via more data/regularization)
		\item more data/regularization to solve variance \\
		without hurting bias (with enough complexity)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Behavior Detail}
\begin{itemize}
\item Low Bias, High Variance (Over-fitting)
	\begin{itemize}
	\item Symptom
		\begin{itemize}
		\item good performance on train set \& poor generalization (bad on val) \\
		\item $\Rightarrow$ good at fitting train set; bad at representing/modeling underlying data source
		\end{itemize}
	\item Cause
		\begin{itemize}
		\item too much representation ability (to fit even the noise)
		\item directly model the likelihood instead of posterior
		\end{itemize}
	\item Remedy
		\begin{itemize}
		\item larger dataset
		\item regularization (model the posterior by accounting prior)
		\end{itemize}
	\end{itemize}
\item High Bias, Low Variance (Under-fitting)
	\begin{itemize}
	\item Symptom
		\begin{itemize}
		\item bad at fitting training examples \& modeling underlying data source \\ 
		(bad at train \& val)
		\item $\Rightarrow$ poor performance on train set \& good generalization (though meaningless)
		\end{itemize}
	\item Cause
		\begin{itemize}
		\item lack of representation ability (not enough flexibility)
		\end{itemize}
	\item Remedy
		\begin{itemize}
		\item try model with better representative ability (more complexity, flexibility)
		\end{itemize}
	\end{itemize}
\item High Bias, High Variance (Over\&Under-fitting)
	\begin{itemize}
	\item Symptom
		\begin{itemize}
		\item bad at fitting some general cases; while good at some rare and special cases \\
		(especially in high dimensional space) \\
		$\Rightarrow$ fitting largely noise
		\end{itemize}
	\item Cause
		\begin{itemize}
		\item model probably not suitable for the dataset
		\end{itemize}
	\item Remedy
		\begin{itemize}
		\item switch to other types of model
		\item dataset preprocessing
		\end{itemize}
	\end{itemize}
\item Low Variance, High Mismatch
	\begin{itemize}
	\item Symptom
		\begin{itemize}
		\item good at generalizing (on train-val); bad at target data (on val)
		\end{itemize}
	\item Cause
		\begin{itemize}
		\item model not able to generalize across mismatch distribution \\ 
		(yet generalized well in the same distribution: as good at train-val)
		\end{itemize}
	\item Remedy
		\begin{itemize}
		\item transform train data towards (more like) target data \\
		e.g. data synthesis: adding noise that is special in target data, etc.
		\item ensure train set contains enough / assign larger weight to, the desired target data
		\item \hyperref[DL_Learning_Transfer]{transfer learning}, adaptive learning, etc.
		\end{itemize}
	\end{itemize}
\item Low Bias, Low Variance, Low Mismatch, High val-test Variance
	\begin{itemize}
	\item Symptom
		\begin{itemize}
		\item model with specific hyperparameter overfitting the val set \\
		$\Rightarrow$ val set NO longer reveal model generalizability
		\end{itemize}
	\item Cause
		\begin{itemize}
		\item val set overfit by iteration of hyperparameter tunning (as a practice of fitting)
		\end{itemize}
	\item Remedy
		\begin{itemize}
		\item more data for val\&test set
		\item re-design/choose the model after val set overfitting fixed \\ 
		(after true generalizability reported)
		\end{itemize}
	\end{itemize}
	
\item Low Bias, Low Variance
	\begin{itemize}
	\item Behavior
		\begin{itemize}
		\item good at fitting training examples \& modeling underlying data source \\ 
		(good at train \& val)
		\item $\Rightarrow$ good performance on training set \& good generalization
		\end{itemize}
	\end{itemize}
\item High Bias, Low Variance, Lower/Negative Mismatch
	\begin{itemize}
	\item Behavior
		\begin{itemize}
		\item perform better on val\& test set then on train set \\
		$\Rightarrow$ target data distribution easier than train set distribution \\
		$\Rightarrow$ able to do well on desired data, even if not good on train set \\
		(better convinced by measuring human performance on both distribution)
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Error Analysis}
\begin{itemize}
\item Categorizing Error Source
	\begin{itemize}
	\item Practice
		\begin{itemize}
		\item create histogram on val set reflecting data categories \\
		(e.g. for image: blurry, rotated, incorrect label, etc...) \\
		$\Rightarrow$ categorize data first
		\item $\Rightarrow$ data leading to error scattered into different categories \\
		$\Rightarrow$ evaluate the contribution to error from different categories
		\item note: 
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item find out the most important error source \\
		$\Rightarrow$ prioritize the direction of tunning model
		\end{itemize}
	\end{itemize}
	
\item Ceiling Analysis
	\begin{itemize}
	\item Definition \& Practice
		\begin{itemize}
		\item
		\end{itemize}
	\item Understanding
	\end{itemize}
\end{itemize}

\subsubsection{Approaches Analysis}
\begin{itemize}
\item End-to-End Approach
	\begin{itemize}
	\item Definition
		\begin{itemize}
		\item use single network to learn the mapping from input directly to desired output \\
		(no intermediate result)
		\end{itemize}
	\item Pros
		\begin{itemize}
		\item reveal the data statics: avoid any specific prior
		\item large \& auto feature extraction
		\end{itemize}
	\item Cons
		\begin{itemize}
		\item need enough data for effective end-to-end model
		\item hard to inject effective prior into model \\
		$\Rightarrow$ exclude potentially hand-designed component/knowledge
		\end{itemize}
	\item Understanding
		\begin{itemize}
		\item end-to-end model works only when enough data to reveal the problem complexity
		\end{itemize}
	\end{itemize}
\end{itemize}

1. Aim:

- Decide which modules might be the best use of time to improve

2. Procedure:

- Draw a table with 2 column (Component - Accuracy)

- Component: the modules simulated to be perfect (100% accuracy)
- Accuracy: the accuracy of the entire system on the test set (define by chosen evaluation matrix)

- |   **Perfect Component**   |             **Accuracy**              |
| :-----------------------: | :-----------------------------------: |
|           none            |                  $f$                  |
|        module $1$         |            $f+\epsilon_1$             |
|       module $1,2$        |       $f+\epsilon_1+\epsilon_2$       |
| $\cdot \\ \cdot \\ \cdot$ |       $\cdot \\ \cdot \\ \cdot$       |
|    module $1,2,...,n$     | $f+\epsilon_1+...+\epsilon_n = 100\%$ |

- => Improving module $x$ will gain at most $\epsilon_x$ improvement in the overall performance

- Choose the module with most significant $\epsilon$ to improve


1. Procedure:
- Algorithm (trained) misclassifies $n$ data in cross validation set
- Classify these $n$ data and rank them
- Maybe more features are found
2. Feature selection => Numerical evaluation
- => test algorithm with / without this feature on **CV set** (compare error rate)

\subsection{Evaluating Hypothesis}


4. Choosing procedure:

- Minimize traning error $J_{train}(\theta)$ 
- Select a model with lowest $J_{cv}(\theta)$ 
- Estimate generalization error as $J_{test}(\theta)$ 


\subsection{Skewed classes}

1. Precision / Recall

- |                 | **Actual 1**   | **Actual 0**   |
| --------------- | -------------- | -------------- |
| **Predicted 1** | True positive  | False positive |
| **Predicted 0** | False negative | True negative  |

- \textbf{Precision} = $\displaystyle \frac{\text {True positive}}{\text{Predicted positive}} = \frac{\text {True positive}}{\text{True pos + False pos}}$

- **Recall** = $\displaystyle \frac{\text{True positive}}{\text{Actual positive}} = \frac{\text{True positive}}{\text{True pos + False neg}}$

2. Evaluation with precision/recall

- Predict 1 if $ h_\theta(x) \geq \epsilon$, 0 if $h_\theta(x) < \epsilon$

- larger $\epsilon$ => higher precision, lower recall $\small \text{(more confident)}$ 
- smaller $\epsilon$ => lower pecision, higher recall $\small \text{(avoid missing)}$       

![Posiible Precision -Recall curev](../../Machine Learning/Statistical 0Machine Learning/Posiible Precision -Recall curev.png)  

3. Compare precision/recall num

- $\displaystyle \text{F}_1 \space  Score = 2\frac{PR}{P+R}$, $P$ as precision, $R$ as recall
- higher better, on cross validation set

4. High precision \& high recall:

- **large num of features $\small\text{(low bias)}$ + large sets of data $\small\text{(low variance)}$**


\section{Supervised Learning}
\begin{itemize}

\item Feature normalization: $\forall x_{ij} \in X, x_{ij}=\frac{x_{ij}-\mu_j}{\sigma_j^2}$, $ X:[instance][feature]$, without $[1...1]^T$ in 1st column $X=[x_1,x_2,...,x_m]$, m instances in total
\item Regularization: add penalty for $\theta$ being large into cost function
\item $\displaystyle J(\theta)= \space... + \frac{\lambda}{2m}\sum^n_{j=1}\theta_j^2$ , \textbf{bias $\theta_0$ shouldn't be penalized} 

\end{itemize}


\section{Latent Variable Analysis}

\subsection{Principal Component Analysis (PCA)}

1. Motivation:

- Data compression (reduce highly related features)
- Data visualization

2. Assumption:

- Gaussian distributions for both the latent and observed variables

3. Two Equivalent Definition of PCA:

- Linear projection of data onto lower dimensional linear space (principal subspace) such that: 

$\Rightarrow \text{variance of projected data is maximized} \\ \Rightarrow \text{distortion error from projection is minimized} $ 

4. Maximum Variance Formulation

- Goal: 

- project data from D dimension to M while maximizing the variance of projected data

- Eigenvalues $\lambda$ of covariance matrix $S$ express the variance of data set $X$ in direction of corresponding eigenvectors

- Projection Vectors: 

- $U = (u_1,...,u_M), \text{ where } \forall i\in\{1,..,M\},u_i \in \mathbb R^D \text{ s.t. } u_i^Tu_i=1 \space \small \text{(only consider direction)} $ 

- Projected Data:

- $\displaystyle \text{Mean} = {\bar x}^TU \text{, where } \bar x = \frac 1 N\sum_{i=1}^Nx^i$ 
- $\displaystyle \text{Variance} =  tr\{U^T S U\} \text{, where } S = \sum_{i=1}^N(x^i-\bar x)(x^i-\bar x)^T \space\small\text{ (outer product)}$ 

- Lagrangian to maximize $\text{Variance}$: 

- $\displaystyle L(U,\lambda)=tr\{U^TSU\}+tr\{(I-U^TU)\lambda\}$ 

	$\small\text{constraint }u_i^Tu_i=1 \text{ to prevent $u_i \rightarrow +\infty$ } $ 

\begin{gather}\text{For each $u_i\in U$, } \displaystyle \frac \partial {\partial u_i}L = 2Su_i-2\lambda_iu_i=0 \\ \Rightarrow Su_i=\lambda_iu_i \\ \Rightarrow \text{$U$ consists of eigenvectors corresponding to the first M large eigenvalue of $S$}\end{gather}

	\(\text{($S$ symmetric $\Rightarrow U$ orthogonal)}\)

5. Minimum Error Formulation:

- Introduce Orthogonal Basis Vector for D dimension:

- $U=(u_1,...,u_D)$ 

- Data representation:

- Original: $\displaystyle x^n = \sum_{i=1}^D\alpha^n_iu_i$ 
- Projected: \(\displaystyle \widetilde {x^n} = \sum_{i=1}^Mz_i^nu_i+\sum_{i=M+1}^Db_iu_i \\ \small\text{$(z_1^n,...,z_M^n)$ is different for different $x^n$, $(b_{M+1},...,b_D)$ is the same for all  $x^n$}\)

- Cost function: $\displaystyle J=\frac1N \sum^N_{n=1}\|x^n-\widetilde{x^n}\|^2, \text{where } \widetilde{x^n}=\sum_{i=1}^Mz^n_iu_i + \sum_{i=M+1}^Db_iu_i$ 

- $\text{Let } \begin{cases} \displaystyle \frac \partial {\partial z^n_j} J=0 \\ \displaystyle \frac \partial {\partial b_j}J=0 \end{cases} \Rightarrow \begin{cases} \displaystyle \frac 1 N 2(x^n-\widetilde{x^n})^T (-u_j) = \frac 2 N (z_j-(x^n)^Tu_j)=0 \\ \displaystyle \frac 1 N \sum_{n=1}^N2 (x^n-\widetilde {x^n})^T(-u_j) = \frac 2 N \sum_{n=1}^N (b_j-(x^n)^Tu_j)=0 \end{cases}$ 

$\Rightarrow \begin{cases} z_j=(x^n)^Tu_j & j\in\{1,...,M\}\\ b_j=\overline{x}^Tu_j & j\in\{M+1,...,D\} \end{cases}$ 

Noticing $\displaystyle (x^n)^Tu_j=(\sum_{i=1}^D\alpha_i^nu_i^T)u_j = a_j \Rightarrow a_j = (x^n)^Tu_j$ 

$\displaystyle \Rightarrow x^n-\widetilde{x^n} = \sum_{i=M+1}^D[(x^n-\overline x)^Tu_i]u_i$ 

- \begin{align} \boldsymbol \Rightarrow \displaystyle J &= \frac 1 N \sum_{n=1}^N\space \left( \sum_{i=M+1}^D [(x^n-\overline x)^Tu_i]u_i\right)^T \left(\sum_{i=M+1}^D [(x^n-\overline x)^Tu_i]u_i\right) \\ &= \frac 1 N \sum_{n=1}^N \left( \sum_{i=M+1}^Du_i^T ((x^n-\overline x)^Tu_i)\right) \left( \sum_{i=M+1}^D ((x^n-\overline x)^Tu_i)u_i \right) \\ &= \frac 1 N \sum_{n=1}^N \sum_{i=M+1}^D u_i^T(x^n-\overline x)^Tu_iu_i^T(x^n-\overline x)u_i & u_i \text{ orthogonal to each other} \\ &= \sum_{i=M+1}^D u_i^T \left( \frac 1 N \sum_{n=1}^N(x^n-\overline x)^T(x^n-\overline x)\right) u_i & \|u_i\|=1 \\ \end{align} 

$ \displaystyle \boldsymbol \Rightarrow J =  \sum_{i=M+1}^D u_i^TSu_i \text{, where }S=\frac 1 N\sum_{n=1}^N(x^n-\overline x)^T(x^n-\overline x)$

- Lagrangian to Minimize $J$: 

- $\displaystyle L(u_{M+1},...,u_D,\lambda_{M+1},...,\lambda_D) = \sum_{i=M+1}^Du_i^TSu_i + \sum_{i=M_1}^D \lambda_i(1-u_i^Tu_i)$ 

	\(\text{constraint $\|u_i\|=1$ to prevent $u_i=0$}\)

\(\text{For each }u_i, \displaystyle \frac \partial {\partial u_i}L=2Su_i-2\lambda_iu_i=0 \\ \Rightarrow Su_i=\lambda_iu_i \\ \Rightarrow \text{To minmize $J$, take eigenvectors with the first $(D-M)$ small eigenvalue orthogonal to (out of) subspace} \\ \Leftrightarrow \text{define subspace with eigenvectors with the first $M$ large eigenvalue} \)

- \begin{align}\displaystyle \text{Intuition: }\widetilde{x_n}&=\sum_{i=1}^M((x^n)^Tu_i)u_i+\sum_{i=M+!}^D(\overline {x}^Tu_i)u_i \\ &= \overline x + \sum_{i=1}^M[(x^n-\overline x)^Tu_i]u_i \end{align}

1. Singular Value Decomposition - SVD:

- Intorduce matrix $A_{m\times n}$ 

- $(A^TA)_{n\times n}$ symmetric matrix ($\text{actually, Gram matrix} \rightarrow \text{semi-definite}$) 
- \begin{gather} \text{eigenvalue decomposition: } \\ A^TA=VDV^T\text{, $V$ is normalized ($v_i^Tv_i=1$) with column as eigenvector} \end{gather}

- $AV=(Av_1,...,Av_n)_{m\times n}$ 

- Let $r(A)=r$

\begin{align} \Rightarrow & r(A^TA)=r(A)=r \space \\&  r(AV)=\min\{r(A),r(V)\}=\min\{r,n\}=r\end{align}

- Reduce $AV$ to basis $(Av_1,...,Av_r)$  

- Let \(\displaystyle U=(u_1,...,u_r) = (\frac {Av_1} {\sqrt {\lambda_1}},...,\frac{Av_r}{\sqrt{\lambda_r}}) \space \text{, $\lambda_i$ is $i$-thh eigenvalue of $A^TA$}\) 

- Orthogonal: $\forall i\neq j, u_i^Tu_j=\frac 1 {\sqrt{\lambda_i\lambda_j}}v_i^TA^TAv_j=\frac {\lambda_j} {\sqrt{\lambda_i\lambda_j}}v_i^Tv_j=0$ 

- Unit: $\|u_i\|=\frac {\|Av_i\|} {\sqrt{\lambda_i}}=\frac {\sqrt {<Av_i,Av_i>}} {\sqrt{\lambda_i}}=1$ 

$\boldsymbol \Rightarrow U \text{ is standard orthogonal (orthonormal) basis}$ 

- $AV=U\Sigma,\text{ where } \Sigma = D^{\frac 1 2}$ 

- Expand $U$ to orthonormal in $\mathbb R^m: \space (u_i,...,u_m)$ 

- Epand corresponding part in $\Sigma$ with $0$ 

- \(A = U\Sigma V^T,\text{ with singular value in $\Sigma$ in decreasing order}\)

2. SVD with PCA:

- $X$ is data matrix in row ($\text{centered - zero mean}$)

- Eigenvectors of corvariance matrix $S=X^TX$ are in $V, \text{ where }X = U\Sigma V^T$ 

- When using $S=U\Sigma V^T \Rightarrow \space U=V \space \land \space S=V\Sigma V^T$ 

	$\text{reduced to eigenvalue decomposition}$ 

- $S=VDV^T \text{ with $V$ orthonormal}$: 

Eigenvalues $\lambda$ of covariance matrix $S$ express the variance of data set $X$ in direction of corresponding eigenvectors

- Projection:

- $\widetilde X = XV_M, \text{ where $V_M$ contains first M-large eigenvectors}$ 
- Projection direction is **not** unique 

3. Reconstruction (approximate): 

- Data is projected onto $k$ dimension using $\text{SVD}$ with $S = U\Sigma V^T$ 
- $x_{approx} = U_{reduce} \cdot z$,  $U_{reduce}$ is n*k matrix, $z$ is k*1 vector
- ![Reconsturction from data Compression](../../Machine%20Learning/Statistical%20Machine%20Learning/Reconsturction%20from%20data%20Compression.png) 

4. Choosing $k$ (num of principal components):

- choose the **smallest** k making $\displaystyle \frac JV \leq 0.01$ => 99% of variance is retained 

- $[U,S,V]$ = svd(Sigma) => $\displaystyle \frac JV=1-\frac {\sum^k_{i=1}S_{ii}}{\sum^n_{i=1}S_{ii}}$, $S$ is diagonal matrix

=> check $\frac JV$ before compress data 

5. Data Preprocessing:

- PCA $vs.$ Normalization:
- Normalization: Individually normalized but still correlated
- PCA: create decorrelated data $-\text{ whitening}$ 
- Whitening: $\text{ projection with normalization}$ 
- $S = VDV^T \text{, where $S$ is Gram matrix over $X^T$}$ 
- \(\forall n, y_n=D^{-\frac12}V^T(x^n-\overline x) \text{, where $\overline x$ is the mean of $X$}\) 

\begin{align} \Rightarrow & \text{{$y^n$} has zero mean} \\ & \displaystyle cov(\{y^n\}) = \frac 1 N \sum_{n=1}^Ny_ny_n^T= D^{\frac {-1} 2}V^TSVD^{\frac{-1}2}=I \end{align}

6. Tips for PCA:

- Do NOT use PCA to prevent overfitting, use regularization instead
- Try original data before implement PCA
- Train PCA only on trainning set

\subsection{Independent Component Analysis (ICA)}

1. Goal:
- Recover original signals from a mixed observed data
- Source signal $S\in \mathbb R^{N\times K}$; mixing matrix $A$; Observed data $X=SA$
- Maximizes statistical independence
- Find $A^{-1}$ to maximizes independence of columns of $S$
2. Assumption: 
- At most one signal is Gaussian distributed
- Ignorde amplitude and order of recovered signals
- Have at least as many observed mixtures as signals
- $A$ invertible
3. Independence $vs.$ Uncorrelatedness
- Independence $\Rightarrow$ Uncorrelatedness
- $p(x_1,x_2)=p(x_1)p(x_2) \Rightarrow \mathbb E(x_1x_2)-\mathbb E(x_1)\mathbb E(x_2) = 0$ 
4. Central Limit Theorem
5. FastICA algorithm

\subsection{t-SNE}

1. Problem \& Focus
2. Compared to PCA:
- No whitening function to use for new data
- PCA can only capture linear structure inside the data
- t-SNE preserves the <u>local distances</u> in the original data

\subsection{Anomaly Detection}

1. Problem to solve:

- Given dataset {$x^1,x^2,...,x^m$}, build density estimation model $p(x)$
- $p(x^{test}<\epsilon)$ => $x^{test}$ anomaly 

2. Hypothesis function: 

- $\displaystyle p(x)=\prod^n_{i=1} p(x_i), \space x \in R^n,\forall i \in [1,n], \space x_i \sim N(\mu_i,\sigma_i^2)$ 
- $\displaystyle \mu=\frac1m \sum^m_{i=1}x^i,\space \sigma^2=\frac1m \sum^m_{i=1}(x^i-\mu)^2$ 
- assume $x_1,...,x_n$ independent from each other

3. Multivariate Gaussian:

- $\displaystyle p(x;\mu,\Sigma)=\frac1{(2\pi)^{\frac n2} |\Sigma|^{\frac 12}} exp(-\frac12 (x-\mu)^T \Sigma^{-1} (x-\mu)),$  

$x\in R^n, \mu\in R^n,\Sigma\in R^{n\times n}$, where $\Sigma$ is covariance matrix

- $\displaystyle \mu=\frac1m \sum^m_{i=1}x^i,\space \Sigma=\frac 1m \sum^m_{i=1}(x^i-\mu)(x^i-\mu)^T$ 
- $x_1,...x_n$ can be correlated but **not** linearly dependent
- need $m > n$ $(m\ge10n\space suggested)$ or elas $\Sigma$ non-invertible

4. Algorithm:

- choose features
- compute $\mu$, $\sigma$
- compute $p(x)$ for new example, anomaly if $p(x) < \epsilon $ 

5. Evaluation (real-number):

- Labeled data into normal/anomalous set

(okay if some anomalies slip into normal set)

- training set: unlabeled data from normal set (60%)
- CV set: labeled data from normal (20%) & anomalous (50%) set
- test set: labeled data from normal (20%) & anomalous (50%) set

- Use evaluation metrics (skewed data)

6. When to use:

- Anomaly detection:
- Very small num of positive data (0-20 commonly); Large num of negative data
- Difficult to learn from positive data (not enough data, too many features...)
- Future anomalies may look nothing like given data
- Supervised Learning:
- Larger num of positive \& negative data
- Enough positive data for algorithm to learn
- Future positive example is likely to be similar to given data

7. Example:

- Anomaly detection:
- Fraud detection, Manufacturing, Monitoring machines in data center...
- Supervised learning:
- Email spam classification (enough data), Weather prediction (sunny/rainy/etc), Cancer classification...

8. Tips:

- Non-guassian feature: transformation / using other distribution
- Choosing features: compare anomaly data with normal data


\subsection{Recommender System}

1. Problem Formulation:

- $r_{i,j}=1$ if item $i$ is rated by user $j$ 

- $y_{i,j}$ = rating of item $i$ given by user $j$ 

- $\theta^j$ = parameter vector for user $j$ 

- $x^i$ = feature vector for movie $i$ 

=> for user $j$, movie $i$, ($r_{i,j}=0$), predict rating $x^i\theta^j$

2. Content Based Recommendations:

- Treat each user as a seperate linear regression problem with the feature vectors of its rated items as traning set

**Assume features for each items ($x^i$) are available and known**

=> given $X$ estimate $\Theta$ 

- Cost Function for $\theta_j$: 

$\displaystyle J(\theta^j)= \frac1 {2} \sum_{i:r_{i,j}=1}(x^i\theta^j-y_{i,j})^2+\frac \lambda {2} \sum_{k=1}^n(\theta^j_k)^2, \theta^j \in R^{n+1} (\theta_0 \text{ not regularized)}$

- Cost Function for $\Theta$:

$\displaystyle J(\Theta)= \frac1{2} \sum_{j=1}^{n_u} \sum_{i:r_{i,j}=1}(x^i\theta^j-y_{i,j})^2+\frac \lambda {2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta^j_k)^2, \\ \theta^j \in R^{n+1} (\theta_0 \text{ not regularized)}, \space n_u \text{ is num of users}$ 

- Update Rule: \(\forall \theta^j_k \in \theta^j, \theta^j_k := \theta^j_k-\alpha\large\frac{\partial J(\Theta)}{\partial\theta^j_k}\), \(\displaystyle \frac{\partial J(\Theta)}{\partial\theta^j_k}=\sum_{i:r_{i,j}=1}(x^i\theta^j-y_{i,j})x_k^i+ \lambda \theta_k^j, \space \text{for $k \neq 0$ ($\theta^j \in R^{n+1}$)}\) 

3. Collaborative Filtering

- \textbf{Assume preference of each users ($\theta^j$) are available and known}

=> given $\Theta$ estimate $X$ 

- Cost Function for $x^i$: $\displaystyle J(x^i)=\frac 1 2 \sum_{j:r_{i,j}=1} (x^i\theta^j - y_{i,j})^2 + \frac \lambda 2 \sum ^n_{k=1} (x_k^i)^2$ 
- Cost Function for $X$: $\displaystyle J(X)=\frac 1 2 \sum_{i=1}^{n_m} \sum_{j:r_{i,j}=1} (x^i\theta^j - y_{i,j})^2 + \frac \lambda 2 \sum_{i=1}^{n_m} \sum ^n_{k=1} (x_k^i)^2 \\ x^j \in R^{n+1} (x_0 \text{ not regularized)}, \space n_m \text{ is num of items}$ 
- Update Rule: \(\forall x^i_k \in x^i, x^i_k := x^i_k-\alpha\large\frac{\partial J(X)}{\partial x^i_k}\), \(\displaystyle \frac{\partial J(X)}{\partial x^i_k}=\sum_{j:r_{i,j}=1}(\theta^jx^i-y_{i,j})\theta_k^j+ \lambda x_k^i, \space \text{for $k \neq 0$ $(x^i \in R^{n+1}$)}\)

- Basic Idea: 

- Randomly initialize $\Theta$ 

- loop:

	Estimate $X$ 

	Estimate $\Theta$ 

- Cost Function: 

$\displaystyle J(X,\Theta) = \frac 1 2 \sum_{(i,j):r_{i,j}=1}(x^i\theta^j - y_{i,j})^2 + \frac \lambda 2 \sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^i)^2 + \frac \lambda 2 \sum_{j=1}^{n_u}\sum_{k=1}^n (\theta_k^j)^2, \space x \in R^n, \space \theta \in R^n$ 

(the sum term in $J(\Theta)$, $J(X)$, and $J(X,\Theta)$ is the same)

- Update Rule: 

- $\forall x^i_k \in x^i, x^i_k := x^i_k-\alpha\large\frac{\partial J(X,\Theta)}{\partial x^i_k}$, $\displaystyle \frac{\partial J(X,\Theta)}{\partial x^i_k} = \frac{\partial J(X)}{\partial x^i_k} =\sum_{j:r_{i,j}=1}(\theta^jx^i-y_{i,j})\theta_k^j+ \lambda x_k^i, \space x^i \in R^n$ 
- $\forall \theta^j_k \in \theta^j, \theta^j_k := \theta^j_k-\alpha\large\frac{\partial J(X,\Theta)}{\partial\theta^j_k}$, $\displaystyle \frac{\partial J(X,\Theta)}{\partial\theta^j_k} = \frac{\partial J(\Theta)}{\partial\theta^j_k} = \sum_{i:r_{i,j}=1}(\theta^jx^i-y_{i,j})x_k^i+ \lambda \theta_k^j, \space \theta^j \in R^n$ 

- \textbf{Algorithm}

- Initialize $X, \Theta$ to **small random values**

=> for symmetry breaking (similar to random initialization in neural network) 

=> so that algorithm learns features $x^1,...,x^{n_m}$ that are different from each other

- Minimize $J(X,\Theta)$ 

- Predict $y_{i,j} = x^i\theta^j$ ($Y = X\Theta$)

- Finding Related Item to Recommend

- $||x^i-x^j||$ is samll => item $i$ and $j$ is similar

- Mean Normalization:

- Problem: if user $j$ hasn't rated any movie, $\theta^j = [0,...,0]$  

=> predicted rating of user $j$ on all item $=0$ 

=> useless prediction

- Algorithm (row version):

	compute vector $\mu, \space \forall \mu_i \in \mu, \mu_i = \text{mean of $Y_i$, where $Y_i$ is the $i^{th}$ row in $Y$}$ 

	manipulate $Y$: $\forall y_{i,j} \in Y \land r_{i,j}=1, \space y_{i,j} -= \mu_i$  => the mean of each row in $Y$ is $0$ 

	predict rating for user $j$ on item $i = x^i\theta^j + \mu_i$ 

- For item $i$ with no rating

=> apply column version of mean normalization

(but user with no rating is generally more important)

\section{Large Scale Machine Learning}

- Compute $cost(\theta,(x^i,y^i))$ before updating 

For every $k$ update iterations, plot average $cost(\theta,(x^i,y^i))$ over the last $k$ examples

- Checking curves:

Increasing $k$ result in smoother line and less noise, but the result is more delayed

\subsection{Online Learning}

1. Situation:
- Has too many data (can be considered as infinite)
- When data comes in as a continuous stream
- Can adapt to changing user preference
2. Procedure:
- Use one example only once (Similar to stochastic gradient decent in this sense

\subsection{Map-reduce}

1. In Batch Gradient Descent:

- Update rule $\displaystyle \theta_j = \theta_j - \alpha \frac 1 m \sum^m_{i=1} (h_\theta(x^i)-y^i)x_j^i$ 
- Parallelize the computation of $\displaystyle \sum^m_{i=1} (h_\theta(x^i)-y^i)x_j^i$ by dividing the data set into multiple sections

2. Ability to reduce:

- Contain operation over the whole data set

(Neural Network can be map-reduced)



\section{Challenge}
\begin{itemize}
\item Dimensionality
	\begin{itemize}
	\item Volume of High Dimensional Sphere
		\begin{itemize}
		\item with $n$ the dimensionality, $R$ the radius, $\epsilon$ the extra radius,
			\begin{itemize}
			\item for $n=2k, k\in N^+$, $V_{2k}(R) = \frac {\pi^k}{k!} R^{2k}$
			\item for $n=2k+1, k\in N$, $V_{2k+1}(R) = \frac {2^{k+1}\pi^k}{(2k+1)!!} R^{2k+1}$
			\end{itemize}
		$\displaystyle \Rightarrow \lim_{D\rightarrow+\infty} \frac {V_D(1)-V_D(1-\epsilon)}{V_D(1)} = \lim_{D\rightarrow+\infty}1-1(1-\epsilon)^D = 1$
		\item $\Rightarrow$ the volume of a $D$-sphere concentrate in a thin shell near the surface!
		\end{itemize}
	\item Volume of High Dimensional Cube
		\begin{itemize}
		\item with $c$ the length of center, $p$ the extra padded length at one side, \\
		$\Rightarrow \frac{V_\text{corner}}{V_\text{all}} = 1 - \frac{V_\text{center}}{V_\text{all}} = 1-\frac{c^D}{(c+2p)^D}$ \\
		$\displaystyle \Rightarrow \lim_{D\rightarrow+\infty}\frac{V_\text{corner}}{V_\text{all}}=1$ \\
		\item $\Rightarrow$ the volume of a $D$-cube concentrates in its corner \& side !
		\end{itemize}
	\item High Dimensional Gaussian
		\begin{itemize}
		\item probability density with respect to radius $r$ for various dimension $D$ \\
		$\Rightarrow$ most density are in a thin shell at a specific $r$			
		\end{itemize}
		\includegraphics[width=0.5\linewidth,center]{./Math/"multivariate gaussian-mass of distribution".jpg}
	\item Resulting Challenge
		\begin{itemize}
		\item input space increase by $\mathcal O(2^n) \Rightarrow$ need more data to represent the whole input space
		\item much more corner case (input lies in corner volume)
		\item distance function in high dimensional space CAN be useless
		\end{itemize}
	\end{itemize}
\end{itemize}