\chapter{Sequential Data}

\begin{itemize}
\item Challenge
	\begin{itemize}
	\item Violated Identically Independent Draw Assumption
		\begin{itemize}
		\item current data depends on previous data $\Rightarrow$ i.i.d. assumption NOT hold
		\item $\Rightarrow$ distribution changing while drawing data
		\end{itemize}
	\end{itemize}
\item Assumption
	\begin{itemize}
	\item yet Gaussian still usually assumed s.t. model complexity remains in iterations \\
	(as exponential family's prior-posterior in the same family)
	\end{itemize}
\item Overview
	\begin{itemize}
	\item Stationary
		\begin{itemize}
		\item data evolves in time; generative distribution stays the same
		\end{itemize}
	\item Non-stationary
		\begin{itemize}
		\item generative distribution changes along the time
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Markov Model}
\subsection{Markov Chain}
\subsubsection{First-order Markov Chain}
\begin{itemize}
\item Assumption
	\begin{itemize}
	\item each data only depends on the most recent node \\
	(direct predecessor)
	\end{itemize}
\item Description
	\begin{itemize}
	\item Bayesian Networks 
		\begin{itemize}
		\item \includegraphics[width=0.3\linewidth, left]{"./Sequential Data/MC-1st-order markov chain".jpg}
		\end{itemize}
	\item $\Rightarrow$ Distribution
		\begin{itemize}
		\item joint distribution: $\displaystyle p(x_1,...,x_N) = p(x_1) \prod_{n=2}^N p(x_n|x_{n-1})$
		\item conditional dependency: $p(x_n|x_1,...,x_{n-1}) = \cdots = p(x_n|x_{n-1})$
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Second-order Markov Chain}
\begin{itemize}
\item Motivation
	\begin{itemize}
	\item Modeling Trend
		\begin{itemize}
		\item at least 2 observations to model a trend
		\end{itemize}
	\end{itemize}
\item Description
	\begin{itemize}
	\item Bayesian Networks
		\begin{itemize}
		\item \includegraphics[width=0.5\linewidth, left]{"./Sequential Data/MC-2nd-order markov chain".jpg}
		\end{itemize}
	\item Distribution
		\begin{itemize}
		\item joint distribution: $\displaystyle p(x_1,...,x_N) = p(x_1)p(x_2|x_1) \prod _{n=3}^N p(x_n|x_{n-1},x_{n-2})$
		\end{itemize}
	\end{itemize}
\item Generalization: $M^{th}$-order Markov Chain
	\begin{itemize}
	\item Distribution
		\begin{itemize}
		\item joint distribution: $\displaystyle p(x_1,...,x_N) = p(x_1)p(x_2|x_1)\cdots p(x_M|x_1,...,x_{M-1}) \times \prod_{n=M+1}^N p(x_n|x_{n-1},...,x_{n-M})$
		\end{itemize}
	\end{itemize}
\item Understanding
	\begin{itemize}
	\item Analysis of $M$ (assume $K$ states for an observation)
		\begin{itemize}
		\item $M=0:$ no markov parameter, data drawn $\text{i.i.d.}$
		\item $M=1:$ first-order chain: $K-1$ for previous observation $\Rightarrow K(K-1)$ parameters
		\item $M:$ $M^{\text{th}}$-order chain: $K-1$ for each of $M$ observation $\Rightarrow K^M(K-1)$ parameters \\
		$\Rightarrow$ num of parameters grows exponentially with $M$
		\end{itemize}
	\end{itemize}
\end{itemize}


\subsection{Hidden Markov Model}
\subsubsection{Overview}
\begin{itemize}
\item Motivation
	\begin{itemize}
	\item Improving High-Order Markov Chain
		\begin{itemize}
		\item avoid exponentially growing parameters in high-order Markov chain
		\item avoid fixed \& restricted length of dependence
		\end{itemize}
	\item Sequential Correlation
		\begin{itemize}
		\item model sequential correlations in data as an extension of mixture models
		\end{itemize}
	\end{itemize}
\item Assumption
	\begin{itemize}
	\item Latent Variable
		\begin{itemize}
		\item latent variables are \underline{discrete} \& form a Markov chain \\
		$\Rightarrow$ assumption on conditional independence
		\item one latent variable $z_n$ for each observation $x_n$
		\end{itemize}
	\end{itemize}
\item Description
	\begin{itemize}
	\item Bayesian Networks \\
	\includegraphics[width=0.5\linewidth, left]{"./Sequential Data/HMM-Hidden Markov model".jpg}
	
	\item Conditional Independence
		\begin{itemize}
		\item $z_{n+1} \perp\!\!\!\perp z_{n-1} \mid z_n$
		\item no blocked path between any two observed $x_i,x_j$ \\
		$\Rightarrow$ prediction depends on all previous observation
		\end{itemize}
	\item Distribution
		\begin{itemize}
		\item joint distribution \begin{align*} \displaystyle p(x_1,...,x_N,z_1,...,z_N) &= p(z_1)[\prod_{n=2}^N p(z_n|z_{n-1})]\prod_{n=1}^N p(x_n|z_n) \\ &= p(z_1)p(x_1|z_1)\prod_{n=2}^N p(z_n|z_{n-1}) p(x_n|z_n) \end{align*}
		\item $\Rightarrow$ as an extension of mixture distribution: \\
		component densities $=p(x|z)$ \\
		choice of component depends on previous state $=p(z_n|z_{n-1})$
		\end{itemize}
	\item Transition Probabilities
		\begin{itemize}
		\item encoding: latent variable $z$ with $K$ state, in one-hot encoding
		\item $p(z_n|z_{n-1}) = \mathbf A_{K\times K}\in [0,1]^{K\times K}$ \\
		where $A_{ik}=p(z_{n,k}=1 \mid z_{n-1,i}=1), \space 0 \le A_{ik}\le 1$ and $\sum_{k=1}^K A_{ik} = 1$ \\
		$\Rightarrow$ num of independent parameters $=K(K-1)$ \\
		$\displaystyle \Rightarrow p(z_n|z_{n-1},A) = \prod_{k=1}^K \prod_{i=1}^K A_{ik}^{z_{n-1,i} \times z_{n,k}}$
		\item $p(z_1) = \mathbf \pi$ \\
		where $\pi_k = p(z_{1k}=1), 0\le\pi_k \le 1  \text{ and } \space \sum_k \pi_k=1$ \\
		$\displaystyle \Rightarrow p(z_1|\pi) = \prod_{k=1}^K \pi_k^{z_{1k}}$
		\item transition diagram \& unfolded transition diagram ($K$=3) \\
		\begin{figure}[h]
			\centering
			\begin{minipage}{.5\textwidth}
				\includegraphics[width=0.5\linewidth, center]{"./Sequential Data/HMM-transition diagram".jpg}
			\end{minipage}%
			\begin{minipage}{.5\textwidth}
				\includegraphics[width=0.5\linewidth, left]{"./Sequential Data/HMM-unfolded transition diagram".jpg}
			\end{minipage}
		\end{figure}
		\end{itemize}
	\item Emission Probabilities
		\begin{itemize}
		\item $\displaystyle p(x_n|z_n,\phi) = \prod_{k=1}^K p(x_n|\phi_k)^{z_{n,k}}$ \\
		where $\phi$ is a set of parameters governing the conditional distribution
		\end{itemize}
	\end{itemize}
\item Variants of Hidden Markov Model
	\begin{itemize}
	\item Homogeneous HMM
		\begin{itemize}
		\item homogeneous assumption: $\forall n=3,...,N, p(z_n|z_{n-1}) = p(z_{n-1}|z_{n-2})$ \\
		(transition probabilities are the same for all)
		\item $\Rightarrow$ joint distribution \begin{align*} \displaystyle p(X,Z|\theta) &= p(z_1|\pi) \left[\prod_{n=2}^N p(z_n|z_{n-1},A)\right] \prod_{n=1}^N p(x_n|z_n,\phi) \\ &= \prod_{k=1}^K \pi_k^{z_{1k}} \left[ \prod_{n=2}^N  \prod_{k=1}^K \prod_{i=1}^K A_{ik}^{z_{n-1,i} \times z_{n,k}} \right] \prod_{n=1}^N \prod_{k=1}^Kp(x_n|\phi_k)^{z_{nk}}\end{align*} \\
		where $X = (x_1,...,x_N), Z=(z_1,...,z_N)$ and $\theta = \{\pi,A,\phi\}$
		\end{itemize}
	\item Left-to-Right HMM
		\begin{itemize}
		\item $\forall i>j,A_{ij} = 0 \Rightarrow$ can only go to larger num state \\
		$\Rightarrow$ transition diagram: \\ 
		\includegraphics[width=0.25\linewidth, left]{"./Sequential Data/HMM-left-to-right HMM".jpg}
		
		\item constraint over the maximal change of state $\Delta$, in one step \\
		$\Rightarrow$ unfolded transition diagram, with $\Delta=1$: \\ 
		\includegraphics[width=0.25\linewidth, left]{"./Sequential Data/HMM-left-to-right HMM with maximal state change".jpg}
		\end{itemize}
	\end{itemize}

\item Maximum Likelihood Solution (via EM)
	\begin{itemize}
	\item Goal
		\begin{itemize}
		\item $\displaystyle \arg\max_{\theta} p(X|\theta)$, where likelihood $\displaystyle p(X|\theta) = \sum_Z p(X,Z|\theta)$ \\
		$\Rightarrow$ determine model parameter $\theta$ given observations $X$
		\end{itemize}
	\item Notation
		\begin{itemize}
		\item $\gamma(z_n) = p(z_n|X,\theta)$, \\
		where $\displaystyle \gamma(z_n) \in \mathbb R^{K} , \sum_{k=1}^K \gamma(z_{nk}) = 1,\forall k,\gamma(z_{nk})\ge0$ \\ 
		$\displaystyle\Rightarrow \gamma(z_{nk}) = p(z_{nk}=1|X,\theta) = \sum_{z_n}\gamma(z_n)z_{nk}$ 
		\item $\xi(z_{n-1},z_n) = p(z_{n-1},z_n|X,\theta)$, \\
		where $\displaystyle \xi(z_{n-1},z_n)\in\mathbb R^{K\times K},\sum_{i,j=1}^K \xi(z_{n-1,i},z_{nj} ) = 1, \forall i,j, \xi(z_{n-1,i},z_{nj})\ge0$ \\
		$\displaystyle \Rightarrow \xi(z_{n-1,i},z_{nk}) = p(z_{n-1,i}=1,z_{nk}=1|X,\theta) = \sum_{z_{n-1},z_n}\xi(z_{n-1},z_n)z_{n-1,i} z_{nk}$ 		
		\end{itemize}
	\item E-step
		\begin{itemize}
		\item evaluate posterior $P(Z|X,\theta) \Rightarrow$ evaluate $\gamma(z_n), \xi(z_{n-1},z_n)$
		\end{itemize}
	\item M-step
		\begin{itemize}
		\item $\displaystyle \theta = \arg \max_{\theta}\ln P(X|\theta) = \arg\max_{\theta}\sum_Z p(Z|X,\theta^\text{old})\ln p(X,Z|\theta) \text{, where } \theta = \{\pi,A,\phi\}$
		\item $\displaystyle \arg\max_{\theta}Q = \sum_{k=1}^K \gamma(z_{1k})\ln \pi_k + \sum_{n=2}^N\sum_{i=1}^K\sum_{k=1}^K \xi(z_{n-1,i},z_{nk})\ln A_{ik} + \sum_{n=1}^N \sum_{k=1}^K \gamma(z_{nk}) \ln p(x_n|\phi_k)$
		\item $\displaystyle \text{where full combination } \small \sum_{Z}\prod_{\text{each combination of }z_{1-n}} = \prod_{X}\sum_{\text{all possibility of each }x}$
		\end{itemize}
	\item Critical Points
		\begin{itemize}
		\item $\displaystyle \pi_k = \frac {\gamma(z_{1k})} {\displaystyle \sum_{i=1}^K \gamma(z_{1i})}$
		\item $\displaystyle A_{ik} = \frac {\sum_{n=2}^N \xi(z_{n-1,i},z_{nk})} {\sum_{n=2}^N \sum_{j=1}^K \xi(z_{n-1,i},z_{nj})}$
		\item $\text{assume independent } \phi_k \Rightarrow \text{if Gaussian } \begin{cases} \displaystyle \mu_k = \frac {\sum_{n=1}^N \gamma(z_{nk})x_n}{\sum_{n=1}^N \gamma(z_{nk})} \\ \displaystyle \Sigma_k = \frac{\sum_{n=1}^N \gamma(z_{nk}) (x_n-\mu_k)(x_n-\mu_k)^T}{\sum_{n=1}^N \gamma(z_{nk})} \end{cases}$
		\end{itemize}
	\item Solving $\gamma(z_{nk}),\xi(z_{n-1,i}, z_{nk})$
		\begin{itemize}
		\item sum-product: based on message passing tree structure in HMM
		\item alpha-beta algorithm: known as forward-backward algorithm \\
		(two approaches are equivalent i.e. derive the same recursion formula)
		\item with Bayesian networks as \\
		\includegraphics[width=0.5\linewidth, center]{"./Sequential Data/HMM-Hidden Markov model".jpg} \\
		$\Rightarrow$ all paths through $z_n$ is blocked conditioned on $z_n$ \\
		$\Rightarrow p(X|z_n) = p(x_1,...,x_n|z_n)p(x_{n+1},...,x_N|z_n)$ \\
		illustration: \\
		\begin{figure}[h]
			\begin{minipage}{0.5\linewidth}
		    \includegraphics[width=0.5\linewidth, center]{"./Sequential Data/HMM-independence".PNG}
		    \end{minipage}
		    \begin{minipage}{0.5\linewidth}
		    \caption{
		    $\displaystyle P(D,B) = \sum_A P(A,B) P(D|A)$ \\
		    $\displaystyle P(F,B) = \sum_A\left( P(A,B) \sum_C P(C|B)P(F|C) \right)$ \\
		    $\displaystyle P(D,F,B) =\sum_A \left( P(A,B) P(D|A)\sum_C P(C|B)P(F|C) \right)$ \\
		    $\Rightarrow P(D|B)P(F|B) = P(D,F|B) \Rightarrow D \perp\!\!\!\perp F | B$
		    }
		    \end{minipage}
		\end{figure}
		
		\end{itemize}
	\end{itemize}
\end{itemize}

6. Maximum Likelihood $-\text{ EM}$ algorithm:
     

   - $\text{alpha-beta algorithm}$: 


     - $\text{Let }\alpha(z_n) = p(x_1,...,x_n,z_n)$ 

       \begin{align} \Rightarrow \alpha(z_n) &= p(x_n|z_n)p(x_1,...,x_{n-1}|z_n)p(z_n) \\ &= \displaystyle p(x_n|z_n)\sum_{z_{n-1}}p(x_1,...,x_{n-1},z_{n-1},z_n) \\ &= p(x_n|z_n) \sum_{z_{n-1}} p(x_1,...,x_{n-1}|z_{n-1})p(z_n|z_{n-1})p(z_{n-1}) \\ &= p(x_n|z_n) \sum_{z_{n-1}}\alpha(z_{n-1}) p(z_n|z_{n-1}) \end{align}

       $\displaystyle \Rightarrow \alpha(z_1) = \prod_{k=1}^K [\pi_k p(x_1|\phi_k)]^{z_{1k}}$ 

     - $\text{Let } \beta(z_n) = p(x_{n+1},...,x_N|z_n)$ 

       \begin{align} \displaystyle \Rightarrow \beta(z_n) &= \frac 1 {p(z_n)} \sum_{z_{n+1}}p(x_{n+1}, ..., x_N, z_n | z_{n+1}) p(z_{n+1}) \\ &= \frac 1 {p(z_n)} \sum_{z_{n+1}}p(x_{n+1}, ..., x_N | z_{n+1})p(z_n | z_{n+1}) p(z_{n+1})  \\ &= \sum_{z_{n+1}}p(x_{n+2},...,x_N|z_{n+1})p(x_{n+1}|z_{n+1}) \frac{p(z_n|z_{n+1})p(z_{n+1}) }{p(z_n)} \\ &= \sum_{z_{n+1}} \beta(z_{n+1}) p(x_{n+1} | z_{n+1}) p(z_{n+1}|z_n) \end{align}

       $\Rightarrow \beta(z_N) = 1, \text{solved from } \gamma(z_N) =  \frac{\alpha(z_N)\beta(z_N)}{p(X)}$ 

     - $\displaystyle \gamma(z_n) = p(z_n|X) = \frac{P(X|z_n)p(z_n)} {p(X)} = \frac{\alpha(z_n) \beta(z_n)}{p(X)}$ 

     - $\displaystyle 1 = \sum_{z_n}\gamma(z_n) = \frac{\sum_{z_n} \alpha(z_n)\beta(z_n)}{p(X)}$ 

       $\displaystyle \Rightarrow p(X) = \sum_{z_n} \alpha(z_n)\beta(z_n)$ 

       $\text{More conveniently, let } \displaystyle z_n=z_N \Rightarrow p(X)=\sum_{z_N}\alpha(z_N)$

       $\Rightarrow \text{complexity: } \mathcal O(n), \text{ instead of } \mathcal O(2^n), \text{ where } n \text{ is the length of the chain}$ 

     - $\displaystyle \xi(z_{n-1}, z_n) = \frac {\alpha(z_{n-1})p(x_n|z_n)p(z_n|z_{n-1})\beta(z_n)} {P(X)}:$ 

       \begin{align} \displaystyle \xi(z_{n-1}, z_n) &= p(z_{n-1},z_n|X) = \frac{p(X|z_{n-1},z_n)p(z_{n-1},z_n)}{p(X)} \\ &= \frac{p(x_1,....,x_{n-1}| z_{n-1},z_n) p(x_n,...,x_N|z_{n-1} ,z_n) \times p(z_n|z_{n-1})p(z_{n-1})} {p(X)} \\ &= \frac{p(x_1,...,x_{n-1}|z_{n-1}) p(x_n|z_n)p(x_{n+1},...,x_N|z_n) \times p(z_n|z_{n-1})p(z_{n-1})} {p(X)} \\ &= \frac {\alpha(z_{n-1})p(x_n|z_n)p(z_n|z_{n-1})\beta(z_n)} {p(X)} \end{align}

       $\text{where factorizing using conditional independence}: \begin{cases} [x_1,...,x_{n-1}],[z_n] &\text{on } z_{n-1} \\ [x_n,...,x_N],[z_{n-1}] &\text{on } z_n  \\ [x_n],[x_{n+1},...,x_N] &\text{on } z_n \end{cases}$ 

       $\small (p(A|B)=p(A) \text{ when } A\perp\!\!\!\perp B) $ 

   - Algorithm description:

     - Initialize $\theta = \{ \pi,A,\phi \}$ 

     - $\text{E step:}$ 

       Forward recursion for $\alpha(z_n)$ 

       Backward recursion for $\beta(z_n)$ 

       Calculate $\gamma(z_n), \xi(z_{n-1}, z_n)$ 

     - $\text{M step:}$ 

       Maximize $Q(\theta,\theta^{\text{old}})$ using critical points

   - Regularized EM

     - add the prior of $\pi, A, \phi$, in the form of $\log p(\theta)$, into $Q(\theta,\theta^\text{old})$ before maximization

7. Prediction

   - Goal

     - predict $x_{N+1}$ given $X=\{x_1,...,x_N\}$ 

   - Algorithm

     - $\alpha$ recusion + summing over $z_N$

       \begin{align} \displaystyle p(x_{N+1}) &= \sum_{z_{N+1}} p(x_{N+1}, z_{N+1}|X) \\ &= \sum_{z_{N+1}} p(x_{N+1}|z_{N+1,X})p(z_{N+1}|X) \\ &= \sum_{z_{N+1}} p(x_{N+1} | z_{N+1}) \sum_{z_N} p(z_{N+1}|z_N,X)p(z_N|X) \\ &= \sum_{z_{N+1}} p(x_{N+1}|z_{N+1}) \sum_{z_N} p(z_{N+1}|z_N)p(z_N|X) \\ &= \frac 1 {P(X)}\sum_{z_{N+1}} p(x_{N+1}| z_{N+1}) \sum_{Z_{N}} p(z_{N+1}|z_N) \alpha(z_N) \end{align}

       $\Rightarrow$ store the $\alpha(z_t)$ for predicting $x_{t+1}$ **and** computing $\alpha(z_{t+1})$ once $x_{t+1}$ observed

     - Intuition: information in $x_1,...,x_N$ stored in $\alpha(z_N)$ 

       $\Rightarrow$ enable real-time application

8. Scaling Factors

   - Original $\alpha-\beta$ Recursion

     - $\displaystyle \alpha(z_n) =  p(x_n|z_n) \sum_{z_{n-1}}\alpha(z_{n-1}) p(z_n|z_{n-1})$
     - $\displaystyle \beta(z_n) =  \sum_{z_{n+1}} \beta(z_{n+1}) p(x_{n+1} | z_{n+1}) p(z_{n+1}|z_n)$ 

     $\Rightarrow$ $\alpha\rightarrow 0$ exponentially quickly to length of chain

     $\Rightarrow$ for long chain (~100), $\alpha$ can exceed the dynamic range of computer

   - Normalized and Rescaled $\alpha$ 

     - normalize: $\displaystyle \hat\alpha(z_n) = \frac{\alpha(z_n)} {p(x_1,...,x_n)} = p(z_n|x_1,...,x_n)$ 

       $\Rightarrow$ stay in the dynamic range

     - let rescale factor $c_n=p(x_n|x_1,...,x_{n-1})$ 

       \begin{align} \displaystyle \Rightarrow \space & p(x_1,...,x_n) = \prod_{m=1}^n c_m \\ \Rightarrow \space & \alpha(z_n) = (\prod_{m=1}^n c_m) \cdot \hat \alpha(z_n) \\ \Rightarrow \space & c_n\hat\alpha(z_n) = p(x_n|z_n)\sum_{z_{n-1}}\hat\alpha(z_{n-1})p(z_n|z_{n-1}) \end{align}

   - Rescaled $\beta$ 

     - let normalization $\displaystyle \hat\beta(z_n) = \frac {\beta(z_n)}{\displaystyle \prod_{m=n+1}^N c_m} = \frac{p(x_{n+1}, ..., x_N|z_n)}{p(x_{n+1}, ..., x_N|x_1,...,x_n)}$ 

       $\displaystyle \Rightarrow c_{n+1} \hat\beta (z_n) = \sum_{z_{n+1}} \hat\beta(z_{n+1}) p(x_{n+1}|z_{n+1})p(z_{n+1}|z_n)$ 
       note: $c_{n+1}$ can be re-used from $\alpha$ recursion

   - EM under Rescaled $\alpha-\beta$ 

     - monitoring likelihood: $\displaystyle p(X) = \prod_{n=1}^Nc_n$ 

     - $E$ step:

       \begin{align}\displaystyle \Rightarrow \space & \gamma(z_n) = \hat\alpha(z_n)\hat\beta(z_n) \\ & \xi(z_{n-1},z_n) = c_n \hat\alpha(z_n)p(x_n|z_n)p(z_n|z_{n+1})\hat\beta(z_n) \end{align}

9. Viterbi Algorithm (max-sum algorithm)

   - Goal:

     - find the most probable sequence of latent variable

       $\Rightarrow$ find $\displaystyle\max_{Z} p(Z|X,\theta), \text{ where $Z$ is sequence of latent states}$ 

     - compare with: set of states being individually most probable $\Rightarrow \forall n$, find $\displaystyle \max_{z_n} p(z_n|X,\theta)$ 

       $\Rightarrow$ maximize $\gamma(z_n)$ for all $n$ 

     - efficiency: searches space of paths efficiently ( $\mathcal O(n)$ to the length of chain )

   - Notation:

     - $\displaystyle w(z_n) = \max_{z_1,...,z_{n-1}} \ln p(x_1,...,x_n,z_1,...,z_n)$ 

       note: $w(z_n)$ is a function of $z_n$, with log probability maximized over $z_1,...,z_{n-1}$ 

   - Recursion from Joint Distribution of HMM:

     - $\displaystyle  w(z_{n+1}) = \ln p(x_{n+1}|z_{n+1}) + \max _{z_{n}} \{ \ln p(z_{n+1}|z_{n}) + w(z_{n}) \}$  

       $\displaystyle w(z_1) =\ln p(x_1,z_1) = \ln p(z_1) + \ln p(x_1|z_1) $ 

   - Backtrack

     - maximization over $z_n$ $\Rightarrow$ individually done for each of the $K$ states of $z_{n+1}$

     - maintain a matrix record for each maximization:

       let $\phi(k_n)$ be the state of $z_n$ when $w(z_{n+1})$ getting maximum given $z_{n+1} = k$ (in state $k$) 

       $\Rightarrow k_n^\text{max} = \phi(k_{n+1}^\text{max})$, where $k_n^\text{max}$ is the desired state of $z_n$

       $\displaystyle \Rightarrow k_{N-1}^\text{max} = \phi(k_{N}^\text{max})=\phi(\arg\max_{z_{N}}w(z_{N}))$   

\section{Linear Dynamic System}

1. Goal

   - Continuous Latent Variable

     - sum becomes integral

     - practical sense $\Rightarrow$ multivariated Gaussian distribution assumed

       (so that complexity of posterior dose NOT increase)

   - Senquential Correlation in Contiuous Data

     - an extension to continous latent variable model (such as probablistic PCA)

2. Notation

   - Underlain Procedure
     - transition: $z_n = A z_{n-1} + w_n, \text{ where noise } w \sim \mathcal N(w|0,\Gamma)$ 
     - emission: $x_n = Cz_n + v_n, \text{ where noise } v \sim \mathcal N(v|0,\Sigma)$
     - initialization: $z_1 = \mu_0 + \mu, \text{ where noise } \mu \sim \mathcal N (\mu|0,V_0)$ 
   - Probabilities
     - transition: $p(z_n|z_{n-1}) = \mathcal N (z_n|Az_{n-1}, \Gamma)$   
     - emission: $p(x_n|z_n) = \mathcal N (x_n|C z_n,\Sigma)$ 
     - initialization: $p(z_1) = \mathcal N(z_1|\mu_0,V_0)$  
   - Model Parameters
     - $\theta = \{ A,\Gamma,C,\Sigma,\mu_0,V_0 \}$ 

3. Maximum Likelihood - EM

   - E step 

     - Inference problem - determine the local posterior marginals for latent variables 

       (sum-product algorithm)

   - M step

4. Linear-Gaussian Model Features

   - sequence of individually most probable latent variable $\Leftrightarrow$ the most probable latent sequence

     $\Rightarrow$ no need for Viterbi algorithm

   - Joint Distribution

     - $\displaystyle P(X,Z) = p(z_1) \left[ \sum_{n=2}^Np(z_n|z_{n-1}) \right] \sum_{n=1}^N p(x_n|z_n)$  - same form as HMM

       $\Rightarrow$ an Gaussian (product of Gaussians)

       $\Rightarrow$ standard result available for its marginals and conditionals 

       $\Rightarrow$ sum-product algorithm for faster computation

5. Inference

   - Goal

     - determine marginal distribution $P(Z|X)$ 
     - prediction: $P(z_n, x_n|x_1,...,x_{n-1}.\theta)$ - used in real-time application

   - Sum-Product Algorithm (Kalman Filter + Kalman Smoother)

     - analogous to $\text{alpha-beta algorithm}$ in HMM

       $\Rightarrow \hat \alpha(z_n) = p(z_n| x_1,...,x_n), \text{ factor } c_n=p(x_n|x_1,...,x_{n-1})$  

       $ \displaystyle \Rightarrow c_n \hat\alpha(z_n) = p(x_n|z_n) \int_{z_{n-1}}\hat\alpha(z_{n-1}) p(z_n|z_{n-1}) \space d z_{n-1}$ 

       \begin{align}\displaystyle \Rightarrow c_n \hat \alpha(z_n) &= c_n \mathcal N(z_n|\mu_n,V_n) \\ &= \mathcal N (x_n|C z_n,\Sigma) \int \mathcal N(z_{n-1} | \mu_{n-1},V_{n-1}) \mathcal N(z_n|Az_{n-1} , \Gamma) \space d z_{n-1} \\ &= \mathcal N (x_n|C z_n,\Sigma) \space \mathcal N (z_n|A\mu_{n-1},P_{n-1}), \\ & \text{where } P_{n-1} = AV_{n-1}A^T+\Gamma \space ( \text{ by integral of } \mathcal N\cdot\mathcal N ) \end{align}

       \begin{align}\displaystyle \Rightarrow \mu_n &= A\mu_{n-1}  + K_n (x_n - CA\mu_{n-1}) \\ V_n &= (I-K_nC) P_{n-1} \\ c_n &= \mathcal N(x_n| CA\mu_{n-1} , CP_{n-1}C^T + \Sigma), \\ & \text{where } K_n = P_{n-1}C^T(CP_{n-1}C^T+\Sigma)^{-1}, \text{ known as Kalman gain matrix} \end{align}

     - Initial Condition

       $\Rightarrow$ $c_1 \hat \alpha (z_1) = p(z_1) p(x_1|z_1), \text{ where } z_1 = p(x_1)$ 

       \begin{align} \displaystyle \Rightarrow \mu_1 &= \mu_0 +K_1 (x_1-C\mu_0) \\ V_1 &= (I-K_1C)V_0 \\ c_1 &= \mathcal N(x_1 |C \mu_0 , CV_0C^T + \Sigma), \\ & \text{where } K_1 = V_0 C^T (CV_0C^T + \Sigma)^{-1} \end{align}

   - Interpretation

     - $A\mu_{n-1}:$ predicted mean of $z_n$, by projecting mean of $z_{n-1}$ one step forward
     - $CA\mu_{n-1}:$ predicted observation $x_n$, by emitting from predicted mean of $z_n$  
     - $x_n-CA\mu_{n-1}:$ error between predicted observation and actual observation
     - $K_n:$ coefficient of error, giving a correction to the predicted mean of $z_{n}$ 

     $\Rightarrow$ making successive predictions \& correcting them in the light of new observation

6. EM - Learning Model Parameters